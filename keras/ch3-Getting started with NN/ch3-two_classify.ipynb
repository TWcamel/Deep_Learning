{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## 3.4 電影評論分類：二分類問題\n本節使用IMDB數據集，其包含 IMDB 中50000條嚴重兩極化的評論。數據集被分為用於訓練的25000條評論，測試集和訓練集都包含50%的正面評論和50%的負面評論。實現如下：\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-1 加載IMDB數據集\n\u0027\u0027\u0027\n參數num_words \u003d 20000的意思是，\n僅保留訓練數據中前20000個最常見的單詞，\n低發生率的單詞將被捨棄，\n這樣得到的向量數據不會太大，便於處理。\n\n而train_data和test_data這兩個變量都式評論組成的list；\n每條評論又是單詞索引組成的list( 表示一系列單詞 )，\ntrain_labels和test_labels都是0(負面)和1(正面)組的list。\n\u0027\u0027\u0027\n\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) \u003d imdb.load_data(\n    num_words\u003d20000)\nprint(train_data[0])\nprint(train_labels[20000])\n\n###\n# 由於限定前10000個最常見的單詞，單詞索引都不會超過20000個\n# 因為索引值通常為位移值\n###\n\nprint(max([max(sequence)for sequence in train_data]))\n\n###\n# 下面這段代碼有個很酷炫的功能\n# 可以將某條評論迅速解碼為英文單詞\n###\n\nword_index \u003d imdb.get_word_index()\nreverse_word_index \u003d dict(\n    [(value, key) for (key, value) in word_index.items()]\n)\ndecoded_review \u003d \u0027 \u0027.join(\n    [reverse_word_index.get(i - 3, \u0027?\u0027) for i in train_data[0]]\n)\n\t\n###\n# word_index是一個將單詞映射為整數索引的字典\n# 而dict的第一行之key、value值顛倒，將整數索引映射為單詞\n# 注意，最後一段程式碼為：\n# 將評論解碼。注意，索引減去了3，因為0、1、2\n# 分別為\u0027padding\u0027(填充)、\u0027start of sequence\u0027(序列開始)、\n# \u0027unknown\u0027(未知詞)，所分別保留的索引值\n###\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4.1 準備數據\n我們不能將整數序列直接輸入NN，需要將列表轉換為張量，轉換方式有兩種，如下：\u003cbr\u003e\n* 填充列表( list )：使其具有相同的長度，再將列表轉換成形狀為( samples, word_indices )的整數張量，然後網路第一層使用能處理這種整數張量的層( 即Embedding層，後續章節會做討論 )。\n* 對列表進行one-hot編碼：將其轉換為只有0和1組成的向量。例如，序列[3,5]轉換為20000維的向量，只有索引為3、5的元素是1，其餘元素都是0。\u003cbr\u003e\n而下面我們採用第二種方法進行數據向量化，如下所示：\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-2 將整數序列編碼為二進制矩陣\nimport numpy as np\nimport pprint\n\ndef vectorize_sequences(sequences, dimension\u003d20000):\n    results \u003d np.zeros((len(sequences), dimension))          # 創建一個形狀為(len(sequences), dimension)的零矩陣\n    for i, sequence in enumerate(sequences):                 # i 為索引值， sequence為索引值的list內容\n        results[i, sequence] \u003d 1.                            # result[i]的指定索引設為1\n    return results\n\nx_train \u003d vectorize_sequences(train_data)\nx_test \u003d vectorize_sequences(test_data)\n\npprint.pprint(x_train[0])       # 訓練樣本現在變成了這樣\npprint.pprint(x_test)           # 測試樣本現在變成了這樣\n\n###\n# 現在，我們將標籤向量化\n# 然後我們就可以將數據加入NN中了\n###\n\ny_train \u003d np.asarray(train_labels).astype(\u0027float32\u0027)\ny_test \u003d np.asarray(test_labels).astype(\u0027float32\u0027)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4.2 構建網路\n輸入數據是向量，標籤是張量(1和0)，這是我們遇到最簡單的情況。有一類網路在這種問題上表現很好，就是帶有relu激活的全連接層( Dense )的簡單堆疊。\u003cbr\u003e\u003cbr\u003e\n\n對於這種Dense層的堆疊，我們需要以下兩個關鍵架構：\n* 網路有多少層\n* 每層有多少個隱藏單元 \u003cbr\u003e\u003cbr\u003e\n\n注意，隱藏單元數越多，網路越能學到更加複雜的表示，但網路的計算代價也變得更大，且可能會導致學到不好的模式(這種模式會提高訓練數據上的性能，但不會提高測試數據上的性能，即overfitting)。\u003cbr\u003e\u003cbr\u003e\n\n下面，我們將以Keras實現如圖3-6所示，三層網路之程式碼。\u003cbr\u003e\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-3 模型定義\nfrom keras import models\nfrom keras import layers\n\nmodel \u003d models.Sequential()\nmodel.add(layers.Dense(16, activation\u003d\u0027relu\u0027, input_shape\u003d(20000,)))\nmodel.add(layers.Dense(16, activation\u003d\u0027relu\u0027))\nmodel.add(layers.Dense(1, activation\u003d\u0027sigmoid\u0027))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-4 編譯模型\nmodel.compile(optimizer\u003d\u0027rmsprop\u0027,\n              loss\u003d\u0027binary_crossentropy\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-5 配置優化器\n\n###\n# 3-4代碼為將優化器、損失函數、指標作為字符串輸入，\n# 這是因為rmsprop, binary_crossentropy和accuracy\n# 都是keras內置的一部分\n# 若我們想要引入自定義的函數，\n# 前者可以向optimizer參數傳入一個優化類實例來實現\n# 後者可以通過向loss、metrics參數傳入函數對向來實現\n###\n\nfrom keras import optimizers\n\nmodel.compile(optimizer\u003doptimizers.RMSprop(lr\u003d0.001),\n              loss\u003d\u0027binary_crossentropy\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-6 使用自定義的損失函數和衡量指標\nfrom keras import losses\nfrom keras import metrics\n\nmodel.compile(optimizer\u003doptimizers.RMSprop(lr\u003d0.001),\n              loss\u003dlosses.binary_crossentropy,\n              metrics\u003d[metrics.binary_accuracy])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4.4 驗證我們的方法\n為了在訓練過程中監控模型在前所未見的數據上的準確度，我們需要將原始數據流出10000個樣本作為驗證集。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-7 切割出驗證集\nx_val \u003d x_train[:10000]\npartial_x_train \u003d x_train[10000:]\n\ny_val \u003d y_train[:10000]\npartial_y_train \u003d y_train[10000:]\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-8 訓練模型\n\n###\n# 現在使用512個樣本組成的小批量\n# 將模型訓練20個輪次(epoch)(即對x_train和y_tran，\n# 兩個張量的所有樣本進行20次迭代)。\n# 與此同時，我們還要監控在切割出的10000個樣本上的損失值\n# 與準確度，我們可以通過將驗證數據傳入validation_data參數來完成\n###\n\nmodel.compile(optimizer\u003d\u0027rmsprop\u0027,\n              loss\u003d\u0027binary_crossentropy\u0027,\n              metrics\u003d[\u0027acc\u0027])\n\nhistory \u003d model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs\u003d20,\n                    batch_size\u003d512,\n                    validation_data\u003d(x_val, y_val))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n###\n# 注意，調用model.fit()返回了一個History對象\n# 這個對象有一個成員history，他是一個字典，\n# 包含了訓練過程中的所有數據，接下來，我們來瞧瞧他的樣子\n###\n\nhistory_dict \u003d history.history\nhistory_dict.keys()\n\n###\n# 字典包含了4個條目，分別對應驗證過程和訓練過程中所監控的指標\n# 接下來，我們來看看兩者的損失值以及準確度的差異。\n###\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-9 繪製訓練損失和驗證損失\n\nimport matplotlib.pyplot as plt\n\nhistory_dict \u003d history.history\nloss_values \u003d history_dict[\u0027loss\u0027]\nval_loss_values \u003d history_dict[\u0027val_loss\u0027]\n\nepochs \u003d range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, \u0027bo\u0027, label\u003d\u0027Training loss\u0027)          # \u0027bo\u0027表示藍色圓點\nplt.plot(epochs, val_loss_values, \u0027b\u0027, label\u003d\u0027Validation loss\u0027)     # \u0027b\u0027表示藍色實線\nplt.title(\u0027Training and validation loss\u0027)\nplt.xlabel(\u0027Epochs\u0027)\nplt.ylabel(\u0027Loss\u0027)\nplt.legend()\n\nplt.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-10 繪製訓練經度和驗證經度\n\nplt.clf()                           # 清空圖像\nacc_values \u003d history_dict[\u0027acc\u0027]\nval_acc_values \u003d history_dict[\u0027val_acc\u0027]\n\nplt.plot(epochs, acc_values, \u0027bo\u0027, label\u003d\u0027Training acc\u0027)\nplt.plot(epochs, val_acc_values, \u0027b\u0027, label\u003d\u0027Validation acc\u0027)\nplt.title(\u0027Training and validation accuracy\u0027)\nplt.xlabel(\u0027Epochs\u0027)\nplt.ylabel(\u0027Loss\u0027)\nplt.legend()\n\nplt.show()\n\n###\n# 我們從圖中的結果可以知，\n# 模型在訓練數據上的表現越來越好，\n# 但是在前所未見的數據上不一定表現得越來越好\n# 也就是說，這是過擬和現象(overfitting)，\n# 最終學到的表示僅針對於訓練數據\n# 無法泛化到訓練集之外的數據\n###\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# 3-11 從頭開始訓練一個模型\n\nmodel \u003d models.Sequential()\nmodel.add(layers.Dense(16, activation\u003d\u0027relu\u0027, input_shape\u003d(20000,)))\nmodel.add(layers.Dense(16, activation\u003d\u0027relu\u0027))\nmodel.add(layers.Dense(1, activation\u003d\u0027sigmoid\u0027))\n\nmodel.compile(optimizer\u003d\u0027adam\u0027,\n              loss\u003d\u0027mse\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n\nmodel.fit(x_train, y_train, epochs\u003d4, batch_size\u003d512)\nresults \u003d model.evaluate(x_test, y_test)\n\nprint(results)\nprint(model.predict(x_test))\n\n###\n# 這種相當簡單的方法得到了88%的準確度\n# 而由model.predict(x_test)回傳的結果可知\n# 網路對某些樣本的結果非常確信(大於等於0.999)\n###\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4.5 進一步的實驗\n通過以下實驗，我們可以確認前面選擇的網路架構是非常合理的，雖然仍有改進的空間。\u003cbr\u003e\n* 前面使用了兩個隱藏層，你可以嘗試使用一個或三個隱藏層，然後觀察對驗證準確度和測試準確度的影響。\n* 嘗試使用更多或更少的隱藏單元，比如32個、64個。\n* 嘗試使用mse損失函數代替binary_crossentropy。\n* 嘗試使用tanh，代替relu。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4.6 小結\n下面是我們應該從這個例子中學到的重點。\u003cbr\u003e\n* 通常需要對原始數據進行大量預處理，以便將其轉換為張量輸入到神經網路中，單詞序列可以編碼為二進制向量，當然，也有其他編碼方式。\n* 帶有relu繳活的Dense層堆疊，可以解決很多問題(包括情感分類)，我們可能會經常用到這種類型。\n* 對於二分類問題(兩個輸出類別)，網路的最後一層應該是只有一個單元並使用sigmoid激活的Dense層，網路輸出應該是0-1範圍內的標量。\n* 對於二分類問題的sigmoid標量輸出，我們應該使用binary_crossentropy損失函數。\n* 隨著NN在訓練數據上的表現越來越好，模型最終會過擬合，並在前所未見的數據上得到越來越差的結果。因此，我們一定要一直監控模型在訓練集之外的數據上的性能。\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}