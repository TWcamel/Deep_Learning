{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "# 卷積神經網路\n",
    "CNN使用於影像辨識、聲音辨識等各種情況。在視覺辨識競賽中，深度學習使用的手法幾乎都是以CNN為基礎。本章將詳細說明CNN的結構，並且利用Python執行處理內容。\n",
    "前面說明過的神經網路是，相鄰各層的所有神經元彼此相連，稱為**全連接( _fully connected_ )**，我們以Affine層的名稱，執行過全連接層。如圖所示，利用Affine層，可以建構出5層全連接的神經網路。<br>\n",
    "![Cnn](./img/ch7-CNN.PNG)<br><br>\n",
    "\n",
    "那用CNN會形成何種結構呢?如下圖所示<br>\n",
    "![Cnn_full](./img/ch7.cnn_fully.PNG)<br>\n",
    "CNN加入新的「Convolution層」與「polling層」。CNN各層的連接順序是「Convlution - ReLU - (Polling)」(有時會省略Polling層)。並於最後輸出層，使用「Affine - ReLU」組合，再將結果以「Affine-Softmax」執行特徵歸一化(全連接層)的動作，這是一般常見的CNN結構。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 7.2 卷積層\n",
    "在CNN中，出現了padding、stride等CNN專用的名詞。另外，傳遞在各層的資料變成具有形狀的資料(例如，三維資料)，與之前的全連接網路不同。因此，我們在這裡花多點時間，徹底介紹CNN所使用的卷積層結構。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.2.1 全連接層的問題\n",
    "前面介紹的全連接神經網路，使用了全連接層( Affine層 )。全連接層連接相鄰各層的所有神經元，可以決定任意輸出數量。<br><br>\n",
    "\n",
    "全連接層的問題在於，他會「忽略」資料的形狀。例如，輸入資料為影像，影像通常是有水平、垂直、色板方向的三維形狀。可是，輸入全連接層時，三維資料必須變成平面(一維)。以前面提過的MNIST資料集為例子，輸入影像就是(1,28,28)，1色板、垂直28像素、水平28像素的形狀，全連接層卻會把這個形狀變成一行784(28x28)個資料，做為輸入資料，輸入最初的Affine層。<br><br>\n",
    "\n",
    "然而，卷積層(Convolution層)能維持形狀。如果是影像，輸入資料可以當作三維資料來處理，對上下一層輸出同樣是三維的資料。因此，CNN(可能)可以正確了解影像等含有形狀的資料。<br><br>\n",
    "\n",
    "有時，我們會把CNN的卷積層輸出入的資料稱作**輸入特徵圖(_input feature map_)**，與**輸出特徵圖(_output feature map_)**，而本書將上面「輸出特徵圖」之專有名詞與「特徵圖」視為等意詞。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.2.2 卷積運算\n",
    "![卷積運算步驟](./img/7.2.2.PNG)<br>\n",
    "卷積運算如圖所示，針對輸入資料，以固定的間隔，一邊移動，一邊套用濾鏡視窗。在各個位置乘上濾鏡的元素與對應輸入的元素，並計算總合(這個部分也稱為稽和運算)。再將結果儲存在對應輸出的位置。在全部的位置執行這格過程，可以得到積運算的輸出。<br><br>\n",
    "\n",
    "在全連接的神經網路中，除了權重參數之外，還有偏權值。CNN的濾鏡參數對應的是前面提到的「權重」而且在CNN中，也有偏權值，下圖表式卷積運算套用濾鏡後的運算。包含偏權值的卷積運算處理流程。<br>\n",
    "![7.2.2卷積運算的偏權值](./img/7.2.2.1.PNG)<br>\n",
    "可以看到，偏權值隨時都只有一值，這個值要加在套用濾鏡後的所有元素。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 7.2.3 填補(padding)\n",
    "進行卷積處理之前，必須在輸入資料的周圍填上固定的資料(例如0)，這個動作稱作**填補( _padding_ )**，也是在卷積運算中，常用的處理。如圖所示，圖中對於大小為(4x4)的輸入資料，進行寬度為1的填補。<br>\n",
    "![7.2.3padding](./img/7.2.3.PNG)<br><br>\n",
    "\n",
    "{Note}<br>\n",
    "使用填補的理由是為了調整輸出大小。假設在大小為(4x4)的輸入資料中，套用(3x3)的濾鏡時，輸出大小變成(2x2)，輸出資料只會縮小輸入資料的2個元素。如此一來，在反覆進行卷積運算的多層網路中，就會造成問題。如果每次進行卷積運算時，空間就會縮小的話，到了某個階段，輸出大小就會變成1，而無法再進行卷積運算，使用填補就是為了避免出現這樣的狀況。將填補大小設定為1，相對於輸入大小為(4x4)，輸出大小也會保持(4x4)。因此，藉由卷積運算，可以維持固定的空間大小，將資料傳遞給下一層。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.2.4 步伐\n",
    "套用濾鏡的位置間隔稱為**步伐( _stride_ )**，若我們把步伐變大，輸出大小就會變小，但我們把填補變大，輸出大小就會變大；所以，我們要把這種關係公式化。<br><br>\n",
    "\n",
    "這裡假設輸入大小為$(H,W)$，濾鏡大小為$(FH,FW)$，輸出大小為$(OH,OW)$，填補為$P$，步伐為$S$，可以利用以下算式，計算輸出大小。<br>\n",
    "$\n",
    "OH = \\frac{H+2P-FH}{S} + 1 \\tag{7.1}\n",
    "$\n",
    "\n",
    "$\n",
    "OW = \\frac{W+2P-FW}{S} + 1 \\tag{7.1}\n",
    "$<br>\n",
    "我們來練習一題：<br>\n",
    "* 輸入大小：(28, 31)、填補：2、步伐：3、濾鏡大小：(5, 5)<br>\n",
    "\n",
    "$\n",
    "OH = \\frac{28+(2*2)-5}{3} + 1 = 10\n",
    "$<br>\n",
    "\n",
    "$\n",
    "OW = \\frac{31+(2*2)-5}{3} + 1 = 11\n",
    "$<br><br>\n",
    "\n",
    "這裡必須注意的是，我們一定要讓算式(7.1)整除，假如輸出大小無法整除(結果為小數)，那麼我們就必須採取輸出錯誤的對策。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.2.5 三維資料的卷積運算\n",
    "三維資料包含，水平、垂直方向的形狀資料，以及色板方向所形成的三維資料。圖為三維資料的卷積運算的步驟。<br>\n",
    "![三維資料的卷積運算範例](./img/7.2.5.PNG)<br>\n",
    "這裡要注意的是，濾鏡大小可以隨意設定，但是色板數與輸入資料的色板數必須同值。這個範例的濾鏡大小是(3x3)，因此色板數量只能設定成3。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.2.6 用區塊來思考\n",
    "我們也可以將三維卷積運算中的資料或濾鏡想像成立體區塊，比較容易思考，例如，色板數C、高度H、寬度W的資料形狀會寫成(C, H, W )，濾鏡則由色板數C、濾鏡高度FH、寬度FW來表示(C, FH, FW)。<br>\n",
    "![多個濾鏡的卷積運算](./img/7.2.6.jpg)<br>\n",
    "由圖可知，套用FN個濾鏡，也會產生FN個輸出的特徵圖。<br><br>\n",
    "\n",
    "在卷積運算中(和全連接層相同)，也有偏權值與權重，偏權值是每個色板只有一個資料。這裡偏權值形狀是(FN, 1, 1)，濾鏡輸出結果的形狀是(FN, OH, OW)。<br>\n",
    "![卷積運算的處理流程(加上偏權值)](./img/7.2.6.1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 7.3 池化層\n",
    "池化層是縮小垂直、水平空間的運算，如圖所示：<br>\n",
    "![池化層運算](./img/7.3.PNG)<br>\n",
    "其中，「最大池化」是用來取得每個視窗的最大值，而「平均池化」則是指，從每個視窗取出平均值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.3.1 池化層的特色\n",
    "池化層有以下特色<br>\n",
    "* 沒有學習參數<br>\n",
    "&emsp; 池化層與卷積層不同，沒有學習參數。池化層指進行從目標區域取得最大值( 或平均值 )的處理，所以沒有必需學習的參數存在。<br>\n",
    "* 色板數量不變<br>\n",
    "&emsp; 輸入資料與輸出資料的色板數量不會隨著池化算而改變，各個色板進行獨立的運算。<br>\n",
    "* 對微小位置變化很穩健<br>\n",
    "&emsp; 即使輸入資料出現小偏差，池化仍會回傳相同結果(因為只取最大或平均池化)。因此，對輸入資料的微小偏差很穩健。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 7.4 執行卷積層 / 池化層\n",
    "或許我們直覺認為執行卷積層與池化層很複雜，但其實只要使用某個「妙招」，就可以輕鬆完成，而在執行的類別中，含有forwawrd與backward等方法(在第5章-誤差反向傳播法提及過)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.4.1 四維陣列\n",
    "前面說明過，在CNN各層流動的是四維資料(FN, C, OH, OW)，使用Python執行，結果如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n",
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "x = np.random.rand(10, 1, 28, 28) # 在此隨機初始化10個色板數為1，寬高為(28x28)的資料\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[9][0].shape) #.shape是由張量最前面的元素開始拆解，拆解到最後面的元素為止\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 「妙招」\n",
    "這樣在CNN就會變成要處理四維資料，使得執行卷積運算的過程變得很複雜，其實只要使用下面要說明的im2col這個「妙招」，問題就會變得簡單。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.4.2 利用im2col展開\n",
    "執行卷積運算時，按部就班的處理，需要重疊多重for陳述式。可是這樣有點麻煩，而且在Numpy使用for陳述式，還有處理速度緩慢的缺點(在Numpy存取元素時，最好盡量別用for陳述式)。這裡不執行for陳述式，改用方便的im2col函數來進行簡單處理。<br><br>\n",
    "\n",
    "im2col是可以針對套用濾鏡(權重)，輕鬆展開輸入資料的函數。如下圖所示，對三維的輸入資料套用im2col，就會轉換成二維陣列(正確來說，是將含有批次數的四維資料轉換成二維資料)。<br>\n",
    "![7.4.2](./img/7.4.2.PNG)<br><br>\n",
    "\n",
    "im2col可以針對套用濾鏡的位置，輕易展開輸入資料。具體而言，如下圖所示，對輸入資料套用濾鏡的區域(三維區塊)往水平方向展開成1行。im2col就是對套用了濾鏡的所有位置進行這種展開處理。<br>\n",
    "![7.4.2.1](./img/7.4.2.1.PNG)<br><br>\n",
    "\n",
    "另外，上圖以易讀性為前提，把步幅設定成較大的數值，避免套用濾鏡的區域重疊。實際上，進行卷積運算時，幾乎濾鏡區域都會重疊。套用濾鏡的區域重疊時，利用im2col展開，展開後的元素數量，會變得比原本區塊的元素數量還要多。因此，使用im2col時，通常會產生占用比較多記憶體的缺點。可是，利用電腦統一計算大型陣列，有很多好處。例如，計算陣列的函式庫(線性代數的函式庫)等，可以先將陣列計算最佳化，再快速執行大型陣列的乘法運算。因此，利用還原陣列計算的方法，能有效運用線性代數函式庫。<br><br>\n",
    "\n",
    "{Note}<br>\n",
    "im2col是「image to column」的縮寫，意思是將影像轉換成陣列。在Caffe及Chainer等深度學的框架中，皆含有名為im2col的函數，執行卷積層時，可以使用各個im2col來進行處理。<br><br>\n",
    "\n",
    "利用im2col展開輸入資料，之後只要把卷積曾的濾鏡(權重)展開成1行，計算2個陣列的乘積(參考下圖)。這個部分與在全連接層Affine層進行的處理幾乎一模一樣。<br>\n",
    "![7.4.2.2](./img/7.4.2.2.PNG)<br>\n",
    "卷積運算的濾鏡處理詳細說明：往垂直方向把濾鏡展開成1行，計算以im2col展開的輸入資料與濾鏡矩陣(權重矩陣)的乘積，最後調整(reshape)輸出資料的大小。<br><br>\n",
    "\n",
    "如上圖所示，利用im2col方法輸出的結果是二維陣列，CNN的資料型態是四維陣列，為求方便處理，將二維輸出資料調整成適當形狀，以上就是卷積層的執行流程。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.4.3 執行卷積層\n",
    "本書提供了im2col函數。請把這個im2col函數想像成在進行黑箱測試(別在意執行的內容)。此外，原始檔案位於common/util.py中，只有10行程式碼。<br><br>\n",
    "\n",
    "im2col考量到「濾鏡大小」、「步幅」、「填補」，把輸入資料展開成二維陣列。接下來，我們來實際使用im2col。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from dl_ex.common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7) #im2col(input_data, filter_h, filter_w, stride=1, pad=0)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) #(9,75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) #(90,75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 解說\n",
    "這裡顯示了兩個範例。第一個批次大小為1，色板為3的7x7資料，第二個是批次大小為10，資料形狀和第1個相同的範例。分別套用im2col函數，這兩個範例，第二維的元素數量是75。這是濾鏡(色板3、大小5x5)的元素數量總和。此外，批次大小為1的時候，im2col的結果是大小為(9x75)。然而，第2個範例的批次大小為10，所以儲存了(9,75)的10倍資料(90x75)。而兩個範例的第一個維度是指，用了多少次數(多少視窗)走完其中一個權重矩陣。因此，若print(col1.shape)輸出結果為(9,75)，則代表有75個以9格視窗走完的輸出矩陣。<br><br>\n",
    "\n",
    "接下來，要用im2col執行卷積層，這裡以名稱為Convolution的類別來執行卷積層。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        N, C, H, w = x.shape\n",
    "        out_h = int(1 + 2*self.pad - FH / self.stride)\n",
    "        out_w = int(1 + 2*self.pad - FW / self.stride)\n",
    "        \n",
    "        ## 重要部分開始 ##\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_w = self.w.reshape(FN, -1).T #展開濾鏡((將C,FH,FW)的N個濾鏡結合成一個矩陣)\n",
    "        out = np.dot(col, col_w) + self.b # 與Affine層做的事情是一樣的(見7.4.2.2圖)\n",
    "        ## 重要部分結束 ##\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) #改變序列(將原本的張量的(0,1,2,3)軸，改變成(0,3,1,2)軸)\n",
    "               \n",
    "        return out\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 解說\n",
    "在執行卷積層過程，以#展開濾鏡那部分最為重要重要的部分。包含以im2col展開輸入資料，濾鏡也使用reshape展開成二維陣列，然後再計算展開後的矩陣乘積。<br><br>\n",
    "\n",
    "展開濾鏡的位置，(如圖7.4.2.2)所示，將各濾鏡的區塊展開成1行。reshape(FN, -1)設定成-1，這是reshape的方便功能之一。<br><br>\n",
    "\n",
    "接下來，我們要使進行卷積層的反向傳播，這個部分與Affine層的執行過程也有很多共通點，不過要注意到，要執行im2col的反向處理，要使用col2im的函數來因應這個問題。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 輸入資料的形狀（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, w, b, stride=1, pad=0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        N, C, H, w = x.shape\n",
    "        out_h = int(1 + 2*self.pad - FH / self.stride)\n",
    "        out_w = int(1 + 2*self.pad - FW / self.stride)\n",
    "        \n",
    "        ## 重要部分開始 ##\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_w = self.w.reshape(FN, -1).T #展開濾鏡((將C,FH,FW)的N個濾鏡結合成一個矩陣)\n",
    "        out = np.dot(col, col_w) + self.b # 與Affine層做的事情是一樣的(見7.4.2.2圖)\n",
    "        ## 重要部分結束 ##\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) #改變序列(將原本的張量的(0,1,2,3)軸，改變成(0,3,1,2)軸)\n",
    "               \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN) #將原本的N個Filter、C個色板數 reshape成N個Filter(Filter資訊含有色板數C、寬、高)\n",
    "                                                          #換句話說，就是將(FN,C,FH,FW)先transpose成(FN,FH,FW,C)，再reshape成(FH x FW x C, FN)個矩陣，如下圖示意\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7.4.4 執行池化層\n",
    "池化層的執行過程和卷積層一樣，都是使用im2col展開輸入資料。但是，池化層的色板方向是獨立的，這點與卷積層不同。具體來說，套用持化層的區域會依照色板獨立展開，如下圖所示。<br>\n",
    "![7.4.4.1](./img/7.4.4.PNG)<br><br>\n",
    "\n",
    "只要展開一次，之後再針對展開後的陣列，計算每行的最大值，調整成適當形狀即可，見下圖。<br>\n",
    "![7.4.4.2](./img/7.4.4.1.PNG)<br>\n",
    "上圖為池化層的執行過程：以灰階顯示套用持化區域內的最大值元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展開 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 最大值 (2)\n",
    "        out = np.max (col, axis = 1)\n",
    "        \n",
    "        # 調整形狀 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "池化層的執行過程如上圖所示，按造以下3個階段來執行\n",
    "* 展開輸入資料\n",
    "* 計算每列的最大值\n",
    "* 調整成適當的輸出大小\n",
    "<br>\n",
    "以上是池化層的forward處理，如這裡所示，只要將輸入資料展開城容易執行池化的形狀，後續的處理就很簡單。<br><br>\n",
    "\n",
    "至於池化層的backward處理，可以參考ReLU層執行時，使用的max反向傳播(5.5.1 ReLU層)。以下為Pooling backward的程式碼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1) #(FN, FH, FW, C)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7.5 執行CNN\n",
    "現在，我們要把卷積層與池化層組合起來，製作辨識手寫數字的CNN。執行CNN的過程如下圖所示。其網路結構是「Conv - ReLU - Pooling - Affine - ReLU - Affine - Softmax」利用名為SimpleConvNet的類別來執行。<br>\n",
    "![7.5](./img/7.5.PNG)<br>\n",
    "接下來，先檢視最簡單的CNN，SimpleConvNet。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('./dl_ex'))  #載入父目錄檔案的設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from dl_ex.common.layers import *\n",
    "from book_dl.dl_ex.common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"簡單的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 輸入資料的色板、高、寬維度（MNIST資料集的輸入為784）\n",
    "    hidden_size_list : 隱藏層(全連接層)的神經元數目（e.g. [100, 100, 100]）\n",
    "    output_size : 輸出層(全連接層)的神經元數目（MNIST資料集的輸出為10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 初始化時的權重標準差（e.g. 0.01）\n",
    "        如果指定'relu'或'he'，請設置“He的初始值”\n",
    "        如果指定'sigmoid'或'xavier'，請設置Xavier的初始值\n",
    "        \n",
    "    卷積層的超參數是以名為conv_parm的字典型態來提供。\n",
    "    例如{'filter_num':30, ... , 'stride':1}，將所需的超參數儲存在裡面\n",
    "    \n",
    "    \"\"\"   \n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))  \n",
    "    \n",
    "    ###\n",
    "    # 從字典型態中，取出當作初期化引數的卷積層超參數(為了方便後續使用)\n",
    "    # 再計算卷積層以及池化層的輸出大小\n",
    "    ###\n",
    "    \n",
    "        # 權重參數初始化的部分\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size) \n",
    "                            # 意思是從標準常態抽樣，輸出大小形狀為 0.01 * [30, 10, 5, 5] 的輸出資料        \n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "    ###\n",
    "    # 學習過程需要的參數是，第1層的卷積層，以及其餘2個全連接層的權重與偏權值\n",
    "    # 這些參數存在實例變數的parms W1, W2, W3字典裡 \n",
    "    # 其中，字典型態的key是第1層的卷積層權重W1與b1\n",
    "    # 同樣分別利用第2層與第3層，全連接層的權重與偏權值W2, b2, W3, b3來儲存參數\n",
    "    ###\n",
    "        \n",
    "        # 產生必要的層級\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    ###\n",
    "    # 從前面開始依序在含有字典型態(OrdereDict)的layers中，增加層數\n",
    "    # 只有最後的SoftmaxWithLoss層加上另一個變數，lastLayer\n",
    "    # 以上就是SimpleConvNet初始化執行的處理\n",
    "    # 只要完成初始化，就可以按造以下所示\n",
    "    # 執行用來推論的predict方法與計算損失函數的loss方法\n",
    "    ###\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"找到損失函數\n",
    "        參數x是輸入資料，t是訓練標籤\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    ###\n",
    "    # 用來推論的predict方法，會以for迴圈的方式，從頭開始依序呼叫追加的層級\n",
    "    # 然後把結果傳遞給下一層\n",
    "    # 在計算損失函數的 loss 中\n",
    "    # 除了用predict方法進行的forward處理之外\n",
    "    # 最後也會以SoftmaxWithLoss層的forward進行推論結果的處理\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"找到斜率（誤差反向傳播法）\n",
    "        接著是利用誤差反向傳播法，計算梯度，過程如下\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 輸入資料\n",
    "        t : 訓練標籤\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        字典變量與每層的實例變數\n",
    "            grads['W1']、grads['W2']、...是每層的權重\n",
    "            grads['b1']、grads['b2']、...是每層的偏權值\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    ###\n",
    "    # 利用誤差反向傳播法(反向傳播)計算出參數的梯度\n",
    "    # 再連續進行正向傳播與反向傳播\n",
    "    # 最後，在grads字典中，儲存各權重參數的梯度\n",
    "    ###\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2996493663945943\n",
      "=== epoch:1, train acc:0.376, test acc:0.361 ===\n",
      "train loss:2.294921520822249\n",
      "train loss:2.289394393284318\n",
      "train loss:2.2820711827277504\n",
      "train loss:2.280585984079891\n",
      "train loss:2.2635626668865667\n",
      "train loss:2.2335916290307862\n",
      "train loss:2.2294397231162857\n",
      "train loss:2.1959707849634724\n",
      "train loss:2.2004110835814035\n",
      "train loss:2.1526137846936204\n",
      "train loss:2.1227469299048427\n",
      "train loss:2.0531861962768265\n",
      "train loss:2.0166600646815858\n",
      "train loss:1.976780358443128\n",
      "train loss:1.8992876684999367\n",
      "train loss:1.8067148994959283\n",
      "train loss:1.721159816257835\n",
      "train loss:1.6549225775863072\n",
      "train loss:1.5961485252125138\n",
      "train loss:1.5147136589816415\n",
      "train loss:1.439924508405349\n",
      "train loss:1.3068861455625624\n",
      "train loss:1.2715118859445582\n",
      "train loss:1.0914554993916175\n",
      "train loss:1.2001290599027874\n",
      "train loss:1.1106436855945423\n",
      "train loss:1.1569485216064979\n",
      "train loss:1.0522237384379973\n",
      "train loss:1.0220188966414554\n",
      "train loss:0.9752015597093276\n",
      "train loss:1.0750134623743013\n",
      "train loss:0.8166946843187028\n",
      "train loss:0.9772699266973798\n",
      "train loss:0.7867704357515312\n",
      "train loss:0.6716718956628119\n",
      "train loss:0.7129535984863279\n",
      "train loss:0.7463010612566499\n",
      "train loss:0.7475551857425274\n",
      "train loss:0.7523907328432325\n",
      "train loss:0.6626741653953238\n",
      "train loss:0.6475406363980754\n",
      "train loss:0.5678866111938363\n",
      "train loss:0.9032098010391603\n",
      "train loss:0.7475001032889268\n",
      "train loss:0.7229912193825355\n",
      "train loss:0.7429012184749\n",
      "train loss:0.5908048891411006\n",
      "train loss:0.5672471264306672\n",
      "train loss:0.544384405872551\n",
      "train loss:0.6435309898155672\n",
      "train loss:0.4581136940086757\n",
      "train loss:0.5604280729843284\n",
      "train loss:0.4879277938094601\n",
      "train loss:0.5246670840759127\n",
      "train loss:0.3599483114316939\n",
      "train loss:0.47413158281001644\n",
      "train loss:0.5027257285456588\n",
      "train loss:0.4631923126407773\n",
      "train loss:0.7251237339418364\n",
      "train loss:0.5269865900458707\n",
      "train loss:0.5837973180148908\n",
      "train loss:0.4555269419744259\n",
      "train loss:0.3449200024609738\n",
      "train loss:0.37786655467622393\n",
      "train loss:0.49547159411199365\n",
      "train loss:0.5299606718609542\n",
      "train loss:0.3442261231486335\n",
      "train loss:0.3695025819958472\n",
      "train loss:0.4199151763709023\n",
      "train loss:0.38379079166633334\n",
      "train loss:0.3834579166609584\n",
      "train loss:0.4322612374177026\n",
      "train loss:0.4986474706508956\n",
      "train loss:0.8504709613181902\n",
      "train loss:0.4728230148157921\n",
      "train loss:0.5639472087708092\n",
      "train loss:0.4210103157002216\n",
      "train loss:0.5334466647652762\n",
      "train loss:0.510918212721537\n",
      "train loss:0.2928310550883191\n",
      "train loss:0.5091597976505587\n",
      "train loss:0.3293434616571264\n",
      "train loss:0.30598865021026084\n",
      "train loss:0.4207957986128278\n",
      "train loss:0.37152082242196977\n",
      "train loss:0.3242651830929056\n",
      "train loss:0.36581422193263063\n",
      "train loss:0.33842416508237194\n",
      "train loss:0.34971437941857253\n",
      "train loss:0.5188680992456083\n",
      "train loss:0.34534288561624366\n",
      "train loss:0.5347419833770225\n",
      "train loss:0.3809302442901348\n",
      "train loss:0.41918630501148596\n",
      "train loss:0.48965373292256004\n",
      "train loss:0.40238589435007094\n",
      "train loss:0.3728405218140292\n",
      "train loss:0.3468584716224526\n",
      "train loss:0.2859679447465322\n",
      "train loss:0.6098165824631695\n",
      "train loss:0.33539097863833767\n",
      "train loss:0.4188921870205637\n",
      "train loss:0.3285498902022593\n",
      "train loss:0.38479932655325916\n",
      "train loss:0.32179065666763995\n",
      "train loss:0.2773346603554311\n",
      "train loss:0.6793437508170626\n",
      "train loss:0.47924143385245843\n",
      "train loss:0.3723484137766863\n",
      "train loss:0.27471564434559387\n",
      "train loss:0.4105269473391803\n",
      "train loss:0.39409604630400485\n",
      "train loss:0.38429462129797487\n",
      "train loss:0.39333237428389856\n",
      "train loss:0.3361076992402531\n",
      "train loss:0.22859061709180234\n",
      "train loss:0.32518436120497796\n",
      "train loss:0.3535587416681175\n",
      "train loss:0.32351778610910004\n",
      "train loss:0.1969640774385389\n",
      "train loss:0.22234436732682636\n",
      "train loss:0.4609469665181192\n",
      "train loss:0.27980775247441253\n",
      "train loss:0.36044106418226685\n",
      "train loss:0.38213308131808155\n",
      "train loss:0.30853737433501505\n",
      "train loss:0.2889193044195631\n",
      "train loss:0.2964077028523927\n",
      "train loss:0.2623029309127795\n",
      "train loss:0.24213020991269626\n",
      "train loss:0.38827370589570515\n",
      "train loss:0.2672201963860089\n",
      "train loss:0.24698088499273269\n",
      "train loss:0.4676107439675479\n",
      "train loss:0.28037553040935725\n",
      "train loss:0.2885402071910329\n",
      "train loss:0.2631789725801628\n",
      "train loss:0.4201845038457521\n",
      "train loss:0.3004770199102159\n",
      "train loss:0.35398348186947637\n",
      "train loss:0.2628868272498939\n",
      "train loss:0.30584634195444205\n",
      "train loss:0.309095421326004\n",
      "train loss:0.37315639691577396\n",
      "train loss:0.35808582937207106\n",
      "train loss:0.24128318933546938\n",
      "train loss:0.3083891656805208\n",
      "train loss:0.25725479427652337\n",
      "train loss:0.3635125050418597\n",
      "train loss:0.4776771325364595\n",
      "train loss:0.2904636934977044\n",
      "train loss:0.27342527689812035\n",
      "train loss:0.22822057402387141\n",
      "train loss:0.40362004912334376\n",
      "train loss:0.43541050170022055\n",
      "train loss:0.27389809878605864\n",
      "train loss:0.3350411577785834\n",
      "train loss:0.20417642512724757\n",
      "train loss:0.2129756677397731\n",
      "train loss:0.34509582335475547\n",
      "train loss:0.3866274188921205\n",
      "train loss:0.29549745563677887\n",
      "train loss:0.26978375716758507\n",
      "train loss:0.2934396052131266\n",
      "train loss:0.22901574499403357\n",
      "train loss:0.4442821053209023\n",
      "train loss:0.29769958497857835\n",
      "train loss:0.21984538621558905\n",
      "train loss:0.22582930188307493\n",
      "train loss:0.17408262916434888\n",
      "train loss:0.4587294273826915\n",
      "train loss:0.2667194676695092\n",
      "train loss:0.2714615209558544\n",
      "train loss:0.16021559676329233\n",
      "train loss:0.235074811666188\n",
      "train loss:0.3484608560175965\n",
      "train loss:0.3862515359973466\n",
      "train loss:0.3007307125636448\n",
      "train loss:0.24385546263977434\n",
      "train loss:0.29545845001793425\n",
      "train loss:0.2726237869455441\n",
      "train loss:0.20352795737008567\n",
      "train loss:0.3650359184641952\n",
      "train loss:0.17457624923885603\n",
      "train loss:0.32126253002917254\n",
      "train loss:0.2205160470597027\n",
      "train loss:0.19333604946979768\n",
      "train loss:0.30615725344889977\n",
      "train loss:0.17666212062196213\n",
      "train loss:0.4417175953869368\n",
      "train loss:0.359892459024272\n",
      "train loss:0.36588181026183386\n",
      "train loss:0.4321583297894102\n",
      "train loss:0.3380116795529098\n",
      "train loss:0.2592560181395163\n",
      "train loss:0.24795105588134497\n",
      "train loss:0.3881418027295126\n",
      "train loss:0.3001267828900575\n",
      "train loss:0.23635311879869775\n",
      "train loss:0.22415593757948787\n",
      "train loss:0.20907986269435905\n",
      "train loss:0.18404459253586838\n",
      "train loss:0.4121641809913015\n",
      "train loss:0.23985349109484136\n",
      "train loss:0.2794419118330428\n",
      "train loss:0.2566387672745026\n",
      "train loss:0.3043741767792732\n",
      "train loss:0.2578132665293045\n",
      "train loss:0.3331518291290162\n",
      "train loss:0.21263849025116713\n",
      "train loss:0.24464740961675127\n",
      "train loss:0.2675358832656223\n",
      "train loss:0.28801439677428803\n",
      "train loss:0.32104379197644783\n",
      "train loss:0.260275139682863\n",
      "train loss:0.24134210823600938\n",
      "train loss:0.17068036825041052\n",
      "train loss:0.2488516032887203\n",
      "train loss:0.31180566654471575\n",
      "train loss:0.33600176515762165\n",
      "train loss:0.2566389979813359\n",
      "train loss:0.24943984944824021\n",
      "train loss:0.22820930136020245\n",
      "train loss:0.21665707698033695\n",
      "train loss:0.20191649567043743\n",
      "train loss:0.2895817259974466\n",
      "train loss:0.14236931982349108\n",
      "train loss:0.15646225373212105\n",
      "train loss:0.21861276799726098\n",
      "train loss:0.16226815933825253\n",
      "train loss:0.2842405629657195\n",
      "train loss:0.2519055189312259\n",
      "train loss:0.1330034305729332\n",
      "train loss:0.36383835972767886\n",
      "train loss:0.26550919020346675\n",
      "train loss:0.12022612060573572\n",
      "train loss:0.158722210791661\n",
      "train loss:0.1592285434711317\n",
      "train loss:0.28395917389850284\n",
      "train loss:0.18907611542872155\n",
      "train loss:0.15327540691865907\n",
      "train loss:0.26039813612784884\n",
      "train loss:0.2559295626309529\n",
      "train loss:0.37182390797130616\n",
      "train loss:0.2780136154275008\n",
      "train loss:0.39018809723294384\n",
      "train loss:0.17418768510014002\n",
      "train loss:0.18525326195229078\n",
      "train loss:0.16819740583891785\n",
      "train loss:0.262816610890617\n",
      "train loss:0.2086456654284495\n",
      "train loss:0.4490453571361218\n",
      "train loss:0.18032459095079975\n",
      "train loss:0.15884017620074542\n",
      "train loss:0.21118969314617125\n",
      "train loss:0.3494710170794367\n",
      "train loss:0.25066581656781284\n",
      "train loss:0.2246226942371095\n",
      "train loss:0.22660232268053895\n",
      "train loss:0.1667557326826505\n",
      "train loss:0.1897026611799368\n",
      "train loss:0.18367506348283247\n",
      "train loss:0.11288210313177469\n",
      "train loss:0.19938674654545935\n",
      "train loss:0.2801189421389974\n",
      "train loss:0.2416980934223118\n",
      "train loss:0.19464947107942904\n",
      "train loss:0.33633640838206597\n",
      "train loss:0.09096347137611203\n",
      "train loss:0.11938271758797622\n",
      "train loss:0.17298132778837577\n",
      "train loss:0.1951727277678415\n",
      "train loss:0.24775117231591026\n",
      "train loss:0.39096265545704734\n",
      "train loss:0.20768477297910207\n",
      "train loss:0.1544749774428457\n",
      "train loss:0.1852391654048274\n",
      "train loss:0.3193771513873758\n",
      "train loss:0.24445611835228326\n",
      "train loss:0.1475538916846768\n",
      "train loss:0.0687405094527998\n",
      "train loss:0.16299380779471048\n",
      "train loss:0.30624332896906453\n",
      "train loss:0.1778650431334761\n",
      "train loss:0.18646907729062295\n",
      "train loss:0.3091447029508212\n",
      "train loss:0.18590143890502503\n",
      "train loss:0.17480458679775687\n",
      "train loss:0.19431036549030317\n",
      "train loss:0.19685479429559613\n",
      "train loss:0.0883169056767889\n",
      "train loss:0.22945826488932625\n",
      "train loss:0.16524481753535228\n",
      "train loss:0.3455055546605255\n",
      "train loss:0.20580693103054812\n",
      "train loss:0.18861197051935427\n",
      "train loss:0.12588079222285445\n",
      "train loss:0.20835558031075568\n",
      "train loss:0.3250554437892816\n",
      "train loss:0.22026338767755835\n",
      "train loss:0.12475996046963689\n",
      "train loss:0.2562356058599917\n",
      "train loss:0.26541362907244603\n",
      "train loss:0.2260839549781601\n",
      "train loss:0.3291584957583863\n",
      "train loss:0.1776210158364449\n",
      "train loss:0.13333600015330238\n",
      "train loss:0.18065935806932443\n",
      "train loss:0.2310363451108611\n",
      "train loss:0.223389238676444\n",
      "train loss:0.1802895882537429\n",
      "train loss:0.11970381243067867\n",
      "train loss:0.13185854356116258\n",
      "train loss:0.2534567498376608\n",
      "train loss:0.19441875097474806\n",
      "train loss:0.18358740542749136\n",
      "train loss:0.16439288967154297\n",
      "train loss:0.27741043869027493\n",
      "train loss:0.3451455677399901\n",
      "train loss:0.17366647808462957\n",
      "train loss:0.16006702752529547\n",
      "train loss:0.21862668133035595\n",
      "train loss:0.20738398934816096\n",
      "train loss:0.1843941951706202\n",
      "train loss:0.1870965044704749\n",
      "train loss:0.12552617712928632\n",
      "train loss:0.1482563419748843\n",
      "train loss:0.2549640361473817\n",
      "train loss:0.11418072321415773\n",
      "train loss:0.2506715755225667\n",
      "train loss:0.28511872395018373\n",
      "train loss:0.06434129923118474\n",
      "train loss:0.17970496416328988\n",
      "train loss:0.0759687941337846\n",
      "train loss:0.13613492651819178\n",
      "train loss:0.14497048636031537\n",
      "train loss:0.12087553222391001\n",
      "train loss:0.14309233370829383\n",
      "train loss:0.17635718670825667\n",
      "train loss:0.1887563744558216\n",
      "train loss:0.11100745744848294\n",
      "train loss:0.1511233305230023\n",
      "train loss:0.14129213496041695\n",
      "train loss:0.1492291882269538\n",
      "train loss:0.2222558665770995\n",
      "train loss:0.10033871459567101\n",
      "train loss:0.10935536907674485\n",
      "train loss:0.10913252768582744\n",
      "train loss:0.15218494458973\n",
      "train loss:0.19720564663487872\n",
      "train loss:0.10467411507754461\n",
      "train loss:0.13717384063428467\n",
      "train loss:0.20851788851724534\n",
      "train loss:0.1123624035509918\n",
      "train loss:0.05798967307515663\n",
      "train loss:0.18956763796155016\n",
      "train loss:0.20520760112560854\n",
      "train loss:0.10626013038396369\n",
      "train loss:0.1797723178033392\n",
      "train loss:0.12738655970815527\n",
      "train loss:0.17670336786377508\n",
      "train loss:0.07655785555425465\n",
      "train loss:0.1448223779100225\n",
      "train loss:0.196752961219917\n",
      "train loss:0.1638547279110133\n",
      "train loss:0.2123882740277912\n",
      "train loss:0.1361461819685551\n",
      "train loss:0.1241107732639426\n",
      "train loss:0.07882466785155302\n",
      "train loss:0.09149470537155663\n",
      "train loss:0.17547558399854288\n",
      "train loss:0.10423806097508102\n",
      "train loss:0.13601116575997887\n",
      "train loss:0.21540939663969333\n",
      "train loss:0.2098005251885129\n",
      "train loss:0.24813772017210173\n",
      "train loss:0.3511875783750195\n",
      "train loss:0.07699828447365933\n",
      "train loss:0.14997607548172082\n",
      "train loss:0.18707194700139254\n",
      "train loss:0.19679581577170144\n",
      "train loss:0.11857413341693133\n",
      "train loss:0.2858261837748707\n",
      "train loss:0.15094884517415902\n",
      "train loss:0.18399713788077743\n",
      "train loss:0.10594299658519736\n",
      "train loss:0.14655179974054705\n",
      "train loss:0.08428944995046131\n",
      "train loss:0.17003133298360246\n",
      "train loss:0.1332594376285699\n",
      "train loss:0.16769666706961917\n",
      "train loss:0.20350614015797996\n",
      "train loss:0.16104270925544706\n",
      "train loss:0.16436122161358438\n",
      "train loss:0.17604674577217316\n",
      "train loss:0.21775918621661616\n",
      "train loss:0.2711473412880925\n",
      "train loss:0.21753213591570372\n",
      "train loss:0.12933618395432547\n",
      "train loss:0.26049632408157447\n",
      "train loss:0.16356986961449038\n",
      "train loss:0.20059053533240526\n",
      "train loss:0.14216609914702708\n",
      "train loss:0.22268337033042798\n",
      "train loss:0.1907497284306621\n",
      "train loss:0.13639163917607955\n",
      "train loss:0.14667423284019113\n",
      "train loss:0.14795985809931456\n",
      "train loss:0.1305526236359619\n",
      "train loss:0.1946767218773101\n",
      "train loss:0.19541994111810104\n",
      "train loss:0.11912576314713189\n",
      "train loss:0.11022292016437504\n",
      "train loss:0.1006849517257153\n",
      "train loss:0.09840295396110312\n",
      "train loss:0.17476657542925034\n",
      "train loss:0.17358651771518732\n",
      "train loss:0.048972575580923286\n",
      "train loss:0.23389871411098523\n",
      "train loss:0.10815250756404976\n",
      "train loss:0.09101288442283562\n",
      "train loss:0.1921391880026407\n",
      "train loss:0.19684228789686176\n",
      "train loss:0.08280247880482536\n",
      "train loss:0.11847888064198632\n",
      "train loss:0.3475201623568875\n",
      "train loss:0.129968624117152\n",
      "train loss:0.17244540116194285\n",
      "train loss:0.11852748138162583\n",
      "train loss:0.08096197634058008\n",
      "train loss:0.08532552514721443\n",
      "train loss:0.22278866625421467\n",
      "train loss:0.06978054308757972\n",
      "train loss:0.13509483865526123\n",
      "train loss:0.09733434958596807\n",
      "train loss:0.23151335969134165\n",
      "train loss:0.08568021469262087\n",
      "train loss:0.15895617470722925\n",
      "train loss:0.0846836423537983\n",
      "train loss:0.11478047811248537\n",
      "train loss:0.17730961088552238\n",
      "train loss:0.1432061221773285\n",
      "train loss:0.10845589965194959\n",
      "train loss:0.10202503767915364\n",
      "train loss:0.20697764910297664\n",
      "train loss:0.07352412220855567\n",
      "train loss:0.0767528342504314\n",
      "train loss:0.13969906011761782\n",
      "train loss:0.21830307418436262\n",
      "train loss:0.1109906223222202\n",
      "train loss:0.1342084967775485\n",
      "train loss:0.22813597465002047\n",
      "train loss:0.182102751643088\n",
      "train loss:0.10925955047549943\n",
      "train loss:0.15091459815977512\n",
      "train loss:0.06626098640521141\n",
      "train loss:0.1358679273398631\n",
      "train loss:0.17068888193711992\n",
      "train loss:0.10638305252625287\n",
      "train loss:0.10779794183448889\n",
      "train loss:0.1760653554830379\n",
      "train loss:0.1027343703270333\n",
      "train loss:0.22701678889767368\n",
      "train loss:0.13809880373975936\n",
      "train loss:0.09185249315702133\n",
      "train loss:0.18942136573803534\n",
      "train loss:0.13953576089438094\n",
      "train loss:0.09574384489477836\n",
      "train loss:0.1876862412633883\n",
      "train loss:0.12790078872883304\n",
      "train loss:0.19477981028770905\n",
      "train loss:0.15937538245374017\n",
      "train loss:0.10770375793909676\n",
      "train loss:0.11594900546817703\n",
      "train loss:0.07908168950327607\n",
      "train loss:0.10850462859832204\n",
      "train loss:0.19485435069056956\n",
      "train loss:0.07377429856771178\n",
      "train loss:0.17785343504009507\n",
      "train loss:0.11944681620511251\n",
      "train loss:0.15118201266492323\n",
      "train loss:0.1645975592358702\n",
      "train loss:0.106612451212048\n",
      "train loss:0.1359730194530315\n",
      "train loss:0.05981474876739216\n",
      "train loss:0.15023073109184742\n",
      "train loss:0.08508033157099881\n",
      "train loss:0.08869627916829115\n",
      "train loss:0.12342802132769615\n",
      "train loss:0.20909034229945697\n",
      "train loss:0.0906559863989213\n",
      "train loss:0.12439687508847752\n",
      "train loss:0.10431535320351265\n",
      "train loss:0.12923074442589486\n",
      "train loss:0.2688883146341727\n",
      "train loss:0.18954991669009844\n",
      "train loss:0.12855053423139837\n",
      "train loss:0.13334069039322308\n",
      "train loss:0.12969064549841267\n",
      "train loss:0.25790492244909635\n",
      "train loss:0.13131666531790734\n",
      "train loss:0.10568451324091764\n",
      "train loss:0.21811972092562446\n",
      "train loss:0.1439629483421669\n",
      "train loss:0.11703270673279834\n",
      "train loss:0.13295808080834198\n",
      "train loss:0.14452233552718277\n",
      "train loss:0.1513889401904623\n",
      "train loss:0.06976525543451322\n",
      "train loss:0.16867584940632038\n",
      "train loss:0.06379696287420866\n",
      "train loss:0.10060745691135471\n",
      "train loss:0.12904752162953723\n",
      "train loss:0.07588340080377833\n",
      "train loss:0.07659033317728445\n",
      "train loss:0.07987048879207849\n",
      "train loss:0.10420329533782505\n",
      "train loss:0.093218505333511\n",
      "train loss:0.05577130366743829\n",
      "train loss:0.0725577386945534\n",
      "train loss:0.17627999584342752\n",
      "train loss:0.12108838166576938\n",
      "train loss:0.0855112487526784\n",
      "train loss:0.1729923094539206\n",
      "train loss:0.1776087899741872\n",
      "train loss:0.057756618368320876\n",
      "train loss:0.05182272300140589\n",
      "train loss:0.2234819071984319\n",
      "train loss:0.1147748715286409\n",
      "train loss:0.13678978662042826\n",
      "train loss:0.08561181216131244\n",
      "train loss:0.16686322862773273\n",
      "train loss:0.3465764995119537\n",
      "train loss:0.058156884297799534\n",
      "train loss:0.08974123985591437\n",
      "train loss:0.12512548708398039\n",
      "train loss:0.13558881109474338\n",
      "train loss:0.11940978191832897\n",
      "train loss:0.07497896642007414\n",
      "train loss:0.11774308081189643\n",
      "train loss:0.05458186645846077\n",
      "train loss:0.11235194061578734\n",
      "train loss:0.05311191118007242\n",
      "train loss:0.08334388345891221\n",
      "train loss:0.05836388442532074\n",
      "train loss:0.3001548946872517\n",
      "train loss:0.22811477738513528\n",
      "train loss:0.11131496453193902\n",
      "train loss:0.15129350906250863\n",
      "train loss:0.18036058580626255\n",
      "train loss:0.053986634859702363\n",
      "train loss:0.12438968245429598\n",
      "train loss:0.14503993199517626\n",
      "train loss:0.17299856884265388\n",
      "train loss:0.14762507406489778\n",
      "train loss:0.129028541028677\n",
      "train loss:0.12929284750172\n",
      "train loss:0.07695800327899385\n",
      "train loss:0.08887232474284007\n",
      "train loss:0.061114383814271804\n",
      "train loss:0.13818613827498535\n",
      "train loss:0.16825354890622843\n",
      "train loss:0.13581074923034378\n",
      "train loss:0.10875630606782494\n",
      "train loss:0.05658776640765518\n",
      "train loss:0.1432085498041541\n",
      "train loss:0.1297605681427598\n",
      "train loss:0.23177248074548615\n",
      "train loss:0.09603367591993256\n",
      "train loss:0.17061326143081232\n",
      "train loss:0.09620462519282671\n",
      "train loss:0.12217233379834354\n",
      "train loss:0.10569101356610884\n",
      "train loss:0.07767147165764979\n",
      "train loss:0.1382282244685143\n",
      "train loss:0.07173037554556633\n",
      "train loss:0.21112675677446757\n",
      "train loss:0.197889674707865\n",
      "train loss:0.06690292184646401\n",
      "train loss:0.10465484708819567\n",
      "train loss:0.08709921133800609\n",
      "train loss:0.16664033193319783\n",
      "train loss:0.03417744282584609\n",
      "train loss:0.09973617655990377\n",
      "train loss:0.15911622240940568\n",
      "train loss:0.03776609726288347\n",
      "train loss:0.06885146861046162\n",
      "train loss:0.17252592315154022\n",
      "train loss:0.2259294812052593\n",
      "train loss:0.035648146376586726\n",
      "train loss:0.09008213605314966\n",
      "train loss:0.19601238574422303\n",
      "train loss:0.09027851161320079\n",
      "train loss:0.17236034216091414\n",
      "train loss:0.14008602745925364\n",
      "train loss:0.2322127274670817\n",
      "train loss:0.16150603847980421\n",
      "train loss:0.14007989405111473\n",
      "train loss:0.13789539323834565\n",
      "train loss:0.19329748615742604\n",
      "=== epoch:2, train acc:0.96, test acc:0.965 ===\n",
      "train loss:0.18005167376911207\n",
      "train loss:0.29553509435177144\n",
      "train loss:0.16306799027523486\n",
      "train loss:0.09121991173117047\n",
      "train loss:0.09944011620808832\n",
      "train loss:0.08927805148183023\n",
      "train loss:0.07427614185296975\n",
      "train loss:0.12860670561642004\n",
      "train loss:0.11994855827280405\n",
      "train loss:0.09062381704315582\n",
      "train loss:0.1102827482311053\n",
      "train loss:0.08494479116523068\n",
      "train loss:0.11368433936223817\n",
      "train loss:0.07668397936352488\n",
      "train loss:0.1150977116647543\n",
      "train loss:0.05592744377787359\n",
      "train loss:0.07364757070842429\n",
      "train loss:0.06846502963423924\n",
      "train loss:0.10091471944953521\n",
      "train loss:0.1738631505986076\n",
      "train loss:0.10250900079425061\n",
      "train loss:0.051288238124815796\n",
      "train loss:0.08128334341282567\n",
      "train loss:0.11359863949695764\n",
      "train loss:0.10601497605209335\n",
      "train loss:0.09540219814600968\n",
      "train loss:0.19913069356304716\n",
      "train loss:0.12297282894366557\n",
      "train loss:0.04416140507056429\n",
      "train loss:0.11375839153580572\n",
      "train loss:0.09757362290578918\n",
      "train loss:0.06573988884222555\n",
      "train loss:0.048137425298459234\n",
      "train loss:0.17776045066631535\n",
      "train loss:0.10206817077163899\n",
      "train loss:0.14834930629703075\n",
      "train loss:0.15283392858014452\n",
      "train loss:0.11753539858002864\n",
      "train loss:0.1245377252577269\n",
      "train loss:0.09593413452414193\n",
      "train loss:0.04013296825696791\n",
      "train loss:0.1513325916152914\n",
      "train loss:0.05273910161450956\n",
      "train loss:0.10143605842234195\n",
      "train loss:0.06928825421668744\n",
      "train loss:0.0821167152784752\n",
      "train loss:0.06323602278951124\n",
      "train loss:0.1485739957131603\n",
      "train loss:0.07646493644949019\n",
      "train loss:0.1650003084584618\n",
      "train loss:0.06364669963226295\n",
      "train loss:0.12850040911355262\n",
      "train loss:0.12192832641491601\n",
      "train loss:0.08375657149846512\n",
      "train loss:0.09464917145395617\n",
      "train loss:0.094952573776658\n",
      "train loss:0.14677763776064823\n",
      "train loss:0.09292791964046704\n",
      "train loss:0.06759963539561735\n",
      "train loss:0.07738517192475224\n",
      "train loss:0.037050329031851674\n",
      "train loss:0.16062274914908803\n",
      "train loss:0.05616706482971803\n",
      "train loss:0.09674510999069415\n",
      "train loss:0.052525870043108165\n",
      "train loss:0.10376019366053468\n",
      "train loss:0.07171968426582613\n",
      "train loss:0.04023554584402477\n",
      "train loss:0.08409256943566853\n",
      "train loss:0.13759991769339944\n",
      "train loss:0.11298959253645037\n",
      "train loss:0.09252308113737508\n",
      "train loss:0.09629006990901283\n",
      "train loss:0.12152284692291397\n",
      "train loss:0.10359138679072988\n",
      "train loss:0.14295363208055478\n",
      "train loss:0.1097581134408196\n",
      "train loss:0.05829836906556752\n",
      "train loss:0.06281161508410973\n",
      "train loss:0.08426302268858411\n",
      "train loss:0.06278230646246548\n",
      "train loss:0.06405573230108236\n",
      "train loss:0.08433412034011095\n",
      "train loss:0.1356447459675987\n",
      "train loss:0.10504405233612644\n",
      "train loss:0.12341982633083648\n",
      "train loss:0.09135822217644149\n",
      "train loss:0.1305514680083987\n",
      "train loss:0.0409616925002556\n",
      "train loss:0.15356580854995736\n",
      "train loss:0.10978320860794626\n",
      "train loss:0.08375882118746754\n",
      "train loss:0.1652502636313881\n",
      "train loss:0.2344176010528902\n",
      "train loss:0.07339072564268338\n",
      "train loss:0.054131147716537355\n",
      "train loss:0.12555619273032856\n",
      "train loss:0.1099447460551548\n",
      "train loss:0.1353518282148818\n",
      "train loss:0.12299057931393204\n",
      "train loss:0.10616373531839782\n",
      "train loss:0.11321929075288739\n",
      "train loss:0.049092262158037865\n",
      "train loss:0.06204927692679043\n",
      "train loss:0.10939569253402391\n",
      "train loss:0.07133988248181752\n",
      "train loss:0.09909637925856167\n",
      "train loss:0.09626974984076563\n",
      "train loss:0.04815703758561762\n",
      "train loss:0.06670561824497927\n",
      "train loss:0.14969690209122097\n",
      "train loss:0.07053220551443563\n",
      "train loss:0.0704402189947672\n",
      "train loss:0.14748395549466892\n",
      "train loss:0.06992361790612797\n",
      "train loss:0.21480782844077706\n",
      "train loss:0.10364688717477474\n",
      "train loss:0.05384928936544398\n",
      "train loss:0.0816772170944839\n",
      "train loss:0.06766766744306806\n",
      "train loss:0.07119195676699598\n",
      "train loss:0.042502957207203124\n",
      "train loss:0.050280328027582366\n",
      "train loss:0.09325397006271731\n",
      "train loss:0.07531200026711601\n",
      "train loss:0.1369986475940607\n",
      "train loss:0.056902070899831514\n",
      "train loss:0.04410696607717132\n",
      "train loss:0.17467632068638608\n",
      "train loss:0.20384976085595938\n",
      "train loss:0.16819945055547897\n",
      "train loss:0.11464584286188806\n",
      "train loss:0.09785454702840585\n",
      "train loss:0.047381888327894764\n",
      "train loss:0.12340521090533381\n",
      "train loss:0.05205972659347723\n",
      "train loss:0.03966967310297488\n",
      "train loss:0.056970331393934634\n",
      "train loss:0.05984850550605942\n",
      "train loss:0.12105373196947124\n",
      "train loss:0.051830547521175205\n",
      "train loss:0.0780004956188525\n",
      "train loss:0.1096928566112055\n",
      "train loss:0.11822844453664075\n",
      "train loss:0.10144579570177767\n",
      "train loss:0.07039505390262027\n",
      "train loss:0.053637614447915524\n",
      "train loss:0.07893636833281423\n",
      "train loss:0.15529310810732874\n",
      "train loss:0.11270931108055315\n",
      "train loss:0.15881962278349593\n",
      "train loss:0.08795798572216071\n",
      "train loss:0.047015806809414966\n",
      "train loss:0.1074724582823965\n",
      "train loss:0.0930179742988883\n",
      "train loss:0.0486213480380144\n",
      "train loss:0.12457231930730045\n",
      "train loss:0.03731844426729782\n",
      "train loss:0.12493441035970293\n",
      "train loss:0.07683878252547247\n",
      "train loss:0.24498972095587157\n",
      "train loss:0.1033554569567961\n",
      "train loss:0.08584338671434293\n",
      "train loss:0.08663296087477722\n",
      "train loss:0.14238799138148817\n",
      "train loss:0.07559804784160527\n",
      "train loss:0.0930660424805419\n",
      "train loss:0.06556278371778114\n",
      "train loss:0.1254804037570848\n",
      "train loss:0.11763975222076589\n",
      "train loss:0.09655747200394195\n",
      "train loss:0.10819401867845435\n",
      "train loss:0.06389507810421297\n",
      "train loss:0.05454256942955435\n",
      "train loss:0.08178257998962409\n",
      "train loss:0.10588853219228053\n",
      "train loss:0.10383831954939375\n",
      "train loss:0.03959817942855655\n",
      "train loss:0.11447640623092761\n",
      "train loss:0.08709656731329832\n",
      "train loss:0.10495162772298043\n",
      "train loss:0.05667782124436841\n",
      "train loss:0.02758384107710863\n",
      "train loss:0.07363725142548141\n",
      "train loss:0.04693845095702432\n",
      "train loss:0.0666283416434855\n",
      "train loss:0.14858518587776054\n",
      "train loss:0.08449046545673332\n",
      "train loss:0.2699148558029888\n",
      "train loss:0.08766841155248707\n",
      "train loss:0.3238440363414904\n",
      "train loss:0.02777740928529709\n",
      "train loss:0.044921550264401804\n",
      "train loss:0.07897598928379786\n",
      "train loss:0.0870596067658179\n",
      "train loss:0.1126647419558844\n",
      "train loss:0.14357164282088142\n",
      "train loss:0.1447208432197354\n",
      "train loss:0.07150213809148752\n",
      "train loss:0.07275044049797896\n",
      "train loss:0.12874525882316984\n",
      "train loss:0.12035560135259445\n",
      "train loss:0.035032494759798555\n",
      "train loss:0.24510980619718045\n",
      "train loss:0.17799584085415668\n",
      "train loss:0.1015811591746651\n",
      "train loss:0.0318432655245959\n",
      "train loss:0.0716091067436606\n",
      "train loss:0.06499046256301627\n",
      "train loss:0.11550619190919337\n",
      "train loss:0.1626441858531974\n",
      "train loss:0.06666373590933847\n",
      "train loss:0.16860827743240756\n",
      "train loss:0.05481762719374839\n",
      "train loss:0.05430539664870999\n",
      "train loss:0.09725621075332919\n",
      "train loss:0.05422712803759116\n",
      "train loss:0.056163329897076064\n",
      "train loss:0.07020647840399166\n",
      "train loss:0.08468487988291351\n",
      "train loss:0.12532311688706355\n",
      "train loss:0.03818705285404485\n",
      "train loss:0.07649817947364673\n",
      "train loss:0.20937799213386538\n",
      "train loss:0.11073107273420225\n",
      "train loss:0.08469215391832069\n",
      "train loss:0.036924996061183636\n",
      "train loss:0.09370281877340203\n",
      "train loss:0.12205383157788315\n",
      "train loss:0.12089506582235049\n",
      "train loss:0.04238108037141039\n",
      "train loss:0.08410273538220739\n",
      "train loss:0.027747718215226935\n",
      "train loss:0.11697646005805634\n",
      "train loss:0.06229219190790454\n",
      "train loss:0.1391045595089337\n",
      "train loss:0.15804794480835763\n",
      "train loss:0.09385950829266627\n",
      "train loss:0.14349330494342216\n",
      "train loss:0.051442969487732554\n",
      "train loss:0.03645977023412484\n",
      "train loss:0.05006705719119843\n",
      "train loss:0.10489290861528615\n",
      "train loss:0.11782326479752314\n",
      "train loss:0.08662058171684889\n",
      "train loss:0.06678949674339305\n",
      "train loss:0.18271634850762006\n",
      "train loss:0.09882059720795222\n",
      "train loss:0.09369568551992184\n",
      "train loss:0.04170901029706057\n",
      "train loss:0.07689685076376596\n",
      "train loss:0.0500623463423944\n",
      "train loss:0.02964800385614873\n",
      "train loss:0.05875511415834041\n",
      "train loss:0.07549047629868672\n",
      "train loss:0.103093151637519\n",
      "train loss:0.040699919476437335\n",
      "train loss:0.14249846771982416\n",
      "train loss:0.08049785807714452\n",
      "train loss:0.07285504700128795\n",
      "train loss:0.19872785937137175\n",
      "train loss:0.10973486516223004\n",
      "train loss:0.15157450991877386\n",
      "train loss:0.08568437284447215\n",
      "train loss:0.04521267241007463\n",
      "train loss:0.06582563655535258\n",
      "train loss:0.10684953891572775\n",
      "train loss:0.1022312153307404\n",
      "train loss:0.11230895490010996\n",
      "train loss:0.10313980967426258\n",
      "train loss:0.0329634097938726\n",
      "train loss:0.05734072422180349\n",
      "train loss:0.10380137031253617\n",
      "train loss:0.0771151659476164\n",
      "train loss:0.05436123250979694\n",
      "train loss:0.022326211087971455\n",
      "train loss:0.04315387904771577\n",
      "train loss:0.10785719990624393\n",
      "train loss:0.1465451796896966\n",
      "train loss:0.03738660854606146\n",
      "train loss:0.05271924597292405\n",
      "train loss:0.044887051321073075\n",
      "train loss:0.07268480464496133\n",
      "train loss:0.05199445432836242\n",
      "train loss:0.050936871481619274\n",
      "train loss:0.04540408138542482\n",
      "train loss:0.07647281757759637\n",
      "train loss:0.1361115158197368\n",
      "train loss:0.05313589127518328\n",
      "train loss:0.06672989375803709\n",
      "train loss:0.05245064379498854\n",
      "train loss:0.09374030156261927\n",
      "train loss:0.053215646904554256\n",
      "train loss:0.036419499254040866\n",
      "train loss:0.04240089833872843\n",
      "train loss:0.07602758806484528\n",
      "train loss:0.07247118281974801\n",
      "train loss:0.03971205932516092\n",
      "train loss:0.043146950206925945\n",
      "train loss:0.05966011666208817\n",
      "train loss:0.10296231525545212\n",
      "train loss:0.09020535752352897\n",
      "train loss:0.025382826292738664\n",
      "train loss:0.07853958717269302\n",
      "train loss:0.022558906420310994\n",
      "train loss:0.0554800888335774\n",
      "train loss:0.040878478582326364\n",
      "train loss:0.025957376545957735\n",
      "train loss:0.03625170368451904\n",
      "train loss:0.0641406103718904\n",
      "train loss:0.055842174835216814\n",
      "train loss:0.06211388804919418\n",
      "train loss:0.06107440311182457\n",
      "train loss:0.06715361965367472\n",
      "train loss:0.126998788075309\n",
      "train loss:0.08622791870494392\n",
      "train loss:0.07296265209155349\n",
      "train loss:0.1001692834727729\n",
      "train loss:0.11687257369291086\n",
      "train loss:0.06341576360462574\n",
      "train loss:0.06377361631157533\n",
      "train loss:0.17482348579507206\n",
      "train loss:0.04601392124669418\n",
      "train loss:0.04198840959381408\n",
      "train loss:0.06775419969902469\n",
      "train loss:0.14968409892588821\n",
      "train loss:0.02097422468169855\n",
      "train loss:0.061277516345456993\n",
      "train loss:0.05311617487803574\n",
      "train loss:0.0631687431520768\n",
      "train loss:0.14523956715526007\n",
      "train loss:0.02931974797483325\n",
      "train loss:0.1952713019712674\n",
      "train loss:0.03625701098185094\n",
      "train loss:0.02321120685378646\n",
      "train loss:0.04014166613865091\n",
      "train loss:0.21301230110006802\n",
      "train loss:0.16454042815495395\n",
      "train loss:0.04409934615394453\n",
      "train loss:0.23584639038985633\n",
      "train loss:0.04462207380278893\n",
      "train loss:0.06436378133686366\n",
      "train loss:0.07165857774560637\n",
      "train loss:0.05291332726504496\n",
      "train loss:0.039072950331172196\n",
      "train loss:0.0511431217012145\n",
      "train loss:0.05032725614422146\n",
      "train loss:0.0574288114041987\n",
      "train loss:0.026802194711090604\n",
      "train loss:0.06233889980094559\n",
      "train loss:0.039013610687699575\n",
      "train loss:0.06380772203410866\n",
      "train loss:0.07002680550754545\n",
      "train loss:0.11110139796623325\n",
      "train loss:0.11207340823462655\n",
      "train loss:0.112423234639411\n",
      "train loss:0.09418433093075489\n",
      "train loss:0.04634581493343285\n",
      "train loss:0.10327384837305469\n",
      "train loss:0.06887306661795882\n",
      "train loss:0.08740981871805939\n",
      "train loss:0.07432504773214367\n",
      "train loss:0.019925000479589255\n",
      "train loss:0.054805622941582186\n",
      "train loss:0.04143360266899575\n",
      "train loss:0.01873805086940486\n",
      "train loss:0.13182318389690043\n",
      "train loss:0.012769321714858789\n",
      "train loss:0.055681751718555855\n",
      "train loss:0.07863186245901554\n",
      "train loss:0.023775234442558343\n",
      "train loss:0.034248063388132216\n",
      "train loss:0.06182264996430147\n",
      "train loss:0.07837885002786059\n",
      "train loss:0.041795345707964025\n",
      "train loss:0.055626629941536034\n",
      "train loss:0.15253341930849362\n",
      "train loss:0.04815546991942782\n",
      "train loss:0.0435071421500655\n",
      "train loss:0.032654207214975445\n",
      "train loss:0.07352462879034602\n",
      "train loss:0.06145494697677376\n",
      "train loss:0.08774058378167222\n",
      "train loss:0.058859153374435244\n",
      "train loss:0.0928607885272032\n",
      "train loss:0.08665928511543351\n",
      "train loss:0.045566943924966935\n",
      "train loss:0.03623660330833049\n",
      "train loss:0.09224195661598841\n",
      "train loss:0.08989871925493123\n",
      "train loss:0.033151024209252\n",
      "train loss:0.09581789597575764\n",
      "train loss:0.03287172620719388\n",
      "train loss:0.03526890366340493\n",
      "train loss:0.14755669048596992\n",
      "train loss:0.0620964349487932\n",
      "train loss:0.12496189476529407\n",
      "train loss:0.04573011811272641\n",
      "train loss:0.12693703391376995\n",
      "train loss:0.09592896680144083\n",
      "train loss:0.06223200831637707\n",
      "train loss:0.017851917506476238\n",
      "train loss:0.04529826303325374\n",
      "train loss:0.04376203994256133\n",
      "train loss:0.06304872936319886\n",
      "train loss:0.041518491382689426\n",
      "train loss:0.09004780945596134\n",
      "train loss:0.04580181247131582\n",
      "train loss:0.03817008926449876\n",
      "train loss:0.10007738351803953\n",
      "train loss:0.14606378644171125\n",
      "train loss:0.031821730837678544\n",
      "train loss:0.06566778070978513\n",
      "train loss:0.04677759317261665\n",
      "train loss:0.07776726055277583\n",
      "train loss:0.04244159145010144\n",
      "train loss:0.11202630217381515\n",
      "train loss:0.072353677088247\n",
      "train loss:0.08103227094913652\n",
      "train loss:0.055903033869558395\n",
      "train loss:0.044083046346662665\n",
      "train loss:0.04166372480381599\n",
      "train loss:0.047007403617443086\n",
      "train loss:0.07400051315739585\n",
      "train loss:0.15974892088691459\n",
      "train loss:0.03479481477004616\n",
      "train loss:0.14028733944855393\n",
      "train loss:0.028639833558851215\n",
      "train loss:0.04364720303970136\n",
      "train loss:0.02993233888697471\n",
      "train loss:0.03749083790733011\n",
      "train loss:0.029151549700351217\n",
      "train loss:0.07438140918941943\n",
      "train loss:0.21463043798108353\n",
      "train loss:0.04237673228569823\n",
      "train loss:0.057533131298585935\n",
      "train loss:0.16762575604925783\n",
      "train loss:0.1024309371856116\n",
      "train loss:0.117548483423589\n",
      "train loss:0.07564897532290821\n",
      "train loss:0.16871700764803765\n",
      "train loss:0.05108004622540603\n",
      "train loss:0.041567958375359364\n",
      "train loss:0.049938474075376105\n",
      "train loss:0.06671396241112752\n",
      "train loss:0.065207917968066\n",
      "train loss:0.08253141976231394\n",
      "train loss:0.09339844555432458\n",
      "train loss:0.050064839294282765\n",
      "train loss:0.0375688106534154\n",
      "train loss:0.1083933253094907\n",
      "train loss:0.02818645293024534\n",
      "train loss:0.15540390811858856\n",
      "train loss:0.14135903108728748\n",
      "train loss:0.14074278015898853\n",
      "train loss:0.05528085203092105\n",
      "train loss:0.07435766505436653\n",
      "train loss:0.04223802712125074\n",
      "train loss:0.07506842332312615\n",
      "train loss:0.10844478622132656\n",
      "train loss:0.07443357117748443\n",
      "train loss:0.037319564507485664\n",
      "train loss:0.05180785188497363\n",
      "train loss:0.08312091803135482\n",
      "train loss:0.10017915414909533\n",
      "train loss:0.02203356447320433\n",
      "train loss:0.02677801623194502\n",
      "train loss:0.09349917657181386\n",
      "train loss:0.059239836145743595\n",
      "train loss:0.06455137794144915\n",
      "train loss:0.11212376243055538\n",
      "train loss:0.03941031183596255\n",
      "train loss:0.11133229396683113\n",
      "train loss:0.08362667908660786\n",
      "train loss:0.03729548571798457\n",
      "train loss:0.06301128212776766\n",
      "train loss:0.05338583178174647\n",
      "train loss:0.06522838090871347\n",
      "train loss:0.10671485379246232\n",
      "train loss:0.05163844485219315\n",
      "train loss:0.1071984652277367\n",
      "train loss:0.05165481075680899\n",
      "train loss:0.09400718367142417\n",
      "train loss:0.07150287649882496\n",
      "train loss:0.10701742381837578\n",
      "train loss:0.04224963970397757\n",
      "train loss:0.14547546694157795\n",
      "train loss:0.03692142274690527\n",
      "train loss:0.055563974629602574\n",
      "train loss:0.06553599316352264\n",
      "train loss:0.08376146161247283\n",
      "train loss:0.07549445630803897\n",
      "train loss:0.1285612469042909\n",
      "train loss:0.05514653681470223\n",
      "train loss:0.08030033991165465\n",
      "train loss:0.033946553221836415\n",
      "train loss:0.06939149055548861\n",
      "train loss:0.013244769613265556\n",
      "train loss:0.12562742518126563\n",
      "train loss:0.03664220704151477\n",
      "train loss:0.03162083925927364\n",
      "train loss:0.14421875276879315\n",
      "train loss:0.05281502060698115\n",
      "train loss:0.04358123283882228\n",
      "train loss:0.05329860613812922\n",
      "train loss:0.07433357894117597\n",
      "train loss:0.04669433215579432\n",
      "train loss:0.01750734123207236\n",
      "train loss:0.011576100529648903\n",
      "train loss:0.07394864484368283\n",
      "train loss:0.053581262847648824\n",
      "train loss:0.13571152504021258\n",
      "train loss:0.0624710925241029\n",
      "train loss:0.07607883731335073\n",
      "train loss:0.04626947433447576\n",
      "train loss:0.04805306185580172\n",
      "train loss:0.06293785796084998\n",
      "train loss:0.12213500968289823\n",
      "train loss:0.13751679245981221\n",
      "train loss:0.10816971378674133\n",
      "train loss:0.05317599595799441\n",
      "train loss:0.0830135597564067\n",
      "train loss:0.03792524057745681\n",
      "train loss:0.180378014354\n",
      "train loss:0.07820780690273885\n",
      "train loss:0.10076908616710833\n",
      "train loss:0.05740805535352044\n",
      "train loss:0.09483543770541698\n",
      "train loss:0.04453339955895111\n",
      "train loss:0.058557030078326903\n",
      "train loss:0.11448172051695236\n",
      "train loss:0.06087749280361046\n",
      "train loss:0.1022562060372573\n",
      "train loss:0.07458132941940628\n",
      "train loss:0.056336762395337046\n",
      "train loss:0.1219320025991398\n",
      "train loss:0.11706684761040602\n",
      "train loss:0.05057118703805972\n",
      "train loss:0.08625744631923656\n",
      "train loss:0.05968824741569765\n",
      "train loss:0.045308569610114764\n",
      "train loss:0.0635980076690338\n",
      "train loss:0.03632562346325072\n",
      "train loss:0.07089435098860918\n",
      "train loss:0.03374564164709106\n",
      "train loss:0.05775616225737984\n",
      "train loss:0.047081325089465366\n",
      "train loss:0.05931049952565842\n",
      "train loss:0.03506370294047518\n",
      "train loss:0.1030953683346722\n",
      "train loss:0.08116823591739085\n",
      "train loss:0.02506397873911201\n",
      "train loss:0.11062144095898363\n",
      "train loss:0.11757944266217235\n",
      "train loss:0.029994004496118762\n",
      "train loss:0.023405055503276387\n",
      "train loss:0.12305872608440321\n",
      "train loss:0.016436763007254173\n",
      "train loss:0.05757986624576447\n",
      "train loss:0.03511601591462578\n",
      "train loss:0.08015890941611666\n",
      "train loss:0.18369577681206536\n",
      "train loss:0.03375349830814672\n",
      "train loss:0.03366986695634425\n",
      "train loss:0.0558638761694146\n",
      "train loss:0.1168935927604995\n",
      "train loss:0.06667597283869733\n",
      "train loss:0.043454234724418346\n",
      "train loss:0.024374716117035934\n",
      "train loss:0.03968494772503985\n",
      "train loss:0.12506822912209034\n",
      "train loss:0.07103776375931081\n",
      "train loss:0.08784898280079992\n",
      "train loss:0.07681214347238427\n",
      "train loss:0.03564043050986183\n",
      "train loss:0.09066888717521779\n",
      "train loss:0.0543079372009823\n",
      "train loss:0.10249652430433276\n",
      "train loss:0.05656224612378653\n",
      "train loss:0.0913155520699289\n",
      "train loss:0.05693164744832964\n",
      "train loss:0.07159375980613811\n",
      "train loss:0.07420807089837295\n",
      "train loss:0.030405789017952534\n",
      "train loss:0.07057906362463645\n",
      "train loss:0.045841125495621915\n",
      "train loss:0.043847920610270916\n",
      "train loss:0.0820099310886799\n",
      "train loss:0.023257008965888937\n",
      "train loss:0.09114495245808538\n",
      "train loss:0.05861931680181548\n",
      "train loss:0.057150380323917005\n",
      "train loss:0.13170415957322723\n",
      "train loss:0.10553649409193516\n",
      "train loss:0.05124992355565114\n",
      "train loss:0.029614813487576518\n",
      "train loss:0.06747820987940063\n",
      "train loss:0.11412446433381307\n",
      "train loss:0.05963917125343558\n",
      "train loss:0.07606953074701232\n",
      "=== epoch:3, train acc:0.976, test acc:0.969 ===\n",
      "train loss:0.09970211930887757\n",
      "train loss:0.04454213652080802\n",
      "train loss:0.038664944031875446\n",
      "train loss:0.037484834554150664\n",
      "train loss:0.12755663146474103\n",
      "train loss:0.11486835330795443\n",
      "train loss:0.06168463370654798\n",
      "train loss:0.027553716299322305\n",
      "train loss:0.02873047217817285\n",
      "train loss:0.0588487477233826\n",
      "train loss:0.0906952377113468\n",
      "train loss:0.04554414944184787\n",
      "train loss:0.038886317817047426\n",
      "train loss:0.12037736820183077\n",
      "train loss:0.09076258522186534\n",
      "train loss:0.0924052949309543\n",
      "train loss:0.06016166641200336\n",
      "train loss:0.04058009971059501\n",
      "train loss:0.04371877110781632\n",
      "train loss:0.07620828495141069\n",
      "train loss:0.11890107977075924\n",
      "train loss:0.01694998812282258\n",
      "train loss:0.07529165046875973\n",
      "train loss:0.05208907241334727\n",
      "train loss:0.024702961999936615\n",
      "train loss:0.09237978949387735\n",
      "train loss:0.02546007563635675\n",
      "train loss:0.02274035048210656\n",
      "train loss:0.1283051170564432\n",
      "train loss:0.07006599462958289\n",
      "train loss:0.1067102320345175\n",
      "train loss:0.010547889160868993\n",
      "train loss:0.01844077473239656\n",
      "train loss:0.031306646676821204\n",
      "train loss:0.04019582294306656\n",
      "train loss:0.05465431609380606\n",
      "train loss:0.14468808539148803\n",
      "train loss:0.07024624663221553\n",
      "train loss:0.06918087158105664\n",
      "train loss:0.025357073398089416\n",
      "train loss:0.03317839231714356\n",
      "train loss:0.047136853901609176\n",
      "train loss:0.07233112456294094\n",
      "train loss:0.18670457628812437\n",
      "train loss:0.05676556293182957\n",
      "train loss:0.035217513302241675\n",
      "train loss:0.049860305009442156\n",
      "train loss:0.1221561853140948\n",
      "train loss:0.0675943332671855\n",
      "train loss:0.05241647762963252\n",
      "train loss:0.025118963976084756\n",
      "train loss:0.07444173504474283\n",
      "train loss:0.03656302047231314\n",
      "train loss:0.043815075063340626\n",
      "train loss:0.04330355615533793\n",
      "train loss:0.06440571432817929\n",
      "train loss:0.04391440528766223\n",
      "train loss:0.01915057673314327\n",
      "train loss:0.06782253712037574\n",
      "train loss:0.05929732847260544\n",
      "train loss:0.14606152758741942\n",
      "train loss:0.02509741050304551\n",
      "train loss:0.04962535220618945\n",
      "train loss:0.026193537534285515\n",
      "train loss:0.05374932602371402\n",
      "train loss:0.10407015058013236\n",
      "train loss:0.08026325946299855\n",
      "train loss:0.01798873750996617\n",
      "train loss:0.04590318989522868\n",
      "train loss:0.0636748767457631\n",
      "train loss:0.06598997558396291\n",
      "train loss:0.01317695459328532\n",
      "train loss:0.03999002566138909\n",
      "train loss:0.06653326223664545\n",
      "train loss:0.02358505959310548\n",
      "train loss:0.044074958626687415\n",
      "train loss:0.027335723164236585\n",
      "train loss:0.06025995234248084\n",
      "train loss:0.07167976132720695\n",
      "train loss:0.03345421024286658\n",
      "train loss:0.03256740289348794\n",
      "train loss:0.056943816274565774\n",
      "train loss:0.05423865463448752\n",
      "train loss:0.03244882686914375\n",
      "train loss:0.05115457273821212\n",
      "train loss:0.024440703542071607\n",
      "train loss:0.03428622961650317\n",
      "train loss:0.13080371147901496\n",
      "train loss:0.07865301708148087\n",
      "train loss:0.038559054525958286\n",
      "train loss:0.15738757225249067\n",
      "train loss:0.05202585934992996\n",
      "train loss:0.03832972613454555\n",
      "train loss:0.03235454987061433\n",
      "train loss:0.11340185112661981\n",
      "train loss:0.030266219785735342\n",
      "train loss:0.02051855352847806\n",
      "train loss:0.03937826056108314\n",
      "train loss:0.07862638174654334\n",
      "train loss:0.08628170860937245\n",
      "train loss:0.06562480263136256\n",
      "train loss:0.05592960842594327\n",
      "train loss:0.05957807708292551\n",
      "train loss:0.03197677542487602\n",
      "train loss:0.013782188569342794\n",
      "train loss:0.03851757222421402\n",
      "train loss:0.02160359057251246\n",
      "train loss:0.08700890053387185\n",
      "train loss:0.04665677331793521\n",
      "train loss:0.011791898846936688\n",
      "train loss:0.06779478149696921\n",
      "train loss:0.047093769719918466\n",
      "train loss:0.06156579753984985\n",
      "train loss:0.03046020521398926\n",
      "train loss:0.022022458776443144\n",
      "train loss:0.09103757067672932\n",
      "train loss:0.0455674540826078\n",
      "train loss:0.08958072688799806\n",
      "train loss:0.04776741478974904\n",
      "train loss:0.029856677509990187\n",
      "train loss:0.052759418568893234\n",
      "train loss:0.041172819597771325\n",
      "train loss:0.024256908168040582\n",
      "train loss:0.10401882762465026\n",
      "train loss:0.03082008247320226\n",
      "train loss:0.031741071013658265\n",
      "train loss:0.10350150274613253\n",
      "train loss:0.025546537070944494\n",
      "train loss:0.06052472051557631\n",
      "train loss:0.04560839240203268\n",
      "train loss:0.04786154186595133\n",
      "train loss:0.02695518506498372\n",
      "train loss:0.056316684248389856\n",
      "train loss:0.03922692760803579\n",
      "train loss:0.05541484882480227\n",
      "train loss:0.038872225970455\n",
      "train loss:0.021497922670020007\n",
      "train loss:0.12361134946227376\n",
      "train loss:0.021850642438470343\n",
      "train loss:0.08253243111213018\n",
      "train loss:0.024027873130286558\n",
      "train loss:0.033511525513931106\n",
      "train loss:0.013257575997154864\n",
      "train loss:0.03478204711985413\n",
      "train loss:0.0517384353944294\n",
      "train loss:0.02644080354427937\n",
      "train loss:0.05155465423709547\n",
      "train loss:0.06121139050927559\n",
      "train loss:0.020029616744462943\n",
      "train loss:0.06369204892112537\n",
      "train loss:0.07755535238922233\n",
      "train loss:0.01952345053754148\n",
      "train loss:0.025833879281520845\n",
      "train loss:0.15119487848505453\n",
      "train loss:0.06152095329458742\n",
      "train loss:0.023166866122224846\n",
      "train loss:0.047669927438078435\n",
      "train loss:0.05390447660881652\n",
      "train loss:0.01605014190221681\n",
      "train loss:0.09540562961300968\n",
      "train loss:0.15811247046512975\n",
      "train loss:0.08167634296499561\n",
      "train loss:0.055173048732660795\n",
      "train loss:0.05028023690954899\n",
      "train loss:0.10915919240588513\n",
      "train loss:0.04659228040374528\n",
      "train loss:0.08104957238595181\n",
      "train loss:0.04081506546662363\n",
      "train loss:0.035483986730167116\n",
      "train loss:0.04885795456072212\n",
      "train loss:0.04133823118338576\n",
      "train loss:0.05726422041944845\n",
      "train loss:0.022847088522328127\n",
      "train loss:0.06444661480996444\n",
      "train loss:0.07190808480204178\n",
      "train loss:0.04427916377462015\n",
      "train loss:0.07354237439567082\n",
      "train loss:0.036576024709409814\n",
      "train loss:0.07007382385344368\n",
      "train loss:0.08497680792585795\n",
      "train loss:0.011702736342257183\n",
      "train loss:0.0686256523212515\n",
      "train loss:0.040830545323921906\n",
      "train loss:0.05794109430450315\n",
      "train loss:0.029993472457335177\n",
      "train loss:0.09687712376571365\n",
      "train loss:0.016726191453269066\n",
      "train loss:0.04600483330082872\n",
      "train loss:0.013986411209171198\n",
      "train loss:0.08145052229325406\n",
      "train loss:0.10125891254414787\n",
      "train loss:0.05614746717606431\n",
      "train loss:0.037319910747238345\n",
      "train loss:0.014147756281290738\n",
      "train loss:0.034031757980277764\n",
      "train loss:0.030204094532733063\n",
      "train loss:0.07933598339355281\n",
      "train loss:0.1146131641899066\n",
      "train loss:0.03819490767124285\n",
      "train loss:0.06963797528110616\n",
      "train loss:0.03702956654806686\n",
      "train loss:0.05386267001039888\n",
      "train loss:0.043025575527469044\n",
      "train loss:0.04353264103455451\n",
      "train loss:0.020879181873751408\n",
      "train loss:0.10833802145708665\n",
      "train loss:0.05852173614041191\n",
      "train loss:0.046206572929574335\n",
      "train loss:0.03843387558924766\n",
      "train loss:0.033744006909438184\n",
      "train loss:0.02261251769663651\n",
      "train loss:0.07159016073334762\n",
      "train loss:0.03353047855210998\n",
      "train loss:0.0544678860389535\n",
      "train loss:0.09942051071577326\n",
      "train loss:0.011136951386204352\n",
      "train loss:0.019973088031336315\n",
      "train loss:0.04522339750417175\n",
      "train loss:0.015775505913007925\n",
      "train loss:0.08093650517862117\n",
      "train loss:0.042930149205959584\n",
      "train loss:0.03273650952761105\n",
      "train loss:0.037777168510360344\n",
      "train loss:0.1039850639452413\n",
      "train loss:0.052593690015908086\n",
      "train loss:0.01900431198044432\n",
      "train loss:0.016550591375433674\n",
      "train loss:0.0666502016787201\n",
      "train loss:0.03777045842906264\n",
      "train loss:0.15155912687438122\n",
      "train loss:0.05311758389164084\n",
      "train loss:0.02186303087945233\n",
      "train loss:0.04373402168982587\n",
      "train loss:0.09833980822346168\n",
      "train loss:0.11416470101422976\n",
      "train loss:0.11278290178451825\n",
      "train loss:0.06268013478389413\n",
      "train loss:0.027141461350999645\n",
      "train loss:0.08913769616794916\n",
      "train loss:0.12251725786280401\n",
      "train loss:0.018234710197034258\n",
      "train loss:0.026137860173383248\n",
      "train loss:0.05378719877894435\n",
      "train loss:0.07215366587721736\n",
      "train loss:0.02326069400549173\n",
      "train loss:0.024430263005155706\n",
      "train loss:0.016342111215864496\n",
      "train loss:0.046064550965099524\n",
      "train loss:0.02522269733264754\n",
      "train loss:0.012678380610266922\n",
      "train loss:0.06114679279811324\n",
      "train loss:0.08292135295106165\n",
      "train loss:0.059227044435236145\n",
      "train loss:0.020589805384621962\n",
      "train loss:0.03297932065983298\n",
      "train loss:0.04596268681938356\n",
      "train loss:0.145863142549755\n",
      "train loss:0.2083507342681956\n",
      "train loss:0.05632566823297194\n",
      "train loss:0.05654378015974518\n",
      "train loss:0.08734064558264992\n",
      "train loss:0.023224784924100454\n",
      "train loss:0.01990797221758954\n",
      "train loss:0.06412215884957068\n",
      "train loss:0.029527943298841196\n",
      "train loss:0.04712741969889705\n",
      "train loss:0.049480298498257615\n",
      "train loss:0.04260743043768917\n",
      "train loss:0.03652910929882725\n",
      "train loss:0.1025875651469528\n",
      "train loss:0.08275391549290122\n",
      "train loss:0.021086701612387963\n",
      "train loss:0.12238510933742866\n",
      "train loss:0.037869555689537544\n",
      "train loss:0.08240785844542503\n",
      "train loss:0.05357290182282262\n",
      "train loss:0.08946830390031024\n",
      "train loss:0.022357558416873268\n",
      "train loss:0.057489181219242\n",
      "train loss:0.07682879080809484\n",
      "train loss:0.06332260723817658\n",
      "train loss:0.031465586827457454\n",
      "train loss:0.036568712853299705\n",
      "train loss:0.04367441980375251\n",
      "train loss:0.05888446629506008\n",
      "train loss:0.04808931749421145\n",
      "train loss:0.012228900137566423\n",
      "train loss:0.05491505838882635\n",
      "train loss:0.042462486969800074\n",
      "train loss:0.04627687367116682\n",
      "train loss:0.01945855217553893\n",
      "train loss:0.039973496127968675\n",
      "train loss:0.027541618317675712\n",
      "train loss:0.023309366603351346\n",
      "train loss:0.061679884419471705\n",
      "train loss:0.03819884004314265\n",
      "train loss:0.04254845812808587\n",
      "train loss:0.027049706327160753\n",
      "train loss:0.023129923672414274\n",
      "train loss:0.024388303251322917\n",
      "train loss:0.05081568159204823\n",
      "train loss:0.04176015590530347\n",
      "train loss:0.02680219953263482\n",
      "train loss:0.05160634546574175\n",
      "train loss:0.027819670994355613\n",
      "train loss:0.014215866825066538\n",
      "train loss:0.07097131142927386\n",
      "train loss:0.10989945350249418\n",
      "train loss:0.023743084622973375\n",
      "train loss:0.016917612699085358\n",
      "train loss:0.014374608394722252\n",
      "train loss:0.02751464878275914\n",
      "train loss:0.08251909347257852\n",
      "train loss:0.013763983627545997\n",
      "train loss:0.0301280503448729\n",
      "train loss:0.04654076500082969\n",
      "train loss:0.017240528126501904\n",
      "train loss:0.12122047968506312\n",
      "train loss:0.07767248041466697\n",
      "train loss:0.04640891464016148\n",
      "train loss:0.03145256487820998\n",
      "train loss:0.06957667648984418\n",
      "train loss:0.041114306831402804\n",
      "train loss:0.022289613671145302\n",
      "train loss:0.030868114208010784\n",
      "train loss:0.04463258240880559\n",
      "train loss:0.04438732821891739\n",
      "train loss:0.10440139193406336\n",
      "train loss:0.05411598741073846\n",
      "train loss:0.017315200291856247\n",
      "train loss:0.01809144180287071\n",
      "train loss:0.04517528517374218\n",
      "train loss:0.08901397950378387\n",
      "train loss:0.0399444209478036\n",
      "train loss:0.0267411949309998\n",
      "train loss:0.034106418920714435\n",
      "train loss:0.061091982632751736\n",
      "train loss:0.048260677906132365\n",
      "train loss:0.035199862227266225\n",
      "train loss:0.02545485492452301\n",
      "train loss:0.07910601552144833\n",
      "train loss:0.007606864508403539\n",
      "train loss:0.02583050353716446\n",
      "train loss:0.04890105176281307\n",
      "train loss:0.018603889864832482\n",
      "train loss:0.07381170628716698\n",
      "train loss:0.027792151076263042\n",
      "train loss:0.12332605736679574\n",
      "train loss:0.052006718071535875\n",
      "train loss:0.09927035891081747\n",
      "train loss:0.05770215456111668\n",
      "train loss:0.021388562369500676\n",
      "train loss:0.023889970954419963\n",
      "train loss:0.0267158422711766\n",
      "train loss:0.06790304343534734\n",
      "train loss:0.03952330811505623\n",
      "train loss:0.013199596813990647\n",
      "train loss:0.10433101178462019\n",
      "train loss:0.011923371798031839\n",
      "train loss:0.03885375655808791\n",
      "train loss:0.10056703270051184\n",
      "train loss:0.050393603322400435\n",
      "train loss:0.037445906936188794\n",
      "train loss:0.03834911511973318\n",
      "train loss:0.059678966830503846\n",
      "train loss:0.059150999329339496\n",
      "train loss:0.02116407187392208\n",
      "train loss:0.026671193327905435\n",
      "train loss:0.17602117491617467\n",
      "train loss:0.11835311018053578\n",
      "train loss:0.04175535716682965\n",
      "train loss:0.11082528874456461\n",
      "train loss:0.019409952511858394\n",
      "train loss:0.05360102655161171\n",
      "train loss:0.06837212696085274\n",
      "train loss:0.020873285749335516\n",
      "train loss:0.014295451466342908\n",
      "train loss:0.1347341585572415\n",
      "train loss:0.032281325755836976\n",
      "train loss:0.08821109434830479\n",
      "train loss:0.04813193756537453\n",
      "train loss:0.058457272548418586\n",
      "train loss:0.023394845999564796\n",
      "train loss:0.06973974409521679\n",
      "train loss:0.062229712699032956\n",
      "train loss:0.060321584516992245\n",
      "train loss:0.0432548693940393\n",
      "train loss:0.030195478902310863\n",
      "train loss:0.02879833187322391\n",
      "train loss:0.014246677020439866\n",
      "train loss:0.02813575918116058\n",
      "train loss:0.0288550073544392\n",
      "train loss:0.048992797889110644\n",
      "train loss:0.05319037380832363\n",
      "train loss:0.027645726322000522\n",
      "train loss:0.042357292264843256\n",
      "train loss:0.10474680109252256\n",
      "train loss:0.08338636733390901\n",
      "train loss:0.0586505585473682\n",
      "train loss:0.02865556342076936\n",
      "train loss:0.01517908843631257\n",
      "train loss:0.03455775560696738\n",
      "train loss:0.09086744098911625\n",
      "train loss:0.014067251560746848\n",
      "train loss:0.06747713375571016\n",
      "train loss:0.03382312258313755\n",
      "train loss:0.017161694666624615\n",
      "train loss:0.03438810024873294\n",
      "train loss:0.15125399765826822\n",
      "train loss:0.03446394966501074\n",
      "train loss:0.03036703272204734\n",
      "train loss:0.08254681503484945\n",
      "train loss:0.013368042685737703\n",
      "train loss:0.04752442669821446\n",
      "train loss:0.09473424265092144\n",
      "train loss:0.04764058304142466\n",
      "train loss:0.04357589221098271\n",
      "train loss:0.14558230667737024\n",
      "train loss:0.12844694984642893\n",
      "train loss:0.10458867531652344\n",
      "train loss:0.05237445338635884\n",
      "train loss:0.019120893036999497\n",
      "train loss:0.01899044847059712\n",
      "train loss:0.01896047097197752\n",
      "train loss:0.0312874571788529\n",
      "train loss:0.0384233092492172\n",
      "train loss:0.05559122865528072\n",
      "train loss:0.016821974223076606\n",
      "train loss:0.07535503957949023\n",
      "train loss:0.04033000062611282\n",
      "train loss:0.05130770419319363\n",
      "train loss:0.025298331358360193\n",
      "train loss:0.05283849053413542\n",
      "train loss:0.14757608304766723\n",
      "train loss:0.07478376843898436\n",
      "train loss:0.10656962839143434\n",
      "train loss:0.17036713271102355\n",
      "train loss:0.02625685307824739\n",
      "train loss:0.05834752103236863\n",
      "train loss:0.020044043350467354\n",
      "train loss:0.019443021090578983\n",
      "train loss:0.03461666738790039\n",
      "train loss:0.029319423027208774\n",
      "train loss:0.013613865386872048\n",
      "train loss:0.02034562833450293\n",
      "train loss:0.05273124947676293\n",
      "train loss:0.01974673075390222\n",
      "train loss:0.035234228849805205\n",
      "train loss:0.031364926212330946\n",
      "train loss:0.13079111924756112\n",
      "train loss:0.04995469043061347\n",
      "train loss:0.014672830619149766\n",
      "train loss:0.03953608048626925\n",
      "train loss:0.03236300881267996\n",
      "train loss:0.015522225068582543\n",
      "train loss:0.0770080419445544\n",
      "train loss:0.02507891590480696\n",
      "train loss:0.03561729927387976\n",
      "train loss:0.05489527714021568\n",
      "train loss:0.014389888365250025\n",
      "train loss:0.03286743636592391\n",
      "train loss:0.03190399321559942\n",
      "train loss:0.015698960824376065\n",
      "train loss:0.025178742340081064\n",
      "train loss:0.10143759485773296\n",
      "train loss:0.036649557011657646\n",
      "train loss:0.012517266660759458\n",
      "train loss:0.01901202399113201\n",
      "train loss:0.022964061872560376\n",
      "train loss:0.020085324445146973\n",
      "train loss:0.02325886790755521\n",
      "train loss:0.19107666222902642\n",
      "train loss:0.17291986545077784\n",
      "train loss:0.013755941230914028\n",
      "train loss:0.046045468340707815\n",
      "train loss:0.012197749332074353\n",
      "train loss:0.05202167430736118\n",
      "train loss:0.02596940333105514\n",
      "train loss:0.015371796640635841\n",
      "train loss:0.0500712744254268\n",
      "train loss:0.01511478868230503\n",
      "train loss:0.030838280541562645\n",
      "train loss:0.01458683070894676\n",
      "train loss:0.04392693207970054\n",
      "train loss:0.020670376959481727\n",
      "train loss:0.08786990371548638\n",
      "train loss:0.018419029550272713\n",
      "train loss:0.053048708177924\n",
      "train loss:0.04417211089385031\n",
      "train loss:0.02099771235531817\n",
      "train loss:0.03111114762309529\n",
      "train loss:0.05690951470752467\n",
      "train loss:0.07680952983340611\n",
      "train loss:0.02004662047199891\n",
      "train loss:0.013753606385618532\n",
      "train loss:0.11405609620651531\n",
      "train loss:0.11358514308279923\n",
      "train loss:0.0070833558792001775\n",
      "train loss:0.024777133877186063\n",
      "train loss:0.022819450937811066\n",
      "train loss:0.02327381386894467\n",
      "train loss:0.009340220781371263\n",
      "train loss:0.027619282345269235\n",
      "train loss:0.014724058848231136\n",
      "train loss:0.02838547133916166\n",
      "train loss:0.02633241319726573\n",
      "train loss:0.036251909909673904\n",
      "train loss:0.06036328885546197\n",
      "train loss:0.016874893285661927\n",
      "train loss:0.10050581547478343\n",
      "train loss:0.08449221309702698\n",
      "train loss:0.015922486585887824\n",
      "train loss:0.027555579060280843\n",
      "train loss:0.01126451258443338\n",
      "train loss:0.060328996242394475\n",
      "train loss:0.03999841752185157\n",
      "train loss:0.09567371764004823\n",
      "train loss:0.03662853362140954\n",
      "train loss:0.055650055974739134\n",
      "train loss:0.05303053469986623\n",
      "train loss:0.013507706690783335\n",
      "train loss:0.039259290570451896\n",
      "train loss:0.026132084367734204\n",
      "train loss:0.03828261321039936\n",
      "train loss:0.12333543856582596\n",
      "train loss:0.05004703539835941\n",
      "train loss:0.02601421706858659\n",
      "train loss:0.03249990744126747\n",
      "train loss:0.022333830033224925\n",
      "train loss:0.020979631372087602\n",
      "train loss:0.018645081973479256\n",
      "train loss:0.029730834885427392\n",
      "train loss:0.13707006107270683\n",
      "train loss:0.04843777288190548\n",
      "train loss:0.05100711612638009\n",
      "train loss:0.08821465198718707\n",
      "train loss:0.03182036107092203\n",
      "train loss:0.048914608856559454\n",
      "train loss:0.02248806696623269\n",
      "train loss:0.03245114116493287\n",
      "train loss:0.016243780705634328\n",
      "train loss:0.03892023173518917\n",
      "train loss:0.03362514264206325\n",
      "train loss:0.033994070255441156\n",
      "train loss:0.07662290768312013\n",
      "train loss:0.06270426858467947\n",
      "train loss:0.02557359182234643\n",
      "train loss:0.08358116302172357\n",
      "train loss:0.044433679175172394\n",
      "train loss:0.04000987369240746\n",
      "train loss:0.02381495102218135\n",
      "train loss:0.04097533509290036\n",
      "train loss:0.026577682451978103\n",
      "train loss:0.037934765469036374\n",
      "train loss:0.013373843710470157\n",
      "train loss:0.08494627030025508\n",
      "train loss:0.03067960987917759\n",
      "train loss:0.04126135523870397\n",
      "train loss:0.033920284710195835\n",
      "train loss:0.0579633299708773\n",
      "train loss:0.03156928356710075\n",
      "train loss:0.044900753201018634\n",
      "train loss:0.027782972832272726\n",
      "train loss:0.018496270975697574\n",
      "train loss:0.0294671964878778\n",
      "train loss:0.02228989217342876\n",
      "train loss:0.034403817294572224\n",
      "train loss:0.03658350607944181\n",
      "train loss:0.03878794933554649\n",
      "train loss:0.05525681453818166\n",
      "train loss:0.03923859843139827\n",
      "train loss:0.015018409234508595\n",
      "train loss:0.02566185223471441\n",
      "train loss:0.012451235097958546\n",
      "train loss:0.09293426251021229\n",
      "train loss:0.024022010503018932\n",
      "train loss:0.12457994559795807\n",
      "train loss:0.06474858158046348\n",
      "train loss:0.09539435551554154\n",
      "train loss:0.04111218550744763\n",
      "train loss:0.039128789490172435\n",
      "train loss:0.012389240609695979\n",
      "train loss:0.01915802575843343\n",
      "train loss:0.10659228989231911\n",
      "train loss:0.10456978431181849\n",
      "train loss:0.0615955699187788\n",
      "train loss:0.06444089577384127\n",
      "train loss:0.018583213762559767\n",
      "train loss:0.12025717129166331\n",
      "train loss:0.05912371719443911\n",
      "train loss:0.09503745757738134\n",
      "train loss:0.04362358101169868\n",
      "train loss:0.028618447363259195\n",
      "train loss:0.08647041549927753\n",
      "train loss:0.028980368376028055\n",
      "train loss:0.01192773687797809\n",
      "train loss:0.011197656000547103\n",
      "train loss:0.041719458364766975\n",
      "train loss:0.05357275543139153\n",
      "train loss:0.026335729086346914\n",
      "=== epoch:4, train acc:0.97, test acc:0.977 ===\n",
      "train loss:0.050194847841959385\n",
      "train loss:0.008239083534175894\n",
      "train loss:0.02170453181698622\n",
      "train loss:0.04240546381198747\n",
      "train loss:0.02164552149159309\n",
      "train loss:0.019996707021377673\n",
      "train loss:0.023090863993473695\n",
      "train loss:0.01295602767543929\n",
      "train loss:0.01824531756895289\n",
      "train loss:0.0047863803382614065\n",
      "train loss:0.020543509418401808\n",
      "train loss:0.011325835374908576\n",
      "train loss:0.022413275106716278\n",
      "train loss:0.04653622128605834\n",
      "train loss:0.0908471246548932\n",
      "train loss:0.017882106996748352\n",
      "train loss:0.007341392093340786\n",
      "train loss:0.03201405808955818\n",
      "train loss:0.027486177357650715\n",
      "train loss:0.013367786767345907\n",
      "train loss:0.01449135867238414\n",
      "train loss:0.09390429047975993\n",
      "train loss:0.05018326314782534\n",
      "train loss:0.015104746642783065\n",
      "train loss:0.017881722017566413\n",
      "train loss:0.011774922063920817\n",
      "train loss:0.04595293513837385\n",
      "train loss:0.07047901382391175\n",
      "train loss:0.02779419909016647\n",
      "train loss:0.03874195173392711\n",
      "train loss:0.04238555871773795\n",
      "train loss:0.0966532403928672\n",
      "train loss:0.05054736214847304\n",
      "train loss:0.05233386759399503\n",
      "train loss:0.08081531479101417\n",
      "train loss:0.009428183739811672\n",
      "train loss:0.051606953327045735\n",
      "train loss:0.01768479180916096\n",
      "train loss:0.01954845025500416\n",
      "train loss:0.028170956122266846\n",
      "train loss:0.022041787991533907\n",
      "train loss:0.00536809829759266\n",
      "train loss:0.047635148604551494\n",
      "train loss:0.011397365360889618\n",
      "train loss:0.0377093198466436\n",
      "train loss:0.026680053365238453\n",
      "train loss:0.028323535606399938\n",
      "train loss:0.12250299146736887\n",
      "train loss:0.027925492014080233\n",
      "train loss:0.03959615177587051\n",
      "train loss:0.035495101511486896\n",
      "train loss:0.08859233261025863\n",
      "train loss:0.12796336764423574\n",
      "train loss:0.02330716652106227\n",
      "train loss:0.035430089248712715\n",
      "train loss:0.014112111319660394\n",
      "train loss:0.025654075643855356\n",
      "train loss:0.04005809291612289\n",
      "train loss:0.04735544308747417\n",
      "train loss:0.06418910729495204\n",
      "train loss:0.013084317490315136\n",
      "train loss:0.06217283053851836\n",
      "train loss:0.02909352730758801\n",
      "train loss:0.019076816030247745\n",
      "train loss:0.05118037100965076\n",
      "train loss:0.011104249037566905\n",
      "train loss:0.02710738309248655\n",
      "train loss:0.07062225322009065\n",
      "train loss:0.0342273492791854\n",
      "train loss:0.004105621858195187\n",
      "train loss:0.05160160607760946\n",
      "train loss:0.043403103674588064\n",
      "train loss:0.009809444829984753\n",
      "train loss:0.020181493562204233\n",
      "train loss:0.04210146886992713\n",
      "train loss:0.0239969466822923\n",
      "train loss:0.02868556752162537\n",
      "train loss:0.025852447510678087\n",
      "train loss:0.035821738685952016\n",
      "train loss:0.022199366881102387\n",
      "train loss:0.043180366055170255\n",
      "train loss:0.04073936367345371\n",
      "train loss:0.05548262971774344\n",
      "train loss:0.01970812961618535\n",
      "train loss:0.03733320932153239\n",
      "train loss:0.04586889281421042\n",
      "train loss:0.07809655934193432\n",
      "train loss:0.020282559412490256\n",
      "train loss:0.023903357480674636\n",
      "train loss:0.024237629268374233\n",
      "train loss:0.05845920457179152\n",
      "train loss:0.010850320690945152\n",
      "train loss:0.02750187776009495\n",
      "train loss:0.017269395289116786\n",
      "train loss:0.037084909024997594\n",
      "train loss:0.06078103146900683\n",
      "train loss:0.079603899513767\n",
      "train loss:0.020234194845980037\n",
      "train loss:0.015339266492853608\n",
      "train loss:0.07856928093753178\n",
      "train loss:0.010982945002149674\n",
      "train loss:0.060960950299728166\n",
      "train loss:0.07733790751654278\n",
      "train loss:0.031975733992721994\n",
      "train loss:0.028151188927513986\n",
      "train loss:0.0641860557075531\n",
      "train loss:0.07682219246664306\n",
      "train loss:0.011658484010885716\n",
      "train loss:0.042036502563458124\n",
      "train loss:0.047586626205391586\n",
      "train loss:0.06807639326095034\n",
      "train loss:0.016887865055100523\n",
      "train loss:0.030120579983825196\n",
      "train loss:0.026727005352920462\n",
      "train loss:0.027531308466996484\n",
      "train loss:0.042990353904043606\n",
      "train loss:0.0409944499953869\n",
      "train loss:0.011151056407294333\n",
      "train loss:0.043779742336646325\n",
      "train loss:0.03681412866609183\n",
      "train loss:0.030073385133822347\n",
      "train loss:0.026983302532977052\n",
      "train loss:0.03581865073688772\n",
      "train loss:0.06774771849296651\n",
      "train loss:0.030177599837196766\n",
      "train loss:0.03492691569489831\n",
      "train loss:0.03003147390287369\n",
      "train loss:0.04589443554466868\n",
      "train loss:0.042067106489299844\n",
      "train loss:0.03652104327919729\n",
      "train loss:0.024730079104289746\n",
      "train loss:0.01804385386252902\n",
      "train loss:0.05414372854178302\n",
      "train loss:0.051342478110353806\n",
      "train loss:0.08390391967928867\n",
      "train loss:0.038833864378552276\n",
      "train loss:0.03243828041207389\n",
      "train loss:0.014921650545773057\n",
      "train loss:0.0072853658140899094\n",
      "train loss:0.11246760521994512\n",
      "train loss:0.011904787256975276\n",
      "train loss:0.025748883087895257\n",
      "train loss:0.02331178917757109\n",
      "train loss:0.026192883008016744\n",
      "train loss:0.06054732906519316\n",
      "train loss:0.063807766849606\n",
      "train loss:0.18415231466520715\n",
      "train loss:0.03126327002902021\n",
      "train loss:0.045954257652934045\n",
      "train loss:0.01004052723403165\n",
      "train loss:0.01232177902957352\n",
      "train loss:0.04586819282056874\n",
      "train loss:0.01910240329440986\n",
      "train loss:0.058819163663405824\n",
      "train loss:0.016946869576903893\n",
      "train loss:0.038031472255391816\n",
      "train loss:0.019904135153096206\n",
      "train loss:0.03443594410315474\n",
      "train loss:0.02557333932454724\n",
      "train loss:0.021093379778452378\n",
      "train loss:0.04748950519137165\n",
      "train loss:0.05268273828588601\n",
      "train loss:0.04165814513123454\n",
      "train loss:0.014905332935125836\n",
      "train loss:0.014644694481091269\n",
      "train loss:0.11047540357191309\n",
      "train loss:0.01846934094456393\n",
      "train loss:0.030483169229378156\n",
      "train loss:0.0766035982200695\n",
      "train loss:0.09067748964757592\n",
      "train loss:0.045800421291804255\n",
      "train loss:0.03633118786581063\n",
      "train loss:0.03825034883851285\n",
      "train loss:0.021010265690233135\n",
      "train loss:0.02739815167433422\n",
      "train loss:0.032054793753771016\n",
      "train loss:0.01306523236323371\n",
      "train loss:0.03495082897738509\n",
      "train loss:0.02152878447326934\n",
      "train loss:0.018039816914909725\n",
      "train loss:0.01696686008309219\n",
      "train loss:0.031988296186299586\n",
      "train loss:0.013908968697989594\n",
      "train loss:0.007450144950663521\n",
      "train loss:0.009554417017978416\n",
      "train loss:0.024011859497952016\n",
      "train loss:0.041730556728973515\n",
      "train loss:0.06264717717622609\n",
      "train loss:0.10355955406368972\n",
      "train loss:0.025843765745549297\n",
      "train loss:0.11636777267787946\n",
      "train loss:0.028733947387370073\n",
      "train loss:0.006969767143540258\n",
      "train loss:0.03853760981639187\n",
      "train loss:0.03881670957980879\n",
      "train loss:0.02287452234906389\n",
      "train loss:0.011247604134205851\n",
      "train loss:0.041980330357408266\n",
      "train loss:0.023433438119526515\n",
      "train loss:0.012096070209179159\n",
      "train loss:0.028767627774707188\n",
      "train loss:0.03330621665459622\n",
      "train loss:0.12921731446433785\n",
      "train loss:0.031072381692702145\n",
      "train loss:0.05125414067319942\n",
      "train loss:0.012091818096714526\n",
      "train loss:0.10008957485541257\n",
      "train loss:0.056001511607482304\n",
      "train loss:0.0981052200927033\n",
      "train loss:0.01776860402890087\n",
      "train loss:0.008607949807641194\n",
      "train loss:0.04422045382615138\n",
      "train loss:0.06093787619077599\n",
      "train loss:0.006045678046948277\n",
      "train loss:0.04761684873164684\n",
      "train loss:0.03220266576876941\n",
      "train loss:0.028331483721822197\n",
      "train loss:0.016843514373830804\n",
      "train loss:0.0046381944432815955\n",
      "train loss:0.03339027775685255\n",
      "train loss:0.015792697882249466\n",
      "train loss:0.02287417110823071\n",
      "train loss:0.010719263756040683\n",
      "train loss:0.027931765674779126\n",
      "train loss:0.012437890795190129\n",
      "train loss:0.07943953881445903\n",
      "train loss:0.10451814203381858\n",
      "train loss:0.05325177635053973\n",
      "train loss:0.06488897752363548\n",
      "train loss:0.07324648346543268\n",
      "train loss:0.0312258388827812\n",
      "train loss:0.030298272517833062\n",
      "train loss:0.01894137536273153\n",
      "train loss:0.06316396370390687\n",
      "train loss:0.06376861796120809\n",
      "train loss:0.0667316491347438\n",
      "train loss:0.014747542583840135\n",
      "train loss:0.012866063737792833\n",
      "train loss:0.00918896346048291\n",
      "train loss:0.026359009504409014\n",
      "train loss:0.08663514170681395\n",
      "train loss:0.032173575564073076\n",
      "train loss:0.06475510851606768\n",
      "train loss:0.11258904980826939\n",
      "train loss:0.029872777489267692\n",
      "train loss:0.02277466692609871\n",
      "train loss:0.01953677706784264\n",
      "train loss:0.01556815502748558\n",
      "train loss:0.028188601098410943\n",
      "train loss:0.08801429703990199\n",
      "train loss:0.010733714419167923\n",
      "train loss:0.023271013590888824\n",
      "train loss:0.037292411510419725\n",
      "train loss:0.022750637803660833\n",
      "train loss:0.05130830293836943\n",
      "train loss:0.05959645582527275\n",
      "train loss:0.005091142934482565\n",
      "train loss:0.008373790912751908\n",
      "train loss:0.04149516169276028\n",
      "train loss:0.00739378843689957\n",
      "train loss:0.03577007239927298\n",
      "train loss:0.048495623362526004\n",
      "train loss:0.06484597503974318\n",
      "train loss:0.0274210119796678\n",
      "train loss:0.007987371831444134\n",
      "train loss:0.047207953318640916\n",
      "train loss:0.042716441003284186\n",
      "train loss:0.01864513757668434\n",
      "train loss:0.06951823168808445\n",
      "train loss:0.04867875270430251\n",
      "train loss:0.019955981943493458\n",
      "train loss:0.03075533919361511\n",
      "train loss:0.011174927267847735\n",
      "train loss:0.06413318362831531\n",
      "train loss:0.016339896102185735\n",
      "train loss:0.04841058409399675\n",
      "train loss:0.032578831269363825\n",
      "train loss:0.027965960439411363\n",
      "train loss:0.023114868425148823\n",
      "train loss:0.008809361245683173\n",
      "train loss:0.07742166329317407\n",
      "train loss:0.05314114775919825\n",
      "train loss:0.006251759171598686\n",
      "train loss:0.11923129036974661\n",
      "train loss:0.03205039730564445\n",
      "train loss:0.0701731022329987\n",
      "train loss:0.01868333434401047\n",
      "train loss:0.01428537208233691\n",
      "train loss:0.08600073032799083\n",
      "train loss:0.08404245801572177\n",
      "train loss:0.041524873862607885\n",
      "train loss:0.009730538421389864\n",
      "train loss:0.041891858018738716\n",
      "train loss:0.06731924444372943\n",
      "train loss:0.021945791970924167\n",
      "train loss:0.07073664148174486\n",
      "train loss:0.07110690983210775\n",
      "train loss:0.01262672862801551\n",
      "train loss:0.0489703522435256\n",
      "train loss:0.03124500551462638\n",
      "train loss:0.016656603041289526\n",
      "train loss:0.052567029899226014\n",
      "train loss:0.03683859071903922\n",
      "train loss:0.011109459835408684\n",
      "train loss:0.04613682747576471\n",
      "train loss:0.05152748532063449\n",
      "train loss:0.061687432439577444\n",
      "train loss:0.01208764877928703\n",
      "train loss:0.047457241985148634\n",
      "train loss:0.02463137252766033\n",
      "train loss:0.030438197349635442\n",
      "train loss:0.050297975588807074\n",
      "train loss:0.0340816618007057\n",
      "train loss:0.038911730922011285\n",
      "train loss:0.022039927347523674\n",
      "train loss:0.008584446174614253\n",
      "train loss:0.03414534323354545\n",
      "train loss:0.03931470383527049\n",
      "train loss:0.024756612231160366\n",
      "train loss:0.022778740555862965\n",
      "train loss:0.013793558933334046\n",
      "train loss:0.047466578761676405\n",
      "train loss:0.03569370115623825\n",
      "train loss:0.027614763504494252\n",
      "train loss:0.00825923049187965\n",
      "train loss:0.015130224231148902\n",
      "train loss:0.05816243654759707\n",
      "train loss:0.04573470480888485\n",
      "train loss:0.006589700908309111\n",
      "train loss:0.04111410340969773\n",
      "train loss:0.038746223838135474\n",
      "train loss:0.0210831539742543\n",
      "train loss:0.03250607374029274\n",
      "train loss:0.03962577055878613\n",
      "train loss:0.02834042475540114\n",
      "train loss:0.026343489394860597\n",
      "train loss:0.09773535140550296\n",
      "train loss:0.04220816903657149\n",
      "train loss:0.021164537962766565\n",
      "train loss:0.02232911527518281\n",
      "train loss:0.032956154371805345\n",
      "train loss:0.014668100350621903\n",
      "train loss:0.019107442312125994\n",
      "train loss:0.028427749763132587\n",
      "train loss:0.03101018624850901\n",
      "train loss:0.0409526344861797\n",
      "train loss:0.043214385487177244\n",
      "train loss:0.018445632307468388\n",
      "train loss:0.021894842812475036\n",
      "train loss:0.01037388958195389\n",
      "train loss:0.02106508709146144\n",
      "train loss:0.008359320313656871\n",
      "train loss:0.03407950502420392\n",
      "train loss:0.009814181099739224\n",
      "train loss:0.10101341479380663\n",
      "train loss:0.04241982971849231\n",
      "train loss:0.010541196677221061\n",
      "train loss:0.023189594349570877\n",
      "train loss:0.01797834476109858\n",
      "train loss:0.042034625018799704\n",
      "train loss:0.054683516593975086\n",
      "train loss:0.019505464381814275\n",
      "train loss:0.05364599220801968\n",
      "train loss:0.02112174141020416\n",
      "train loss:0.054250141513125405\n",
      "train loss:0.00858486286858165\n",
      "train loss:0.022331053305473123\n",
      "train loss:0.011640518425940867\n",
      "train loss:0.010077243330442864\n",
      "train loss:0.04401040710471842\n",
      "train loss:0.05269617792684898\n",
      "train loss:0.020635666948979194\n",
      "train loss:0.04565153444390282\n",
      "train loss:0.01885819921277885\n",
      "train loss:0.0258056706941603\n",
      "train loss:0.06245828623259439\n",
      "train loss:0.007289917227913853\n",
      "train loss:0.007719387293118134\n",
      "train loss:0.034850491148659624\n",
      "train loss:0.05841373535248999\n",
      "train loss:0.00954991710671525\n",
      "train loss:0.029722998225781696\n",
      "train loss:0.021062634068252727\n",
      "train loss:0.026767819521782515\n",
      "train loss:0.02749083891505761\n",
      "train loss:0.03861820625268863\n",
      "train loss:0.06247879083173667\n",
      "train loss:0.019037178042129507\n",
      "train loss:0.018350209644352616\n",
      "train loss:0.04022964887449296\n",
      "train loss:0.07558677233811022\n",
      "train loss:0.01866159550153204\n",
      "train loss:0.03644199929709389\n",
      "train loss:0.0297719834201795\n",
      "train loss:0.009568223899971692\n",
      "train loss:0.037260687314916295\n",
      "train loss:0.040828190935945655\n",
      "train loss:0.008306817187143753\n",
      "train loss:0.07425424819609144\n",
      "train loss:0.09572392438098232\n",
      "train loss:0.042550443941317795\n",
      "train loss:0.047755227797460546\n",
      "train loss:0.03456647180992849\n",
      "train loss:0.0266351285651195\n",
      "train loss:0.01407418335941683\n",
      "train loss:0.03826114236301817\n",
      "train loss:0.052202594639731\n",
      "train loss:0.034814123646768\n",
      "train loss:0.04069449149553297\n",
      "train loss:0.015125192934628176\n",
      "train loss:0.0730498980515371\n",
      "train loss:0.026219595001477827\n",
      "train loss:0.036866763363163364\n",
      "train loss:0.0175985480674467\n",
      "train loss:0.016987461243375114\n",
      "train loss:0.013328825161495042\n",
      "train loss:0.024993816716893956\n",
      "train loss:0.021426439668290328\n",
      "train loss:0.023423394742402737\n",
      "train loss:0.026567379677697574\n",
      "train loss:0.03951998198563178\n",
      "train loss:0.04722370929992858\n",
      "train loss:0.0030549584002122886\n",
      "train loss:0.021662684709474266\n",
      "train loss:0.05082500080860001\n",
      "train loss:0.06816024306419562\n",
      "train loss:0.022818421824470424\n",
      "train loss:0.02623873982917004\n",
      "train loss:0.03614844143971741\n",
      "train loss:0.009837622296364344\n",
      "train loss:0.013553024548982722\n",
      "train loss:0.019259422765064045\n",
      "train loss:0.02612010593905677\n",
      "train loss:0.06218107758235172\n",
      "train loss:0.13759841021103253\n",
      "train loss:0.049069641147919035\n",
      "train loss:0.040315070914465796\n",
      "train loss:0.007680640669298744\n",
      "train loss:0.058024008148374856\n",
      "train loss:0.012530165927960002\n",
      "train loss:0.005026126845449459\n",
      "train loss:0.016081617697077475\n",
      "train loss:0.1405575111914022\n",
      "train loss:0.03580812621039615\n",
      "train loss:0.05124484267799482\n",
      "train loss:0.033149115712404344\n",
      "train loss:0.0172809522569631\n",
      "train loss:0.06298695221225344\n",
      "train loss:0.016501010064425656\n",
      "train loss:0.012606830574268197\n",
      "train loss:0.012507361725310744\n",
      "train loss:0.01467512793990104\n",
      "train loss:0.008395776071442873\n",
      "train loss:0.0149256846817696\n",
      "train loss:0.05109677983252667\n",
      "train loss:0.01819336635592039\n",
      "train loss:0.03993578353620582\n",
      "train loss:0.014419914080091679\n",
      "train loss:0.006808521308129497\n",
      "train loss:0.010552378148617552\n",
      "train loss:0.021829627447670003\n",
      "train loss:0.12049377612239395\n",
      "train loss:0.029490904488635773\n",
      "train loss:0.021461212802847896\n",
      "train loss:0.03776692574675676\n",
      "train loss:0.027185149082213377\n",
      "train loss:0.01781788530405257\n",
      "train loss:0.04583983729901758\n",
      "train loss:0.02460743358506109\n",
      "train loss:0.011007568389111313\n",
      "train loss:0.018789025775043048\n",
      "train loss:0.0352805911367686\n",
      "train loss:0.014084217360494262\n",
      "train loss:0.0593895192822284\n",
      "train loss:0.013911815952233356\n",
      "train loss:0.07154298957254754\n",
      "train loss:0.028084076335266838\n",
      "train loss:0.06509455849971658\n",
      "train loss:0.09627013690922645\n",
      "train loss:0.04324990198502213\n",
      "train loss:0.05495366322246312\n",
      "train loss:0.049404696447215325\n",
      "train loss:0.037718643013561336\n",
      "train loss:0.0777799493837807\n",
      "train loss:0.04434105505461474\n",
      "train loss:0.045273777131465236\n",
      "train loss:0.06166182449183654\n",
      "train loss:0.03898202173933807\n",
      "train loss:0.016410274923419697\n",
      "train loss:0.01458767636253778\n",
      "train loss:0.04079424483060019\n",
      "train loss:0.11119763252552001\n",
      "train loss:0.027728396142579718\n",
      "train loss:0.05448145215745755\n",
      "train loss:0.01914852145934396\n",
      "train loss:0.012778844564918098\n",
      "train loss:0.007446955942977507\n",
      "train loss:0.015715408586170352\n",
      "train loss:0.023356040192660462\n",
      "train loss:0.10634091582516182\n",
      "train loss:0.02871082607156074\n",
      "train loss:0.012047769611270695\n",
      "train loss:0.01834145507057841\n",
      "train loss:0.023341733622004514\n",
      "train loss:0.024126119209149047\n",
      "train loss:0.015732487540219695\n",
      "train loss:0.015441996190808195\n",
      "train loss:0.01052588412605153\n",
      "train loss:0.0823506700846176\n",
      "train loss:0.016636177794634256\n",
      "train loss:0.016584248986648772\n",
      "train loss:0.025681583816767613\n",
      "train loss:0.01891925608793443\n",
      "train loss:0.010934772142105817\n",
      "train loss:0.04297641149078248\n",
      "train loss:0.024089952291228487\n",
      "train loss:0.010941046404315763\n",
      "train loss:0.013575106697055313\n",
      "train loss:0.0065410070855537175\n",
      "train loss:0.015355140908258623\n",
      "train loss:0.040558081781949064\n",
      "train loss:0.05474090889769506\n",
      "train loss:0.06947603550193901\n",
      "train loss:0.01825723864695551\n",
      "train loss:0.03556855177931314\n",
      "train loss:0.020126707831157192\n",
      "train loss:0.025311055290298187\n",
      "train loss:0.024500197593583444\n",
      "train loss:0.03141918808711663\n",
      "train loss:0.12057248931954295\n",
      "train loss:0.019375252448854675\n",
      "train loss:0.008706284432088178\n",
      "train loss:0.03186536034640687\n",
      "train loss:0.02597879578894389\n",
      "train loss:0.012313995135113586\n",
      "train loss:0.021587444251058404\n",
      "train loss:0.01690767562910744\n",
      "train loss:0.02049359189766757\n",
      "train loss:0.037694136814347134\n",
      "train loss:0.02282866920716781\n",
      "train loss:0.06737891645203933\n",
      "train loss:0.0782987326196046\n",
      "train loss:0.019219744711338728\n",
      "train loss:0.04826180607075349\n",
      "train loss:0.013585744280123706\n",
      "train loss:0.016498531248591682\n",
      "train loss:0.02267816581106934\n",
      "train loss:0.019433794782317916\n",
      "train loss:0.04833641047216464\n",
      "train loss:0.01655769860950118\n",
      "train loss:0.014398384164150149\n",
      "train loss:0.05535688633801462\n",
      "train loss:0.023163420472389215\n",
      "train loss:0.031118411622374925\n",
      "train loss:0.06052497611841045\n",
      "train loss:0.07146976866990239\n",
      "train loss:0.030282277614391385\n",
      "train loss:0.010722207082187043\n",
      "train loss:0.00945789662765317\n",
      "train loss:0.05708152136500632\n",
      "train loss:0.055282362555871004\n",
      "train loss:0.009600460603342935\n",
      "train loss:0.048282532337510994\n",
      "train loss:0.01457335917598072\n",
      "train loss:0.06918388212312762\n",
      "train loss:0.03750472807712765\n",
      "train loss:0.029481147134680752\n",
      "train loss:0.02917762021987383\n",
      "train loss:0.010294094816101516\n",
      "train loss:0.01725806893738588\n",
      "train loss:0.023179566296985827\n",
      "train loss:0.07012692129159749\n",
      "train loss:0.03503041819670233\n",
      "train loss:0.03400778637532289\n",
      "train loss:0.016061983997183515\n",
      "train loss:0.014895820187752365\n",
      "train loss:0.06511232754369002\n",
      "train loss:0.008565065287469056\n",
      "train loss:0.03542532469906971\n",
      "train loss:0.017063213310751082\n",
      "train loss:0.042628774480611996\n",
      "train loss:0.04455214053039112\n",
      "train loss:0.04392438644604117\n",
      "train loss:0.03456419269246062\n",
      "train loss:0.0263839453561919\n",
      "train loss:0.06030729848464022\n",
      "train loss:0.004261710013284725\n",
      "train loss:0.019800051573880727\n",
      "train loss:0.011876818115571477\n",
      "train loss:0.05206333696028207\n",
      "train loss:0.06122118697098669\n",
      "train loss:0.017266141118649672\n",
      "train loss:0.02678326428666512\n",
      "train loss:0.011128438876606814\n",
      "train loss:0.023344573049124354\n",
      "train loss:0.023438574332183073\n",
      "train loss:0.023213109530387185\n",
      "train loss:0.0228863573776341\n",
      "train loss:0.023970295972346546\n",
      "train loss:0.04911103697893563\n",
      "=== epoch:5, train acc:0.988, test acc:0.977 ===\n",
      "train loss:0.006588489231928333\n",
      "train loss:0.013396945527060035\n",
      "train loss:0.03312738598731352\n",
      "train loss:0.015423374287296234\n",
      "train loss:0.018517579426310965\n",
      "train loss:0.04002332772990817\n",
      "train loss:0.027195699411342363\n",
      "train loss:0.02335883627223506\n",
      "train loss:0.00740424096478571\n",
      "train loss:0.013716712887684559\n",
      "train loss:0.008957239963570182\n",
      "train loss:0.029162442893135278\n",
      "train loss:0.014774849363463299\n",
      "train loss:0.041327752937953474\n",
      "train loss:0.0364698208060807\n",
      "train loss:0.014940973681674971\n",
      "train loss:0.057241144612662616\n",
      "train loss:0.003646580461866018\n",
      "train loss:0.04596460421853916\n",
      "train loss:0.01921874098744555\n",
      "train loss:0.05890382759596035\n",
      "train loss:0.014219058006395282\n",
      "train loss:0.033688369327106005\n",
      "train loss:0.03316054721304428\n",
      "train loss:0.013321026115838701\n",
      "train loss:0.01824305971520024\n",
      "train loss:0.005200998252672915\n",
      "train loss:0.03181510741691731\n",
      "train loss:0.03241441976137413\n",
      "train loss:0.007488930329839191\n",
      "train loss:0.027120272580514432\n",
      "train loss:0.014086507913722022\n",
      "train loss:0.024687760119096597\n",
      "train loss:0.02481384889115284\n",
      "train loss:0.006998476896502845\n",
      "train loss:0.011459586516898991\n",
      "train loss:0.01277264945893318\n",
      "train loss:0.013859850461092425\n",
      "train loss:0.03464204941577856\n",
      "train loss:0.006831153793562217\n",
      "train loss:0.054427321237050415\n",
      "train loss:0.031425366656675405\n",
      "train loss:0.016338528717115747\n",
      "train loss:0.021402000439238872\n",
      "train loss:0.03599118147014193\n",
      "train loss:0.09879266502587424\n",
      "train loss:0.05771060532523998\n",
      "train loss:0.10523777991861726\n",
      "train loss:0.03024724553900172\n",
      "train loss:0.033199798324114196\n",
      "train loss:0.026931036961385692\n",
      "train loss:0.02792540272093981\n",
      "train loss:0.024697174500882935\n",
      "train loss:0.021585001491476873\n",
      "train loss:0.03654224150261712\n",
      "train loss:0.03345626631822155\n",
      "train loss:0.050856268936684206\n",
      "train loss:0.05376849430415266\n",
      "train loss:0.007988137706619314\n",
      "train loss:0.09647015126789311\n",
      "train loss:0.016229805853570204\n",
      "train loss:0.027682721886650854\n",
      "train loss:0.07392443776041228\n",
      "train loss:0.00937255974517764\n",
      "train loss:0.004660908671758716\n",
      "train loss:0.13830917475881577\n",
      "train loss:0.025825868485237095\n",
      "train loss:0.06158100253178266\n",
      "train loss:0.012249537044924631\n",
      "train loss:0.06291795261717156\n",
      "train loss:0.02185221178607593\n",
      "train loss:0.06454142359792966\n",
      "train loss:0.019242107167345646\n",
      "train loss:0.07560878959011394\n",
      "train loss:0.018514646229037205\n",
      "train loss:0.020964246989586443\n",
      "train loss:0.027357630554697053\n",
      "train loss:0.034592505629720265\n",
      "train loss:0.05160043279224249\n",
      "train loss:0.09376130873325453\n",
      "train loss:0.03971701034861152\n",
      "train loss:0.004057101775415101\n",
      "train loss:0.03335585598283527\n",
      "train loss:0.015289415535263823\n",
      "train loss:0.048098369815279575\n",
      "train loss:0.03438523220151184\n",
      "train loss:0.012794118961670615\n",
      "train loss:0.07351004162482465\n",
      "train loss:0.018851746939996597\n",
      "train loss:0.08324532985661554\n",
      "train loss:0.015131116201742541\n",
      "train loss:0.007257247076142741\n",
      "train loss:0.029518285720844407\n",
      "train loss:0.037166815488863476\n",
      "train loss:0.060603184133526555\n",
      "train loss:0.031632354020809356\n",
      "train loss:0.010340759241224805\n",
      "train loss:0.03218951333899719\n",
      "train loss:0.033980652247271356\n",
      "train loss:0.023497445469036916\n",
      "train loss:0.04810611537103695\n",
      "train loss:0.023035216433793936\n",
      "train loss:0.024509579446215297\n",
      "train loss:0.013302875755455355\n",
      "train loss:0.027678214485946567\n",
      "train loss:0.011163867830298081\n",
      "train loss:0.021064810089966392\n",
      "train loss:0.013830815546779476\n",
      "train loss:0.024170454212232115\n",
      "train loss:0.024801225183076463\n",
      "train loss:0.024536938820833548\n",
      "train loss:0.003363309289155321\n",
      "train loss:0.011001819213414312\n",
      "train loss:0.05509829571671111\n",
      "train loss:0.01879467736888813\n",
      "train loss:0.015736013745334797\n",
      "train loss:0.02252857100904464\n",
      "train loss:0.033785284968289614\n",
      "train loss:0.038238473478072486\n",
      "train loss:0.027616529320591995\n",
      "train loss:0.02673002069837914\n",
      "train loss:0.03121793574547522\n",
      "train loss:0.06016184940124944\n",
      "train loss:0.006754714322468559\n",
      "train loss:0.023377224476900292\n",
      "train loss:0.023471204071405412\n",
      "train loss:0.012971863058537212\n",
      "train loss:0.015837566067227204\n",
      "train loss:0.007986399332343408\n",
      "train loss:0.0185215988354119\n",
      "train loss:0.06487747086457278\n",
      "train loss:0.019906855225215278\n",
      "train loss:0.011592141589112363\n",
      "train loss:0.027603423970048516\n",
      "train loss:0.044185316644449385\n",
      "train loss:0.0319743904189612\n",
      "train loss:0.02221764543586037\n",
      "train loss:0.08376006258230316\n",
      "train loss:0.03779783234318954\n",
      "train loss:0.012247638385729951\n",
      "train loss:0.08043850366742414\n",
      "train loss:0.026433411893936314\n",
      "train loss:0.15598424813840572\n",
      "train loss:0.02442771840896769\n",
      "train loss:0.02657757268930067\n",
      "train loss:0.012119202199043791\n",
      "train loss:0.03284554322848145\n",
      "train loss:0.045149072848939695\n",
      "train loss:0.041097172673994875\n",
      "train loss:0.038945581037598404\n",
      "train loss:0.15230160322020436\n",
      "train loss:0.03188023049601959\n",
      "train loss:0.04731846265193193\n",
      "train loss:0.037724093617081456\n",
      "train loss:0.04017078603142171\n",
      "train loss:0.01656866236191456\n",
      "train loss:0.05599231863172582\n",
      "train loss:0.1066578843939256\n",
      "train loss:0.03762451511850196\n",
      "train loss:0.04557109413559294\n",
      "train loss:0.039354416393232984\n",
      "train loss:0.03559292040814699\n",
      "train loss:0.0332793758739203\n",
      "train loss:0.017915987533729374\n",
      "train loss:0.018904819306913893\n",
      "train loss:0.07236908488422021\n",
      "train loss:0.04754272609153218\n",
      "train loss:0.014986260834888998\n",
      "train loss:0.03227867364250618\n",
      "train loss:0.021520811523166608\n",
      "train loss:0.015672522772752467\n",
      "train loss:0.024255461918933817\n",
      "train loss:0.07492301960327982\n",
      "train loss:0.07552971718673743\n",
      "train loss:0.01212094291664827\n",
      "train loss:0.0377502793776497\n",
      "train loss:0.04062416002921155\n",
      "train loss:0.020613998597861796\n",
      "train loss:0.048568354384645095\n",
      "train loss:0.0076610651352318785\n",
      "train loss:0.03686804950961052\n",
      "train loss:0.008772294361959938\n",
      "train loss:0.010163080362719538\n",
      "train loss:0.015353539321761516\n",
      "train loss:0.030394942988868367\n",
      "train loss:0.03381836348971384\n",
      "train loss:0.00857978981999339\n",
      "train loss:0.008607318962620234\n",
      "train loss:0.0013986781655252437\n",
      "train loss:0.006709785577346299\n",
      "train loss:0.014170888289607584\n",
      "train loss:0.01636212992540188\n",
      "train loss:0.014258463647603614\n",
      "train loss:0.009576427989445553\n",
      "train loss:0.0225984323402272\n",
      "train loss:0.1363964624138138\n",
      "train loss:0.007317044099038602\n",
      "train loss:0.06918250263815039\n",
      "train loss:0.03360433785795523\n",
      "train loss:0.08407445727230133\n",
      "train loss:0.03703919966604835\n",
      "train loss:0.062349909398210175\n",
      "train loss:0.04013354431753585\n",
      "train loss:0.023849525135963705\n",
      "train loss:0.07433167353504523\n",
      "train loss:0.041934806323012344\n",
      "train loss:0.02155720050486887\n",
      "train loss:0.08302114114152792\n",
      "train loss:0.05299388808614613\n",
      "train loss:0.007076521898318887\n",
      "train loss:0.012326157445426414\n",
      "train loss:0.020997438321582927\n",
      "train loss:0.025292459616428897\n",
      "train loss:0.01617180554583131\n",
      "train loss:0.010626834954306412\n",
      "train loss:0.008892734209951174\n",
      "train loss:0.016354897163298198\n",
      "train loss:0.015697401905126123\n",
      "train loss:0.10607198125125938\n",
      "train loss:0.025417905357214905\n",
      "train loss:0.060082184215187386\n",
      "train loss:0.013892557684848612\n",
      "train loss:0.02634940877774378\n",
      "train loss:0.0046488566850903705\n",
      "train loss:0.04335208288521228\n",
      "train loss:0.007468956905806468\n",
      "train loss:0.04196631819850301\n",
      "train loss:0.04086686950327124\n",
      "train loss:0.0277684397330821\n",
      "train loss:0.01659402741639373\n",
      "train loss:0.06762755250108321\n",
      "train loss:0.01165716135946451\n",
      "train loss:0.05550201446900162\n",
      "train loss:0.021106552847767635\n",
      "train loss:0.029489275233809727\n",
      "train loss:0.013584414566191291\n",
      "train loss:0.04275562938605283\n",
      "train loss:0.011294288060567488\n",
      "train loss:0.015417104428093426\n",
      "train loss:0.017576221395670236\n",
      "train loss:0.009904709989475935\n",
      "train loss:0.03148715930051704\n",
      "train loss:0.09483547738764689\n",
      "train loss:0.04896600667905525\n",
      "train loss:0.007681357152967372\n",
      "train loss:0.027300572890184477\n",
      "train loss:0.01385490014379094\n",
      "train loss:0.013082155219787011\n",
      "train loss:0.008220469701500121\n",
      "train loss:0.014626775082626654\n",
      "train loss:0.01923897826168115\n",
      "train loss:0.008316863104120718\n",
      "train loss:0.006520935085222013\n",
      "train loss:0.02099924025352591\n",
      "train loss:0.014539810939142035\n",
      "train loss:0.01215672662426112\n",
      "train loss:0.024837880836403127\n",
      "train loss:0.12814173146039035\n",
      "train loss:0.026807298898754665\n",
      "train loss:0.01230478064581383\n",
      "train loss:0.00883935596820157\n",
      "train loss:0.01904871222715993\n",
      "train loss:0.03172779682055661\n",
      "train loss:0.03807841456017091\n",
      "train loss:0.02248619235766337\n",
      "train loss:0.044902729335374925\n",
      "train loss:0.026841112232366373\n",
      "train loss:0.024632721747648776\n",
      "train loss:0.041036066052941876\n",
      "train loss:0.03102132686111654\n",
      "train loss:0.03623630345765083\n",
      "train loss:0.02547501009586205\n",
      "train loss:0.034656109511041845\n",
      "train loss:0.058397333398318256\n",
      "train loss:0.002536656702451518\n",
      "train loss:0.12448821698680847\n",
      "train loss:0.0340307120447617\n",
      "train loss:0.04826998171055047\n",
      "train loss:0.0223458291035709\n",
      "train loss:0.026605838545147455\n",
      "train loss:0.013657561008002443\n",
      "train loss:0.03886034962199597\n",
      "train loss:0.019113598361386812\n",
      "train loss:0.01750742826194083\n",
      "train loss:0.03356454336673491\n",
      "train loss:0.021654953672238977\n",
      "train loss:0.02029021102624744\n",
      "train loss:0.020515348527964418\n",
      "train loss:0.0065284884637138805\n",
      "train loss:0.046069295612902864\n",
      "train loss:0.014373805747830974\n",
      "train loss:0.02703590381591408\n",
      "train loss:0.06062249128451108\n",
      "train loss:0.03281445994661965\n",
      "train loss:0.024168904661120302\n",
      "train loss:0.013787551709721133\n",
      "train loss:0.01348583427973586\n",
      "train loss:0.07365140235241957\n",
      "train loss:0.015502030528222683\n",
      "train loss:0.031585602482572084\n",
      "train loss:0.027893699011430916\n",
      "train loss:0.020843960973913788\n",
      "train loss:0.043112364986364705\n",
      "train loss:0.010167239854902296\n",
      "train loss:0.0570404881641252\n",
      "train loss:0.008447173209227493\n",
      "train loss:0.01451112810328045\n",
      "train loss:0.018227714774666087\n",
      "train loss:0.005940263836098172\n",
      "train loss:0.013009236062911536\n",
      "train loss:0.0097591541956399\n",
      "train loss:0.007355718060919455\n",
      "train loss:0.03340333252756512\n",
      "train loss:0.019472138305006666\n",
      "train loss:0.01176763486513691\n",
      "train loss:0.015545924379544598\n",
      "train loss:0.053599053938706585\n",
      "train loss:0.042973732794246905\n",
      "train loss:0.06433707792578104\n",
      "train loss:0.06699197596308604\n",
      "train loss:0.04159563374470098\n",
      "train loss:0.025831229543298962\n",
      "train loss:0.025209970282528107\n",
      "train loss:0.05026593981061727\n",
      "train loss:0.011760403909980642\n",
      "train loss:0.019497030884652402\n",
      "train loss:0.007978709899622898\n",
      "train loss:0.13019972981587716\n",
      "train loss:0.03976089253082074\n",
      "train loss:0.012701244944103803\n",
      "train loss:0.05883081999641278\n",
      "train loss:0.023820778409312304\n",
      "train loss:0.03567382265118821\n",
      "train loss:0.01984306374424874\n",
      "train loss:0.014503460222133885\n",
      "train loss:0.009034510049150193\n",
      "train loss:0.05305855119635695\n",
      "train loss:0.014489550777906458\n",
      "train loss:0.1126507994970761\n",
      "train loss:0.03922511733696833\n",
      "train loss:0.03833266638565286\n",
      "train loss:0.013348170570955959\n",
      "train loss:0.0528871411081307\n",
      "train loss:0.0171118558667268\n",
      "train loss:0.018514927992448558\n",
      "train loss:0.02479793526239117\n",
      "train loss:0.03616868498021212\n",
      "train loss:0.013264667890586314\n",
      "train loss:0.007122533052058621\n",
      "train loss:0.038015856568309025\n",
      "train loss:0.019246826284992677\n",
      "train loss:0.01265066845370564\n",
      "train loss:0.01736259603507183\n",
      "train loss:0.017648148986988223\n",
      "train loss:0.021748478233808148\n",
      "train loss:0.015010135115534388\n",
      "train loss:0.008822801711934588\n",
      "train loss:0.014752731017979888\n",
      "train loss:0.05600235770013216\n",
      "train loss:0.05098741377150201\n",
      "train loss:0.021957488235096537\n",
      "train loss:0.010327557964045234\n",
      "train loss:0.04909148138300087\n",
      "train loss:0.010860415301863262\n",
      "train loss:0.022392293213499434\n",
      "train loss:0.018744579645419743\n",
      "train loss:0.010269878753965796\n",
      "train loss:0.010588255626518788\n",
      "train loss:0.09915015988065264\n",
      "train loss:0.012705138504691732\n",
      "train loss:0.013361733540870408\n",
      "train loss:0.022911754025628908\n",
      "train loss:0.011102557633242988\n",
      "train loss:0.018474945131967454\n",
      "train loss:0.012407352209676812\n",
      "train loss:0.024362636214065102\n",
      "train loss:0.04324500269376749\n",
      "train loss:0.012865158646947062\n",
      "train loss:0.020773771662829246\n",
      "train loss:0.011941695899720793\n",
      "train loss:0.09419840528436643\n",
      "train loss:0.020377414014022727\n",
      "train loss:0.08269800264278478\n",
      "train loss:0.03723868849179238\n",
      "train loss:0.025276906446775847\n",
      "train loss:0.021853917090414444\n",
      "train loss:0.0024449598856471806\n",
      "train loss:0.047437528534863764\n",
      "train loss:0.010532075738574018\n",
      "train loss:0.039370185537704945\n",
      "train loss:0.02115420604220402\n",
      "train loss:0.007233370083917407\n",
      "train loss:0.030126531654176985\n",
      "train loss:0.02931657284723772\n",
      "train loss:0.018689093437357664\n",
      "train loss:0.0026316012213000992\n",
      "train loss:0.032624609574251624\n",
      "train loss:0.0196874024566564\n",
      "train loss:0.0400421080981602\n",
      "train loss:0.017801919444742808\n",
      "train loss:0.04538100346975388\n",
      "train loss:0.005164600962669716\n",
      "train loss:0.012005869478424106\n",
      "train loss:0.004835061035917512\n",
      "train loss:0.013694267274195916\n",
      "train loss:0.004406008931941637\n",
      "train loss:0.08760018508464466\n",
      "train loss:0.016230811569644153\n",
      "train loss:0.013848256010501771\n",
      "train loss:0.03510439593169165\n",
      "train loss:0.031243776937363302\n",
      "train loss:0.0028055730194179863\n",
      "train loss:0.02236330647240519\n",
      "train loss:0.04988936535465191\n",
      "train loss:0.08479789338789749\n",
      "train loss:0.015761323017492647\n",
      "train loss:0.049312916204161394\n",
      "train loss:0.005968573391204374\n",
      "train loss:0.019617329179108002\n",
      "train loss:0.03252178453741526\n",
      "train loss:0.033209068035272786\n",
      "train loss:0.05961192346718666\n",
      "train loss:0.009421949181221507\n",
      "train loss:0.0819829076265095\n",
      "train loss:0.007504766375141197\n",
      "train loss:0.013875105265015942\n",
      "train loss:0.027034694984803775\n",
      "train loss:0.006602155887538804\n",
      "train loss:0.015209447212855625\n",
      "train loss:0.0038026710633929754\n",
      "train loss:0.03347359678236884\n",
      "train loss:0.004531331621037935\n",
      "train loss:0.047598720950002775\n",
      "train loss:0.015016078973066269\n",
      "train loss:0.02915538450226726\n",
      "train loss:0.003691705745165663\n",
      "train loss:0.02613427743802475\n",
      "train loss:0.08675908173066503\n",
      "train loss:0.0328310914286533\n",
      "train loss:0.010587732046092431\n",
      "train loss:0.010247013434128309\n",
      "train loss:0.10760849647169275\n",
      "train loss:0.030156975636384392\n",
      "train loss:0.07476668836254838\n",
      "train loss:0.03522115908551373\n",
      "train loss:0.02986622477777233\n",
      "train loss:0.01042094408366393\n",
      "train loss:0.01869535336973034\n",
      "train loss:0.0349484278318211\n",
      "train loss:0.08712113743742834\n",
      "train loss:0.004345709948308824\n",
      "train loss:0.04534976256114887\n",
      "train loss:0.005890154212962179\n",
      "train loss:0.0714079365798828\n",
      "train loss:0.025763265396232273\n",
      "train loss:0.039908646467420435\n",
      "train loss:0.016334634118422342\n",
      "train loss:0.03460532746428422\n",
      "train loss:0.03513478409576116\n",
      "train loss:0.015109709879942156\n",
      "train loss:0.04420252797372124\n",
      "train loss:0.029758294109203873\n",
      "train loss:0.012927766686699023\n",
      "train loss:0.05576186705031644\n",
      "train loss:0.004923749635076001\n",
      "train loss:0.02313089399141692\n",
      "train loss:0.005690324505175069\n",
      "train loss:0.01971493691859633\n",
      "train loss:0.01385421799420407\n",
      "train loss:0.07393553955589849\n",
      "train loss:0.014684988861737486\n",
      "train loss:0.019260923700796226\n",
      "train loss:0.030527560498179677\n",
      "train loss:0.037511806207075435\n",
      "train loss:0.018222984946127715\n",
      "train loss:0.005088079667015719\n",
      "train loss:0.021360151987200558\n",
      "train loss:0.013907808675242907\n",
      "train loss:0.029467024307244776\n",
      "train loss:0.027096105026680938\n",
      "train loss:0.012804368403712565\n",
      "train loss:0.0056549334436399624\n",
      "train loss:0.010920070873842861\n",
      "train loss:0.02483171932908782\n",
      "train loss:0.017030574237363667\n",
      "train loss:0.00850110822748594\n",
      "train loss:0.01909593144761345\n",
      "train loss:0.03062645197667889\n",
      "train loss:0.021398735117982296\n",
      "train loss:0.00960279063699033\n",
      "train loss:0.04795643734165176\n",
      "train loss:0.007079609520886622\n",
      "train loss:0.014526512101650147\n",
      "train loss:0.0038569671005206957\n",
      "train loss:0.04375392383166122\n",
      "train loss:0.010325675956433628\n",
      "train loss:0.038524095110731425\n",
      "train loss:0.026558017526016962\n",
      "train loss:0.011899666873141772\n",
      "train loss:0.006033755178390605\n",
      "train loss:0.019994306329091677\n",
      "train loss:0.006286172663964996\n",
      "train loss:0.020965518060928258\n",
      "train loss:0.015971149438019822\n",
      "train loss:0.07422693870648808\n",
      "train loss:0.027997551008772784\n",
      "train loss:0.03931436449951339\n",
      "train loss:0.05434943829831116\n",
      "train loss:0.06586128418683818\n",
      "train loss:0.04826769172314051\n",
      "train loss:0.013307482650542525\n",
      "train loss:0.04177875873024773\n",
      "train loss:0.011775642990526018\n",
      "train loss:0.013074195442841465\n",
      "train loss:0.027792474577421913\n",
      "train loss:0.011593377531057492\n",
      "train loss:0.018239767565881203\n",
      "train loss:0.06956265175146165\n",
      "train loss:0.01189296966088119\n",
      "train loss:0.06271229679263224\n",
      "train loss:0.006240520251256262\n",
      "train loss:0.013716040207052941\n",
      "train loss:0.03530321067898365\n",
      "train loss:0.01859878209549254\n",
      "train loss:0.012342301458871196\n",
      "train loss:0.0075228610634846226\n",
      "train loss:0.006687153960964841\n",
      "train loss:0.00664653522965249\n",
      "train loss:0.009029846443226984\n",
      "train loss:0.010608373245514766\n",
      "train loss:0.03259885216194199\n",
      "train loss:0.012155548380907438\n",
      "train loss:0.012710129681638194\n",
      "train loss:0.025203450994887057\n",
      "train loss:0.039227567977652075\n",
      "train loss:0.06630371096498218\n",
      "train loss:0.015973497916543792\n",
      "train loss:0.013394872887906856\n",
      "train loss:0.06446887179954984\n",
      "train loss:0.01848424687355477\n",
      "train loss:0.009232483615769994\n",
      "train loss:0.020214533004488385\n",
      "train loss:0.008745595580837007\n",
      "train loss:0.006413916620018885\n",
      "train loss:0.08117944098785422\n",
      "train loss:0.010393200322837186\n",
      "train loss:0.0031572579310710407\n",
      "train loss:0.014268139535027908\n",
      "train loss:0.011278264882905044\n",
      "train loss:0.033172498717576035\n",
      "train loss:0.016704878007648225\n",
      "train loss:0.032468263816218534\n",
      "train loss:0.008947327341526696\n",
      "train loss:0.011469639485046486\n",
      "train loss:0.0030647477065512065\n",
      "train loss:0.010967856951314685\n",
      "train loss:0.015947815126712103\n",
      "train loss:0.0580374957563222\n",
      "train loss:0.021730065463500422\n",
      "train loss:0.00911434849905921\n",
      "train loss:0.014093858885002275\n",
      "train loss:0.03303516326042132\n",
      "train loss:0.017118650091959682\n",
      "train loss:0.05218162220194711\n",
      "train loss:0.029033091433640676\n",
      "train loss:0.03464682435023911\n",
      "train loss:0.022940642245877097\n",
      "train loss:0.011547757379296133\n",
      "train loss:0.0062453666237953995\n",
      "train loss:0.00921079275591643\n",
      "train loss:0.008138444047154123\n",
      "train loss:0.011827114015931587\n",
      "train loss:0.015178659598913416\n",
      "train loss:0.01440350759945589\n",
      "train loss:0.04771932527932326\n",
      "train loss:0.036530061008423806\n",
      "train loss:0.008316966980818326\n",
      "train loss:0.0012097846502661458\n",
      "train loss:0.023671487336626953\n",
      "train loss:0.025299932049175303\n",
      "train loss:0.02270680521438965\n",
      "train loss:0.010261432686226256\n",
      "train loss:0.018357270566716356\n",
      "train loss:0.02495359063241602\n",
      "train loss:0.004736273995465532\n",
      "train loss:0.004616434975306828\n",
      "train loss:0.020867238660750534\n",
      "train loss:0.023215971540610187\n",
      "train loss:0.040204535865419985\n",
      "train loss:0.006893064308623862\n",
      "train loss:0.05105690283433175\n",
      "train loss:0.014146858091917833\n",
      "train loss:0.008727907199889725\n",
      "train loss:0.009022712553458448\n",
      "train loss:0.008617218039830418\n",
      "train loss:0.004618681412640388\n",
      "train loss:0.005659987672388903\n",
      "train loss:0.01586375356764998\n",
      "train loss:0.003920881778586438\n",
      "train loss:0.018791295353741074\n",
      "=== epoch:6, train acc:0.986, test acc:0.98 ===\n",
      "train loss:0.09168342160558575\n",
      "train loss:0.005509236516007394\n",
      "train loss:0.022238953896132223\n",
      "train loss:0.03767373954423361\n",
      "train loss:0.025993432547678323\n",
      "train loss:0.0167626003842936\n",
      "train loss:0.024337330391663584\n",
      "train loss:0.0041533077365399785\n",
      "train loss:0.011475385574111407\n",
      "train loss:0.050982674812810984\n",
      "train loss:0.03756260654261753\n",
      "train loss:0.010800569905341218\n",
      "train loss:0.05435814465777954\n",
      "train loss:0.00408055147392598\n",
      "train loss:0.06337318591187646\n",
      "train loss:0.008150195336926725\n",
      "train loss:0.02812458590490472\n",
      "train loss:0.03188226085830398\n",
      "train loss:0.0167406739196248\n",
      "train loss:0.025095411787937647\n",
      "train loss:0.07244593535763064\n",
      "train loss:0.012857260687936037\n",
      "train loss:0.05691387999016913\n",
      "train loss:0.035041266924863985\n",
      "train loss:0.001942018702116141\n",
      "train loss:0.057829761572935935\n",
      "train loss:0.006520487812899849\n",
      "train loss:0.01972239749333726\n",
      "train loss:0.007080477306140973\n",
      "train loss:0.032096261511420095\n",
      "train loss:0.03730796322352637\n",
      "train loss:0.05194943786394871\n",
      "train loss:0.01672312387051932\n",
      "train loss:0.010099945210405541\n",
      "train loss:0.02625093037481536\n",
      "train loss:0.07250633577315703\n",
      "train loss:0.020388941506062682\n",
      "train loss:0.01609596684794465\n",
      "train loss:0.014690633523842924\n",
      "train loss:0.08208611152692447\n",
      "train loss:0.0065531504965093725\n",
      "train loss:0.011677374816470878\n",
      "train loss:0.004311748432797562\n",
      "train loss:0.014376242815677278\n",
      "train loss:0.03856708910577236\n",
      "train loss:0.020935751698308393\n",
      "train loss:0.03177388050773177\n",
      "train loss:0.007166647402107037\n",
      "train loss:0.027589379554841306\n",
      "train loss:0.009587619756974652\n",
      "train loss:0.014301230778477234\n",
      "train loss:0.027230631519287098\n",
      "train loss:0.037411639801011164\n",
      "train loss:0.001819456928389775\n",
      "train loss:0.014900452271376565\n",
      "train loss:0.008181509040584954\n",
      "train loss:0.021862269159710082\n",
      "train loss:0.015182802095534333\n",
      "train loss:0.010243563963196627\n",
      "train loss:0.034493122135819794\n",
      "train loss:0.02279568838534112\n",
      "train loss:0.10197709415292851\n",
      "train loss:0.006416532235443555\n",
      "train loss:0.0029378147470051808\n",
      "train loss:0.024913518290040598\n",
      "train loss:0.015736460333514127\n",
      "train loss:0.008568724213016438\n",
      "train loss:0.01972509151042387\n",
      "train loss:0.03949083113868172\n",
      "train loss:0.016029347298537125\n",
      "train loss:0.016205764568249632\n",
      "train loss:0.007698136767221334\n",
      "train loss:0.0031649709957881395\n",
      "train loss:0.03383572681680029\n",
      "train loss:0.004603687311538578\n",
      "train loss:0.00775715493549094\n",
      "train loss:0.013474341785126652\n",
      "train loss:0.01500217604777436\n",
      "train loss:0.03493859819815196\n",
      "train loss:0.015539966003563855\n",
      "train loss:0.05636339477955767\n",
      "train loss:0.013106038066860693\n",
      "train loss:0.015340986996528852\n",
      "train loss:0.00838903464603942\n",
      "train loss:0.006810027492670866\n",
      "train loss:0.005029880987027537\n",
      "train loss:0.07876788436355583\n",
      "train loss:0.028243276067841657\n",
      "train loss:0.01044283898288229\n",
      "train loss:0.03589478777907234\n",
      "train loss:0.00476621367398602\n",
      "train loss:0.02690221477270749\n",
      "train loss:0.01918282474980165\n",
      "train loss:0.06260553179111197\n",
      "train loss:0.005868938690238469\n",
      "train loss:0.009634255108265887\n",
      "train loss:0.003063442943276673\n",
      "train loss:0.03621867908804586\n",
      "train loss:0.012224465167416322\n",
      "train loss:0.03011377053978537\n",
      "train loss:0.026047652358761622\n",
      "train loss:0.015941065922643383\n",
      "train loss:0.014583065233129768\n",
      "train loss:0.029918951694462795\n",
      "train loss:0.006813404983665003\n",
      "train loss:0.08873108063045444\n",
      "train loss:0.08432306191986022\n",
      "train loss:0.008104592170071041\n",
      "train loss:0.013361517599679964\n",
      "train loss:0.007585456995972994\n",
      "train loss:0.016545822039154\n",
      "train loss:0.019703700291890086\n",
      "train loss:0.0767659957505192\n",
      "train loss:0.0033256818858334496\n",
      "train loss:0.01040647616964279\n",
      "train loss:0.02191406644194012\n",
      "train loss:0.0065912182166570745\n",
      "train loss:0.00602549876836273\n",
      "train loss:0.04697501332495388\n",
      "train loss:0.026482714866381537\n",
      "train loss:0.05828761681569034\n",
      "train loss:0.007738917117723072\n",
      "train loss:0.0203479672100042\n",
      "train loss:0.006692028093079897\n",
      "train loss:0.028418884067827478\n",
      "train loss:0.018493871914452974\n",
      "train loss:0.014330892477360744\n",
      "train loss:0.011616741503674944\n",
      "train loss:0.016847475193768703\n",
      "train loss:0.02661378984844934\n",
      "train loss:0.026987054902704855\n",
      "train loss:0.02322877341885018\n",
      "train loss:0.06031850896066205\n",
      "train loss:0.07199146826857543\n",
      "train loss:0.01921023343794309\n",
      "train loss:0.013525720470509688\n",
      "train loss:0.02739655052554452\n",
      "train loss:0.041767671052408584\n",
      "train loss:0.018137343279465735\n",
      "train loss:0.040914505550819165\n",
      "train loss:0.04330370729180691\n",
      "train loss:0.007717912345127712\n",
      "train loss:0.01221834469564303\n",
      "train loss:0.020010547805498137\n",
      "train loss:0.01301259375605595\n",
      "train loss:0.07369266653336676\n",
      "train loss:0.005519241065297438\n",
      "train loss:0.022387396581552282\n",
      "train loss:0.09943569731548367\n",
      "train loss:0.024299845691119638\n",
      "train loss:0.01522735508608579\n",
      "train loss:0.05596076282320294\n",
      "train loss:0.043998025634961774\n",
      "train loss:0.02788273654415729\n",
      "train loss:0.010992467341520753\n",
      "train loss:0.008427525257946647\n",
      "train loss:0.09499045225065877\n",
      "train loss:0.006619821728584216\n",
      "train loss:0.015116201813861481\n",
      "train loss:0.010625706955219664\n",
      "train loss:0.0864000257066882\n",
      "train loss:0.015554373379611582\n",
      "train loss:0.05676363140208344\n",
      "train loss:0.03458936721716201\n",
      "train loss:0.03567695549282116\n",
      "train loss:0.009525832215195465\n",
      "train loss:0.018405906676954284\n",
      "train loss:0.01107000376721281\n",
      "train loss:0.01869270889577744\n",
      "train loss:0.035602214553227596\n",
      "train loss:0.01710869962083375\n",
      "train loss:0.055395319549176675\n",
      "train loss:0.015411459742404977\n",
      "train loss:0.010107559165184774\n",
      "train loss:0.0077714004093560884\n",
      "train loss:0.010315970591339067\n",
      "train loss:0.00429830256917044\n",
      "train loss:0.021488069713701624\n",
      "train loss:0.016976240473378985\n",
      "train loss:0.059607271174786265\n",
      "train loss:0.01625650664360095\n",
      "train loss:0.08363058928078432\n",
      "train loss:0.03144244219419515\n",
      "train loss:0.003387518566138402\n",
      "train loss:0.0516881233497273\n",
      "train loss:0.05078046961492383\n",
      "train loss:0.03335050496140509\n",
      "train loss:0.04925079125746325\n",
      "train loss:0.04371970786143998\n",
      "train loss:0.040172643134199555\n",
      "train loss:0.016857639664116807\n",
      "train loss:0.029777207229363147\n",
      "train loss:0.08680360840942354\n",
      "train loss:0.020529109309701768\n",
      "train loss:0.02079829327652265\n",
      "train loss:0.025881182845097838\n",
      "train loss:0.026944245633733516\n",
      "train loss:0.0068538816858519265\n",
      "train loss:0.021958489572677972\n",
      "train loss:0.00774551372449811\n",
      "train loss:0.009314247197238404\n",
      "train loss:0.008609428724478453\n",
      "train loss:0.03521538397213415\n",
      "train loss:0.01495605995364303\n",
      "train loss:0.015556084313522702\n",
      "train loss:0.018003796189631976\n",
      "train loss:0.0024203125727175544\n",
      "train loss:0.004919270998090442\n",
      "train loss:0.006634932850470683\n",
      "train loss:0.06505476106291666\n",
      "train loss:0.008295501671409529\n",
      "train loss:0.02269553569127336\n",
      "train loss:0.025932375766799787\n",
      "train loss:0.009864657044352293\n",
      "train loss:0.020558123119981498\n",
      "train loss:0.00198976250368578\n",
      "train loss:0.07562389129128075\n",
      "train loss:0.017893929594925154\n",
      "train loss:0.016991248729676424\n",
      "train loss:0.007560987577416712\n",
      "train loss:0.025557468868643705\n",
      "train loss:0.02272066970778508\n",
      "train loss:0.022746724147833186\n",
      "train loss:0.14397232072007032\n",
      "train loss:0.041827393738220905\n",
      "train loss:0.03755100219432219\n",
      "train loss:0.007908644937717977\n",
      "train loss:0.015349536169946458\n",
      "train loss:0.009268299669560085\n",
      "train loss:0.01742880842042881\n",
      "train loss:0.016719706811909818\n",
      "train loss:0.007159381724648952\n",
      "train loss:0.026496101567180932\n",
      "train loss:0.008294607340959388\n",
      "train loss:0.03717774747810203\n",
      "train loss:0.005373816648917753\n",
      "train loss:0.02721191676183866\n",
      "train loss:0.009364601325416014\n",
      "train loss:0.048429620309116475\n",
      "train loss:0.060215910736873754\n",
      "train loss:0.008649136055795587\n",
      "train loss:0.01593681753440018\n",
      "train loss:0.005895131664734189\n",
      "train loss:0.011753650776901506\n",
      "train loss:0.014825521749116858\n",
      "train loss:0.005173406471519276\n",
      "train loss:0.015349230937070446\n",
      "train loss:0.030634908068535128\n",
      "train loss:0.08687256134474326\n",
      "train loss:0.004108141009887541\n",
      "train loss:0.004628447759988553\n",
      "train loss:0.023539880466072645\n",
      "train loss:0.03199275270749061\n",
      "train loss:0.008461450847332491\n",
      "train loss:0.0173920202542667\n",
      "train loss:0.02774817845137133\n",
      "train loss:0.057310624874299484\n",
      "train loss:0.008426645712344274\n",
      "train loss:0.012214997092336604\n",
      "train loss:0.08585378412269547\n",
      "train loss:0.020522402796510682\n",
      "train loss:0.007159636902355825\n",
      "train loss:0.0030775443875686644\n",
      "train loss:0.03795093596914925\n",
      "train loss:0.0188506282421236\n",
      "train loss:0.003979587591060687\n",
      "train loss:0.01662540512880702\n",
      "train loss:0.016520733311089986\n",
      "train loss:0.023180305261724158\n",
      "train loss:0.024024854303699942\n",
      "train loss:0.03783061443438102\n",
      "train loss:0.007567174845393389\n",
      "train loss:0.046338402860909636\n",
      "train loss:0.011836479529045237\n",
      "train loss:0.024074808283590224\n",
      "train loss:0.02693410781801326\n",
      "train loss:0.011388570790945502\n",
      "train loss:0.025804462823440955\n",
      "train loss:0.006145789911853754\n",
      "train loss:0.03197438082108854\n",
      "train loss:0.028044932285129335\n",
      "train loss:0.019439614893947865\n",
      "train loss:0.11119592774467536\n",
      "train loss:0.08972940496022203\n",
      "train loss:0.06375537325182998\n",
      "train loss:0.014694799554136748\n",
      "train loss:0.05917484494324003\n",
      "train loss:0.03290650313544685\n",
      "train loss:0.07861674632839666\n",
      "train loss:0.01397836571318283\n",
      "train loss:0.002765173701531165\n",
      "train loss:0.013191694892563116\n",
      "train loss:0.03978493902164333\n",
      "train loss:0.03190080161063416\n",
      "train loss:0.024898196521162305\n",
      "train loss:0.15634670280989924\n",
      "train loss:0.0162076532762387\n",
      "train loss:0.0369331023042234\n",
      "train loss:0.027918084612576926\n",
      "train loss:0.018198997118837525\n",
      "train loss:0.006217583740916209\n",
      "train loss:0.02894566229050866\n",
      "train loss:0.030834654886358216\n",
      "train loss:0.01762325128225296\n",
      "train loss:0.052650361169417924\n",
      "train loss:0.005772619642233868\n",
      "train loss:0.005679609402646387\n",
      "train loss:0.020050665766487417\n",
      "train loss:0.013732218848692697\n",
      "train loss:0.0043914671460181885\n",
      "train loss:0.016739009304148067\n",
      "train loss:0.009101424750510458\n",
      "train loss:0.009470360904373799\n",
      "train loss:0.02168572289792519\n",
      "train loss:0.04364102744662903\n",
      "train loss:0.020568202650598787\n",
      "train loss:0.02367468405128266\n",
      "train loss:0.00862146158135444\n",
      "train loss:0.01827953958450119\n",
      "train loss:0.019429683929590175\n",
      "train loss:0.015156946274152866\n",
      "train loss:0.01255539807433939\n",
      "train loss:0.021653733140096013\n",
      "train loss:0.00870803265463922\n",
      "train loss:0.014614740050446225\n",
      "train loss:0.009400692168313128\n",
      "train loss:0.015167162753309125\n",
      "train loss:0.05063862792474085\n",
      "train loss:0.023096578696389515\n",
      "train loss:0.0686720959040681\n",
      "train loss:0.0723194103629025\n",
      "train loss:0.10525525742799523\n",
      "train loss:0.010847424643191404\n",
      "train loss:0.008850277151508682\n",
      "train loss:0.015238750971500396\n",
      "train loss:0.24050861748661473\n",
      "train loss:0.033191453865979166\n",
      "train loss:0.02446950736521008\n",
      "train loss:0.008225519262218388\n",
      "train loss:0.03308381348267175\n",
      "train loss:0.013489550432357576\n",
      "train loss:0.002853455493494496\n",
      "train loss:0.004479647454908372\n",
      "train loss:0.004261331691738615\n",
      "train loss:0.014772108499554331\n",
      "train loss:0.00750890074874983\n",
      "train loss:0.0066813677396241585\n",
      "train loss:0.022441057055904056\n",
      "train loss:0.018816194137628396\n",
      "train loss:0.011378251563783384\n",
      "train loss:0.03629854337501576\n",
      "train loss:0.01800187754556879\n",
      "train loss:0.024758943231342107\n",
      "train loss:0.01767616748796377\n",
      "train loss:0.007613449183308947\n",
      "train loss:0.03666135515153391\n",
      "train loss:0.0162545067205539\n",
      "train loss:0.0363160848588973\n",
      "train loss:0.010025516901604753\n",
      "train loss:0.003203930927672571\n",
      "train loss:0.04380114417375876\n",
      "train loss:0.007093408372057933\n",
      "train loss:0.00416895018884301\n",
      "train loss:0.04140324079540596\n",
      "train loss:0.0049237888358151585\n",
      "train loss:0.00568295655574325\n",
      "train loss:0.029233833891784675\n",
      "train loss:0.013812447683446931\n",
      "train loss:0.0271214519812994\n",
      "train loss:0.013333363213723992\n",
      "train loss:0.03915594664825856\n",
      "train loss:0.005257262417491453\n",
      "train loss:0.009602479750420284\n",
      "train loss:0.03627321285516721\n",
      "train loss:0.013173410029419423\n",
      "train loss:0.014552342673290823\n",
      "train loss:0.023669236109079256\n",
      "train loss:0.02110927922028911\n",
      "train loss:0.04433896361811533\n",
      "train loss:0.004628290696846089\n",
      "train loss:0.02051800650550285\n",
      "train loss:0.016812983639823767\n",
      "train loss:0.013546311180263997\n",
      "train loss:0.018239793249995064\n",
      "train loss:0.012951621299031475\n",
      "train loss:0.009636506824694874\n",
      "train loss:0.03722275988041593\n",
      "train loss:0.007804542732596248\n",
      "train loss:0.06936606214079126\n",
      "train loss:0.05614860397866686\n",
      "train loss:0.020122414298455838\n",
      "train loss:0.017448446873174487\n",
      "train loss:0.016156296701097063\n",
      "train loss:0.006355358613164837\n",
      "train loss:0.011031662142254973\n",
      "train loss:0.011162332155266292\n",
      "train loss:0.007516939027153852\n",
      "train loss:0.023646261636618145\n",
      "train loss:0.009702610007922295\n",
      "train loss:0.12423213750425242\n",
      "train loss:0.005531470151509914\n",
      "train loss:0.0024724897441915296\n",
      "train loss:0.0030795559769398826\n",
      "train loss:0.03579739412739765\n",
      "train loss:0.002746693305415314\n",
      "train loss:0.003577109786915559\n",
      "train loss:0.00671752310814579\n",
      "train loss:0.022849370832417438\n",
      "train loss:0.006526431555516506\n",
      "train loss:0.08088784733583841\n",
      "train loss:0.050127003279114166\n",
      "train loss:0.011044232302600147\n",
      "train loss:0.026169202650567384\n",
      "train loss:0.011118502827036816\n",
      "train loss:0.008231296320048147\n",
      "train loss:0.043369881211880464\n",
      "train loss:0.009082943317279773\n",
      "train loss:0.004390989430971691\n",
      "train loss:0.011674125398202638\n",
      "train loss:0.0027985185867213875\n",
      "train loss:0.007387307108253388\n",
      "train loss:0.018538577106858162\n",
      "train loss:0.01730884561642138\n",
      "train loss:0.004578334589139548\n",
      "train loss:0.009375098570649344\n",
      "train loss:0.003559716092153797\n",
      "train loss:0.023168840682834188\n",
      "train loss:0.004578891545773975\n",
      "train loss:0.007183183532261329\n",
      "train loss:0.028943088445282705\n",
      "train loss:0.04324416581864988\n",
      "train loss:0.015433241530523755\n",
      "train loss:0.01109992665030948\n",
      "train loss:0.02808095105003794\n",
      "train loss:0.013768973591444286\n",
      "train loss:0.0083907467957204\n",
      "train loss:0.0922262085278781\n",
      "train loss:0.018703650417554278\n",
      "train loss:0.018821243776518223\n",
      "train loss:0.0014052379885689797\n",
      "train loss:0.006901087617590953\n",
      "train loss:0.02031302512807081\n",
      "train loss:0.025235255226492295\n",
      "train loss:0.015761174943476734\n",
      "train loss:0.008475088825358579\n",
      "train loss:0.08738347684851704\n",
      "train loss:0.020298810810454735\n",
      "train loss:0.002626205469612617\n",
      "train loss:0.02459715438965518\n",
      "train loss:0.014059740218912646\n",
      "train loss:0.03129732225196843\n",
      "train loss:0.009658621863650295\n",
      "train loss:0.025383751386845024\n",
      "train loss:0.007963309573676166\n",
      "train loss:0.014794615039933935\n",
      "train loss:0.08487182585578079\n",
      "train loss:0.034780796666602616\n",
      "train loss:0.009458094658361515\n",
      "train loss:0.010959764756010317\n",
      "train loss:0.006721813083101936\n",
      "train loss:0.01977431851735354\n",
      "train loss:0.007464136301073997\n",
      "train loss:0.022334100658622243\n",
      "train loss:0.0370951478329546\n",
      "train loss:0.030616228692134366\n",
      "train loss:0.006495007286213123\n",
      "train loss:0.004555697847674326\n",
      "train loss:0.010929131723621423\n",
      "train loss:0.005362744629495515\n",
      "train loss:0.06438654135829017\n",
      "train loss:0.019207734846709262\n",
      "train loss:0.003820996814914789\n",
      "train loss:0.01655471904398263\n",
      "train loss:0.01527026812294351\n",
      "train loss:0.009572938461540465\n",
      "train loss:0.006036373243906663\n",
      "train loss:0.0466339171670201\n",
      "train loss:0.006382244079422975\n",
      "train loss:0.013350561517564984\n",
      "train loss:0.00782992557739976\n",
      "train loss:0.006433805311746351\n",
      "train loss:0.03344882118490022\n",
      "train loss:0.007831122805417378\n",
      "train loss:0.01353920070298373\n",
      "train loss:0.0015711859608509573\n",
      "train loss:0.009425415348845492\n",
      "train loss:0.007441956882504062\n",
      "train loss:0.009980600416190534\n",
      "train loss:0.03453861913340065\n",
      "train loss:0.014543470499429523\n",
      "train loss:0.010731938462280084\n",
      "train loss:0.05032729659920276\n",
      "train loss:0.013494281814159938\n",
      "train loss:0.008712569755855016\n",
      "train loss:0.07142534602099675\n",
      "train loss:0.024556704835812827\n",
      "train loss:0.005636270387014468\n",
      "train loss:0.01149608534865239\n",
      "train loss:0.027175928109712085\n",
      "train loss:0.016684952469906783\n",
      "train loss:0.027988478639312357\n",
      "train loss:0.008366650764660928\n",
      "train loss:0.0049842822012382\n",
      "train loss:0.001158384166627001\n",
      "train loss:0.08497908808555821\n",
      "train loss:0.033531842147238373\n",
      "train loss:0.014785732466746136\n",
      "train loss:0.013635716562624583\n",
      "train loss:0.02046079138378623\n",
      "train loss:0.02967466334576401\n",
      "train loss:0.018189091201540462\n",
      "train loss:0.020231858418905194\n",
      "train loss:0.02212859537348674\n",
      "train loss:0.037682566134779105\n",
      "train loss:0.00350206054849954\n",
      "train loss:0.01902013185518309\n",
      "train loss:0.02071736277837123\n",
      "train loss:0.00723797132428877\n",
      "train loss:0.018454598994690544\n",
      "train loss:0.04336879164191666\n",
      "train loss:0.005542385736280445\n",
      "train loss:0.028186145921232256\n",
      "train loss:0.025656406784398515\n",
      "train loss:0.015232177746719845\n",
      "train loss:0.014246374082265027\n",
      "train loss:0.002863557105750045\n",
      "train loss:0.003927304188107463\n",
      "train loss:0.006272650247977474\n",
      "train loss:0.0033438629964106853\n",
      "train loss:0.035530137373424475\n",
      "train loss:0.06329466205838799\n",
      "train loss:0.002963965922043046\n",
      "train loss:0.013968735326206651\n",
      "train loss:0.019213819898317788\n",
      "train loss:0.010108989845263976\n",
      "train loss:0.020042171689603823\n",
      "train loss:0.00733955832331294\n",
      "train loss:0.025026557782992963\n",
      "train loss:0.04233901866931204\n",
      "train loss:0.021893863906542403\n",
      "train loss:0.002891149932254446\n",
      "train loss:0.009842588989834762\n",
      "train loss:0.01807861979702625\n",
      "train loss:0.0039384920763143845\n",
      "train loss:0.01713176521299184\n",
      "train loss:0.0262747547156277\n",
      "train loss:0.04689625088289496\n",
      "train loss:0.011820183635670375\n",
      "train loss:0.0032071361437175983\n",
      "train loss:0.02279445913115657\n",
      "train loss:0.0011138652406714044\n",
      "train loss:0.05654451121971338\n",
      "train loss:0.022620116268434007\n",
      "train loss:0.028107274534509574\n",
      "train loss:0.02991883900844139\n",
      "train loss:0.0573790968738268\n",
      "train loss:0.011968512703865775\n",
      "train loss:0.008485668939637475\n",
      "train loss:0.007295355022630202\n",
      "train loss:0.00912311275436641\n",
      "train loss:0.012729113891329335\n",
      "train loss:0.021962685668292257\n",
      "train loss:0.03260313739197991\n",
      "train loss:0.008628433299899036\n",
      "train loss:0.018744628662233604\n",
      "train loss:0.021463080290110684\n",
      "train loss:0.009202012474969167\n",
      "train loss:0.005997725124059355\n",
      "train loss:0.019017351039470626\n",
      "train loss:0.005702010709515459\n",
      "train loss:0.01473653993351275\n",
      "train loss:0.010600326072396016\n",
      "train loss:0.004871560833552057\n",
      "train loss:0.03833109043843368\n",
      "train loss:0.06083950794781998\n",
      "train loss:0.015213026049540428\n",
      "train loss:0.04473567073997711\n",
      "train loss:0.015789982086097897\n",
      "train loss:0.006480590855090319\n",
      "train loss:0.015663629598716672\n",
      "train loss:0.00911996183621543\n",
      "train loss:0.027952144432085914\n",
      "train loss:0.015650732173381893\n",
      "train loss:0.006225966020559466\n",
      "train loss:0.007877422651858085\n",
      "train loss:0.006371835562881535\n",
      "train loss:0.012714942281776682\n",
      "train loss:0.019481147684254085\n",
      "train loss:0.0034709454618657687\n",
      "train loss:0.012715698174788815\n",
      "train loss:0.0406195013502344\n",
      "train loss:0.04282544136892213\n",
      "train loss:0.0018985683894908285\n",
      "train loss:0.007718717199118968\n",
      "train loss:0.005410657566931504\n",
      "train loss:0.012536685684812325\n",
      "train loss:0.03788144014956914\n",
      "train loss:0.004030388807237657\n",
      "train loss:0.025302886440188167\n",
      "train loss:0.014766800350436022\n",
      "=== epoch:7, train acc:0.989, test acc:0.978 ===\n",
      "train loss:0.044103681977166305\n",
      "train loss:0.014895326783382123\n",
      "train loss:0.011902322261372347\n",
      "train loss:0.010598640986423245\n",
      "train loss:0.036124316374326886\n",
      "train loss:0.004055099335869238\n",
      "train loss:0.014637145300077353\n",
      "train loss:0.02034793310230094\n",
      "train loss:0.012193399744579881\n",
      "train loss:0.04529622663392762\n",
      "train loss:0.04681429433657237\n",
      "train loss:0.008656623339940968\n",
      "train loss:0.03141714700340599\n",
      "train loss:0.012569681627356727\n",
      "train loss:0.011970322363856726\n",
      "train loss:0.008647821402078116\n",
      "train loss:0.008820674970183084\n",
      "train loss:0.026175386499270134\n",
      "train loss:0.009902629242584997\n",
      "train loss:0.005089630802052572\n",
      "train loss:0.023282208907794942\n",
      "train loss:0.0077467482634363235\n",
      "train loss:0.0031760885369435782\n",
      "train loss:0.025229982842981044\n",
      "train loss:0.01410725076602925\n",
      "train loss:0.003998857196513455\n",
      "train loss:0.06310853336179024\n",
      "train loss:0.05205771938376426\n",
      "train loss:0.0237465480656662\n",
      "train loss:0.027599253542497915\n",
      "train loss:0.007159912421214149\n",
      "train loss:0.012232440332929426\n",
      "train loss:0.01259981135908477\n",
      "train loss:0.06149969283856125\n",
      "train loss:0.008105826199893829\n",
      "train loss:0.008792708512635021\n",
      "train loss:0.003011886862043965\n",
      "train loss:0.000624168992479277\n",
      "train loss:0.03970840665713829\n",
      "train loss:0.003635642226862335\n",
      "train loss:0.015947760218886647\n",
      "train loss:0.007913369370632826\n",
      "train loss:0.028950853842378613\n",
      "train loss:0.01084475671144524\n",
      "train loss:0.014638733781469628\n",
      "train loss:0.004502923190583241\n",
      "train loss:0.021081280320051\n",
      "train loss:0.005447074514557531\n",
      "train loss:0.07905560101627954\n",
      "train loss:0.016447913424187277\n",
      "train loss:0.012612324779328525\n",
      "train loss:0.022099116370272648\n",
      "train loss:0.009574005393761423\n",
      "train loss:0.014798072416982597\n",
      "train loss:0.029077701771492853\n",
      "train loss:0.015354672943891541\n",
      "train loss:0.004499495526679913\n",
      "train loss:0.012547228917600638\n",
      "train loss:0.007958597610306695\n",
      "train loss:0.04087892585190672\n",
      "train loss:0.026738306838807967\n",
      "train loss:0.047672698936277166\n",
      "train loss:0.005976804432715984\n",
      "train loss:0.002999172430274456\n",
      "train loss:0.035152117077257725\n",
      "train loss:0.023154447110208705\n",
      "train loss:0.03244897315463921\n",
      "train loss:0.002318066168042941\n",
      "train loss:0.08744073914854068\n",
      "train loss:0.008339542070715866\n",
      "train loss:0.01494897717214112\n",
      "train loss:0.009463981137177856\n",
      "train loss:0.019971639928814146\n",
      "train loss:0.020378824421851346\n",
      "train loss:0.011883250840452463\n",
      "train loss:0.006877024254442226\n",
      "train loss:0.00849911303595259\n",
      "train loss:0.030980366150485657\n",
      "train loss:0.017365176107772044\n",
      "train loss:0.0017825052587240003\n",
      "train loss:0.018192155498278212\n",
      "train loss:0.01683860573174232\n",
      "train loss:0.09190613592007979\n",
      "train loss:0.07474612001383273\n",
      "train loss:0.015159066229413466\n",
      "train loss:0.026080597132316433\n",
      "train loss:0.00829386758741637\n",
      "train loss:0.009936632168759302\n",
      "train loss:0.024078899768558478\n",
      "train loss:0.012998616794189273\n",
      "train loss:0.01143466442396787\n",
      "train loss:0.027217907979954614\n",
      "train loss:0.0074688637316729775\n",
      "train loss:0.01852312358899766\n",
      "train loss:0.012982859965191343\n",
      "train loss:0.00793094527753672\n",
      "train loss:0.004823273451832938\n",
      "train loss:0.008876853109669553\n",
      "train loss:0.016063457075954034\n",
      "train loss:0.009933172468970262\n",
      "train loss:0.006169431135983736\n",
      "train loss:0.019412749891792664\n",
      "train loss:0.007621284135394875\n",
      "train loss:0.02174839614424857\n",
      "train loss:0.011889020452101778\n",
      "train loss:0.0025886059309714136\n",
      "train loss:0.009217782763720574\n",
      "train loss:0.00943193994650485\n",
      "train loss:0.01952674553847512\n",
      "train loss:0.00788297275574281\n",
      "train loss:0.012105278547109992\n",
      "train loss:0.008851667959461603\n",
      "train loss:0.0535587488950553\n",
      "train loss:0.0367948366300908\n",
      "train loss:0.019740275811803477\n",
      "train loss:0.00411179591926807\n",
      "train loss:0.0074598539698413555\n",
      "train loss:0.015486001006061645\n",
      "train loss:0.02602268352102474\n",
      "train loss:0.010644409683312754\n",
      "train loss:0.031437690949733046\n",
      "train loss:0.010372277642865935\n",
      "train loss:0.021200961061778448\n",
      "train loss:0.009371881497074635\n",
      "train loss:0.01101422793565479\n",
      "train loss:0.028442521528608614\n",
      "train loss:0.03231908034095774\n",
      "train loss:0.006043584129916823\n",
      "train loss:0.009692223112785005\n",
      "train loss:0.003723033692076909\n",
      "train loss:0.0065429345525689145\n",
      "train loss:0.021213049616714868\n",
      "train loss:0.0010283113588664546\n",
      "train loss:0.011384527129137192\n",
      "train loss:0.0041869312832066964\n",
      "train loss:0.015166603601828525\n",
      "train loss:0.02122009413829444\n",
      "train loss:0.0017169477646775061\n",
      "train loss:0.02348149899554153\n",
      "train loss:0.009238496620522774\n",
      "train loss:0.013336411137927193\n",
      "train loss:0.021047450494823008\n",
      "train loss:0.02204301680995386\n",
      "train loss:0.011888576480089608\n",
      "train loss:0.010362886087704947\n",
      "train loss:0.011836825304770628\n",
      "train loss:0.004582368500329745\n",
      "train loss:0.014352657732833318\n",
      "train loss:0.03858155204611882\n",
      "train loss:0.04059538678142003\n",
      "train loss:0.03316724114010355\n",
      "train loss:0.008903330540598857\n",
      "train loss:0.011981267747587183\n",
      "train loss:0.028390437064073053\n",
      "train loss:0.005391038518563714\n",
      "train loss:0.0016506115646523723\n",
      "train loss:0.016794173281617693\n",
      "train loss:0.003179922663250468\n",
      "train loss:0.03019998796267209\n",
      "train loss:0.003995819048756587\n",
      "train loss:0.013032913038076027\n",
      "train loss:0.03482608747489536\n",
      "train loss:0.017813482116343073\n",
      "train loss:0.004299935593697579\n",
      "train loss:0.010922250952918062\n",
      "train loss:0.017453957206447525\n",
      "train loss:0.026117471609481385\n",
      "train loss:0.028362337172469276\n",
      "train loss:0.0054818173135297524\n",
      "train loss:0.015058566241532657\n",
      "train loss:0.019668197229590356\n",
      "train loss:0.021627967713100243\n",
      "train loss:0.01981701190258476\n",
      "train loss:0.008464947687231366\n",
      "train loss:0.0257621865172108\n",
      "train loss:0.001253251779851475\n",
      "train loss:0.0054738348500439245\n",
      "train loss:0.005645033004188373\n",
      "train loss:0.010116905154972442\n",
      "train loss:0.0015447593997174605\n",
      "train loss:0.047774724464274276\n",
      "train loss:0.0029678036399410274\n",
      "train loss:0.007694845668380126\n",
      "train loss:0.09227761141762583\n",
      "train loss:0.017283651741424867\n",
      "train loss:0.005623738721406764\n",
      "train loss:0.02072672593016142\n",
      "train loss:0.008465416531608306\n",
      "train loss:0.005584615991261242\n",
      "train loss:0.08449303640576876\n",
      "train loss:0.012125128740872433\n",
      "train loss:0.01967290128263679\n",
      "train loss:0.010559997444599827\n",
      "train loss:0.009927458342521164\n",
      "train loss:0.0033431816454027497\n",
      "train loss:0.023103389364979906\n",
      "train loss:0.01612186719181528\n",
      "train loss:0.006697706362981786\n",
      "train loss:0.005613092178336515\n",
      "train loss:0.01405923206945603\n",
      "train loss:0.045826430993422186\n",
      "train loss:0.018803604060242313\n",
      "train loss:0.004873996859617733\n",
      "train loss:0.01023335013902139\n",
      "train loss:0.018835823421868337\n",
      "train loss:0.006111240388294243\n",
      "train loss:0.006299940248281707\n",
      "train loss:0.01062501380988254\n",
      "train loss:0.004133003626327568\n",
      "train loss:0.014123039749556483\n",
      "train loss:0.003831761217009433\n",
      "train loss:0.0040452065246584425\n",
      "train loss:0.01383215186052613\n",
      "train loss:0.005168547071096841\n",
      "train loss:0.06427688458286487\n",
      "train loss:0.036137424529519485\n",
      "train loss:0.007847093072136761\n",
      "train loss:0.03986757295146171\n",
      "train loss:0.005232982097645631\n",
      "train loss:0.012194862852224611\n",
      "train loss:0.01595357576614649\n",
      "train loss:0.0028390553956627053\n",
      "train loss:0.02856256685067932\n",
      "train loss:0.0028628744147460635\n",
      "train loss:0.05231142893226873\n",
      "train loss:0.016362538007316418\n",
      "train loss:0.00952768461619264\n",
      "train loss:0.03447110880241229\n",
      "train loss:0.04200309702265689\n",
      "train loss:0.13271315864174174\n",
      "train loss:0.029895664069477138\n",
      "train loss:0.005178235707322971\n",
      "train loss:0.0387319442347054\n",
      "train loss:0.05094106381234946\n",
      "train loss:0.03222715739292939\n",
      "train loss:0.015287864834352594\n",
      "train loss:0.006216437270329868\n",
      "train loss:0.010189336595315512\n",
      "train loss:0.06489092292271657\n",
      "train loss:0.015883228516147825\n",
      "train loss:0.04731086905180874\n",
      "train loss:0.006670655291494874\n",
      "train loss:0.011189457482820303\n",
      "train loss:0.02474015622846604\n",
      "train loss:0.046588894333086835\n",
      "train loss:0.010614620459067702\n",
      "train loss:0.03561989584185276\n",
      "train loss:0.014169476155620721\n",
      "train loss:0.026799281608593472\n",
      "train loss:0.00953487884215506\n",
      "train loss:0.007662802806948734\n",
      "train loss:0.009693655364788906\n",
      "train loss:0.0034873119072892805\n",
      "train loss:0.009217539579616357\n",
      "train loss:0.012935449376726485\n",
      "train loss:0.006047332100613858\n",
      "train loss:0.006083228373006002\n",
      "train loss:0.0071544941990731295\n",
      "train loss:0.018478158231695702\n",
      "train loss:0.00920026446788409\n",
      "train loss:0.009787840262701637\n",
      "train loss:0.011836758918452752\n",
      "train loss:0.038476800922877016\n",
      "train loss:0.0007281190732539805\n",
      "train loss:0.010980912959630534\n",
      "train loss:0.0029693672078595483\n",
      "train loss:0.019318176965677294\n",
      "train loss:0.020069710906189114\n",
      "train loss:0.007586959340405405\n",
      "train loss:0.06162803984473556\n",
      "train loss:0.007125798450997521\n",
      "train loss:0.01481133143320957\n",
      "train loss:0.04693860234958239\n",
      "train loss:0.006648621930502576\n",
      "train loss:0.001884504634869799\n",
      "train loss:0.002208575023168361\n",
      "train loss:0.016159287355980906\n",
      "train loss:0.03948876141754331\n",
      "train loss:0.1742259439781298\n",
      "train loss:0.08823845429348706\n",
      "train loss:0.011058239993779292\n",
      "train loss:0.028609466950766126\n",
      "train loss:0.024810977411427633\n",
      "train loss:0.014746012857547482\n",
      "train loss:0.018655549027583162\n",
      "train loss:0.006890124231782741\n",
      "train loss:0.028410738662995122\n",
      "train loss:0.0018117695224472196\n",
      "train loss:0.05314224033097231\n",
      "train loss:0.01970712449907308\n",
      "train loss:0.01892608213447639\n",
      "train loss:0.007585174397854147\n",
      "train loss:0.007491369777102871\n",
      "train loss:0.009439747004994533\n",
      "train loss:0.02196954183931026\n",
      "train loss:0.023431151436782486\n",
      "train loss:0.004148550101369868\n",
      "train loss:0.015834586864309896\n",
      "train loss:0.007049224082444486\n",
      "train loss:0.05741883642286972\n",
      "train loss:0.007589907125438575\n",
      "train loss:0.005714777292415168\n",
      "train loss:0.0032143838581628636\n",
      "train loss:0.021558915308213508\n",
      "train loss:0.0375896157803967\n",
      "train loss:0.001185189573842081\n",
      "train loss:0.013212464103413866\n",
      "train loss:0.01063219146446661\n",
      "train loss:0.008547100374303545\n",
      "train loss:0.028082178226624014\n",
      "train loss:0.05195524910400133\n",
      "train loss:0.0225108383468282\n",
      "train loss:0.02290392112367179\n",
      "train loss:0.02275908491141787\n",
      "train loss:0.005858581824950145\n",
      "train loss:0.009411102695402518\n",
      "train loss:0.008843231687226106\n",
      "train loss:0.06345767035589704\n",
      "train loss:0.019644898556387697\n",
      "train loss:0.015426135094841429\n",
      "train loss:0.012182025610469351\n",
      "train loss:0.015829971074063462\n",
      "train loss:0.028819106753304614\n",
      "train loss:0.003917893117433348\n",
      "train loss:0.004287184992796\n",
      "train loss:0.01694856629002724\n",
      "train loss:0.034865094697056564\n",
      "train loss:0.04595840024288272\n",
      "train loss:0.015735218607515186\n",
      "train loss:0.007506441692936383\n",
      "train loss:0.003457601137308951\n",
      "train loss:0.02562932092307655\n",
      "train loss:0.04098004684419318\n",
      "train loss:0.004917058086206165\n",
      "train loss:0.01393255014451797\n",
      "train loss:0.019169729048796724\n",
      "train loss:0.04706137956116487\n",
      "train loss:0.007673851689786298\n",
      "train loss:0.03539239768628856\n",
      "train loss:0.023205222874597527\n",
      "train loss:0.017724756888588936\n",
      "train loss:0.03462570631386471\n",
      "train loss:0.011773978777876382\n",
      "train loss:0.011567763840631295\n",
      "train loss:0.01652382667731511\n",
      "train loss:0.01697316195041535\n",
      "train loss:0.004350929972167672\n",
      "train loss:0.008332562435629916\n",
      "train loss:0.018666523812129188\n",
      "train loss:0.019697727934276682\n",
      "train loss:0.014851381249698101\n",
      "train loss:0.0032764283896327077\n",
      "train loss:0.03192924063895483\n",
      "train loss:0.02838028719712921\n",
      "train loss:0.060489048863688326\n",
      "train loss:0.0048360333986799614\n",
      "train loss:0.02126357798469435\n",
      "train loss:0.017758410205358264\n",
      "train loss:0.04361848535679171\n",
      "train loss:0.013367405394438734\n",
      "train loss:0.01787436710482326\n",
      "train loss:0.020308351231590555\n",
      "train loss:0.0038679514023334948\n",
      "train loss:0.020266937315701505\n",
      "train loss:0.01688133546778797\n",
      "train loss:0.006372166142551458\n",
      "train loss:0.004820942310846849\n",
      "train loss:0.015429927808424863\n",
      "train loss:0.02244146576980169\n",
      "train loss:0.006096505582300298\n",
      "train loss:0.054274865038207006\n",
      "train loss:0.008850100689018401\n",
      "train loss:0.006070671044810837\n",
      "train loss:0.008854232647545434\n",
      "train loss:0.01213107913598044\n",
      "train loss:0.004260144717714028\n",
      "train loss:0.007203734836947176\n",
      "train loss:0.004410925554487269\n",
      "train loss:0.003908793563951932\n",
      "train loss:0.0033276974700436747\n",
      "train loss:0.01685075936226347\n",
      "train loss:0.03211720480329284\n",
      "train loss:0.01931788545503922\n",
      "train loss:0.003542966123555558\n",
      "train loss:0.00960503620007093\n",
      "train loss:0.04050900281961756\n",
      "train loss:0.02275197264155236\n",
      "train loss:0.005904998465398277\n",
      "train loss:0.003563508639657712\n",
      "train loss:0.0025999324063348487\n",
      "train loss:0.015286605573175795\n",
      "train loss:0.0034192085495686787\n",
      "train loss:0.005130452284874757\n",
      "train loss:0.014382358475014536\n",
      "train loss:0.004536901587681119\n",
      "train loss:0.014882486316185193\n",
      "train loss:0.009512427680540858\n",
      "train loss:0.002186073215796501\n",
      "train loss:0.016550624985146797\n",
      "train loss:0.007471679393854318\n",
      "train loss:0.01948690858747604\n",
      "train loss:0.007230218078670738\n",
      "train loss:0.0026012793366751038\n",
      "train loss:0.00836310524830908\n",
      "train loss:0.08003092522807025\n",
      "train loss:0.0032500452541094084\n",
      "train loss:0.003409655171642798\n",
      "train loss:0.010478810453206486\n",
      "train loss:0.032948429598913004\n",
      "train loss:0.005144563093383888\n",
      "train loss:0.015849978895997575\n",
      "train loss:0.041868304000623614\n",
      "train loss:0.0056925332689552735\n",
      "train loss:0.0029948486954263836\n",
      "train loss:0.03493434650668517\n",
      "train loss:0.07485191809359781\n",
      "train loss:0.004764489240305115\n",
      "train loss:0.016642441409917046\n",
      "train loss:0.031026769425654036\n",
      "train loss:0.014140892334407908\n",
      "train loss:0.015394706638085778\n",
      "train loss:0.014737940072344332\n",
      "train loss:0.023499064735454565\n",
      "train loss:0.03309497624492689\n",
      "train loss:0.004949537709908202\n",
      "train loss:0.04194766615804073\n",
      "train loss:0.004433868054808245\n",
      "train loss:0.004318320179485153\n",
      "train loss:0.005705961901893713\n",
      "train loss:0.025729656662383058\n",
      "train loss:0.0426439937250396\n",
      "train loss:0.006063089352936611\n",
      "train loss:0.041799995755258716\n",
      "train loss:0.018942228823298487\n",
      "train loss:0.01709323807624948\n",
      "train loss:0.006698855174025225\n",
      "train loss:0.007413559087880919\n",
      "train loss:0.03657305874886837\n",
      "train loss:0.004034124179538356\n",
      "train loss:0.07528959105644395\n",
      "train loss:0.029775838873194925\n",
      "train loss:0.04094933976073343\n",
      "train loss:0.044959495275388235\n",
      "train loss:0.011432474314029172\n",
      "train loss:0.014847325018960513\n",
      "train loss:0.008552914215569533\n",
      "train loss:0.020207569647427204\n",
      "train loss:0.023481948263102036\n",
      "train loss:0.00953628320717251\n",
      "train loss:0.022817733700902548\n",
      "train loss:0.0046316560867566145\n",
      "train loss:0.046561101478171836\n",
      "train loss:0.008562171744109358\n",
      "train loss:0.00888109954103116\n",
      "train loss:0.028992664313202993\n",
      "train loss:0.00831425032081329\n",
      "train loss:0.005450704393724759\n",
      "train loss:0.016124864545428604\n",
      "train loss:0.006111546529588487\n",
      "train loss:0.026297755183857213\n",
      "train loss:0.017382878151296955\n",
      "train loss:0.027884306781768727\n",
      "train loss:0.01619971833540713\n",
      "train loss:0.004624810110128067\n",
      "train loss:0.012891293172600026\n",
      "train loss:0.0019191735311654848\n",
      "train loss:0.02170944865922503\n",
      "train loss:0.024279520958625525\n",
      "train loss:0.06856004166231208\n",
      "train loss:0.04173766055119614\n",
      "train loss:0.00940426673936035\n",
      "train loss:0.00341864594755573\n",
      "train loss:0.03388084812253819\n",
      "train loss:0.06873933832510483\n",
      "train loss:0.010012738169947832\n",
      "train loss:0.03592755364508409\n",
      "train loss:0.014921291906872301\n",
      "train loss:0.005651981113258592\n",
      "train loss:0.01852326564166745\n",
      "train loss:0.009742795630041235\n",
      "train loss:0.05151934729266116\n",
      "train loss:0.007633045198211228\n",
      "train loss:0.009549124733875907\n",
      "train loss:0.02133370077597985\n",
      "train loss:0.010357143161187365\n",
      "train loss:0.0010194884675496486\n",
      "train loss:0.04427340576337142\n",
      "train loss:0.0020850470053848345\n",
      "train loss:0.0015762498101057193\n",
      "train loss:0.03169770099934263\n",
      "train loss:0.002265018561119869\n",
      "train loss:0.02615625849132082\n",
      "train loss:0.0035228340128821744\n",
      "train loss:0.006415788374808824\n",
      "train loss:0.01853400383838939\n",
      "train loss:0.022320113787352512\n",
      "train loss:0.041568378030537124\n",
      "train loss:0.014533774623209516\n",
      "train loss:0.007229658645279908\n",
      "train loss:0.008754158423783768\n",
      "train loss:0.04170592340717915\n",
      "train loss:0.0088384088315618\n",
      "train loss:0.004211429890915416\n",
      "train loss:0.015912483737587784\n",
      "train loss:0.0032836325994978228\n",
      "train loss:0.0034709005347895753\n",
      "train loss:0.010051487090887127\n",
      "train loss:0.008089679109753743\n",
      "train loss:0.017494675647865394\n",
      "train loss:0.023871606067794624\n",
      "train loss:0.007807890023464186\n",
      "train loss:0.01225672590268954\n",
      "train loss:0.004334823689848712\n",
      "train loss:0.04783874070384045\n",
      "train loss:0.018056432287524203\n",
      "train loss:0.012501861086793907\n",
      "train loss:0.02622929719164189\n",
      "train loss:0.0066541961388433424\n",
      "train loss:0.003753210105621255\n",
      "train loss:0.00402684668572287\n",
      "train loss:0.0032743782353471013\n",
      "train loss:0.026066427363470975\n",
      "train loss:0.022320455107582956\n",
      "train loss:0.024808085919243963\n",
      "train loss:0.004886238308895366\n",
      "train loss:0.0027873926133314196\n",
      "train loss:0.008718508089794347\n",
      "train loss:0.008610896770941212\n",
      "train loss:0.006363438393043445\n",
      "train loss:0.006012786916369187\n",
      "train loss:0.007806775978969575\n",
      "train loss:0.012812976921269575\n",
      "train loss:0.009237684833246244\n",
      "train loss:0.010004640055541638\n",
      "train loss:0.01148111266214747\n",
      "train loss:0.007019968366713478\n",
      "train loss:0.059742157146242744\n",
      "train loss:0.024695377633308264\n",
      "train loss:0.008765403589919443\n",
      "train loss:0.003808480347593321\n",
      "train loss:0.0056795176054645225\n",
      "train loss:0.01804796438566301\n",
      "train loss:0.002993466958289345\n",
      "train loss:0.010071212462036776\n",
      "train loss:0.014107423216136741\n",
      "train loss:0.014625375547811842\n",
      "train loss:0.048601057500537656\n",
      "train loss:0.01568451463545621\n",
      "train loss:0.0053015183085984895\n",
      "train loss:0.014799588939035355\n",
      "train loss:0.00991250701161839\n",
      "train loss:0.002974518976969997\n",
      "train loss:0.015542333056609063\n",
      "train loss:0.00305125051290776\n",
      "train loss:0.003291770447024337\n",
      "train loss:0.010760447423063223\n",
      "train loss:0.010962638280293303\n",
      "train loss:0.0072645496763594376\n",
      "train loss:0.009307385990736602\n",
      "train loss:0.026335642785526418\n",
      "train loss:0.08679658950959247\n",
      "train loss:0.030318191597085292\n",
      "train loss:0.013684400714753414\n",
      "train loss:0.015641810282587827\n",
      "train loss:0.0034625543831736704\n",
      "train loss:0.04317784312125186\n",
      "train loss:0.009742962815516072\n",
      "train loss:0.005292197046274811\n",
      "train loss:0.010258587152001803\n",
      "train loss:0.0135837214396879\n",
      "train loss:0.03624705322613836\n",
      "train loss:0.0021203023651886147\n",
      "train loss:0.002142357500563104\n",
      "train loss:0.002707829196245289\n",
      "train loss:0.03855734003731329\n",
      "train loss:0.018616146893571602\n",
      "train loss:0.024315396697465397\n",
      "train loss:0.003964382326040772\n",
      "train loss:0.0480459849269397\n",
      "train loss:0.0087084825027168\n",
      "train loss:0.0040368202728506735\n",
      "train loss:0.0186553239043703\n",
      "train loss:0.002277827360664257\n",
      "train loss:0.009286823897420167\n",
      "train loss:0.03665969475234912\n",
      "train loss:0.0348902443130777\n",
      "train loss:0.0028549792939821694\n",
      "train loss:0.0008162985157465556\n",
      "train loss:0.015079907847606757\n",
      "train loss:0.0055516260012781595\n",
      "train loss:0.006820789628991566\n",
      "train loss:0.020517104539307454\n",
      "train loss:0.0355257496802767\n",
      "train loss:0.0030853093269182733\n",
      "train loss:0.013563245783663372\n",
      "train loss:0.023071022243493337\n",
      "train loss:0.009509440774490905\n",
      "train loss:0.009693691602396342\n",
      "train loss:0.011205893723279214\n",
      "train loss:0.007491709833035011\n",
      "=== epoch:8, train acc:0.99, test acc:0.987 ===\n",
      "train loss:0.010104410545530821\n",
      "train loss:0.009956410322103822\n",
      "train loss:0.1530183254629246\n",
      "train loss:0.002968511695298359\n",
      "train loss:0.18045109091740058\n",
      "train loss:0.026781587339102488\n",
      "train loss:0.006917666770921776\n",
      "train loss:0.011092121173562515\n",
      "train loss:0.02060457274342757\n",
      "train loss:0.024566259006801462\n",
      "train loss:0.013571676205604281\n",
      "train loss:0.013218824350973551\n",
      "train loss:0.041697551199543305\n",
      "train loss:0.0444745256631778\n",
      "train loss:0.009351518684526801\n",
      "train loss:0.050552676168860805\n",
      "train loss:0.020721717239358582\n",
      "train loss:0.009538747884640352\n",
      "train loss:0.21244070472005253\n",
      "train loss:0.009619610065196156\n",
      "train loss:0.03549665225573086\n",
      "train loss:0.018468295937481397\n",
      "train loss:0.007519973411733423\n",
      "train loss:0.06315640463081705\n",
      "train loss:0.05061490744839717\n",
      "train loss:0.0028034103615066064\n",
      "train loss:0.010981485184544617\n",
      "train loss:0.021793015526910778\n",
      "train loss:0.015670167677398653\n",
      "train loss:0.013688915601377725\n",
      "train loss:0.017423593136329425\n",
      "train loss:0.02282745018842614\n",
      "train loss:0.010834118489797333\n",
      "train loss:0.006593467539630759\n",
      "train loss:0.03506351213617899\n",
      "train loss:0.027071423336885128\n",
      "train loss:0.01389782837186202\n",
      "train loss:0.005521594454289363\n",
      "train loss:0.02253769345284322\n",
      "train loss:0.019601098094785545\n",
      "train loss:0.019944005302038385\n",
      "train loss:0.006847605111414843\n",
      "train loss:0.003425423657555146\n",
      "train loss:0.011633504858946278\n",
      "train loss:0.03698728294461176\n",
      "train loss:0.02115296593438896\n",
      "train loss:0.009817144613781763\n",
      "train loss:0.010544185152175756\n",
      "train loss:0.005518463848931304\n",
      "train loss:0.021102144428509786\n",
      "train loss:0.07630732837895107\n",
      "train loss:0.033756837450452444\n",
      "train loss:0.024748277852431798\n",
      "train loss:0.02212369352759474\n",
      "train loss:0.023464203926361175\n",
      "train loss:0.0024845555964848882\n",
      "train loss:0.02037994466943111\n",
      "train loss:0.0034379812768541755\n",
      "train loss:0.014004486229231035\n",
      "train loss:0.027792751889151493\n",
      "train loss:0.005626460761026325\n",
      "train loss:0.01411018184933017\n",
      "train loss:0.00785061427713114\n",
      "train loss:0.007443250540635873\n",
      "train loss:0.0034236631802070107\n",
      "train loss:0.0028534668403829427\n",
      "train loss:0.007385885519071052\n",
      "train loss:0.0032593771374554562\n",
      "train loss:0.003569952767903991\n",
      "train loss:0.013406258183643736\n",
      "train loss:0.01026370384950774\n",
      "train loss:0.016633896822062483\n",
      "train loss:0.012335756937861788\n",
      "train loss:0.006266520432242592\n",
      "train loss:0.015879228379285616\n",
      "train loss:0.03781526208519262\n",
      "train loss:0.002590464922479344\n",
      "train loss:0.020045537372496782\n",
      "train loss:0.010798220535322905\n",
      "train loss:0.014038913975873197\n",
      "train loss:0.024362174476273132\n",
      "train loss:0.0051476460289469105\n",
      "train loss:0.04890840899975189\n",
      "train loss:0.0028015058746958606\n",
      "train loss:0.03354330149322876\n",
      "train loss:0.00849771030822944\n",
      "train loss:0.010002012937698194\n",
      "train loss:0.010035339102301432\n",
      "train loss:0.003492860385221422\n",
      "train loss:0.026034195744034264\n",
      "train loss:0.00520761657874776\n",
      "train loss:0.001680925805229927\n",
      "train loss:0.021219209795607943\n",
      "train loss:0.009524419767458976\n",
      "train loss:0.008268232480009945\n",
      "train loss:0.004742716869534467\n",
      "train loss:0.007529005243863505\n",
      "train loss:0.023305419205470083\n",
      "train loss:0.02746154719125277\n",
      "train loss:0.005582746427748813\n",
      "train loss:0.00419325063681115\n",
      "train loss:0.007077621276596435\n",
      "train loss:0.03726797108835072\n",
      "train loss:0.01167379725236901\n",
      "train loss:0.009057631856283228\n",
      "train loss:0.004591934634510905\n",
      "train loss:0.00817275618547786\n",
      "train loss:0.001688349605946307\n",
      "train loss:0.016510260179196233\n",
      "train loss:0.008906689667093943\n",
      "train loss:0.034902912458439794\n",
      "train loss:0.03676130398471582\n",
      "train loss:0.01338117846610828\n",
      "train loss:0.02416862300275472\n",
      "train loss:0.010534347778650461\n",
      "train loss:0.023824740964787993\n",
      "train loss:0.03282084895403199\n",
      "train loss:0.0011778626208853312\n",
      "train loss:0.010711106557425434\n",
      "train loss:0.0174290453182528\n",
      "train loss:0.04345266995419424\n",
      "train loss:0.004633574393035333\n",
      "train loss:0.010254881537577754\n",
      "train loss:0.015903231826899372\n",
      "train loss:0.02134553221998169\n",
      "train loss:0.012350271205641652\n",
      "train loss:0.01091631804000414\n",
      "train loss:0.002885983622633011\n",
      "train loss:0.012978351249112396\n",
      "train loss:0.003841823330083503\n",
      "train loss:0.004248990823752091\n",
      "train loss:0.007531029307410991\n",
      "train loss:0.002712244430155891\n",
      "train loss:0.003486633597626402\n",
      "train loss:0.002042582812581438\n",
      "train loss:0.007724335472348146\n",
      "train loss:0.001914805888241033\n",
      "train loss:0.0034112622096162002\n",
      "train loss:0.008205242219344265\n",
      "train loss:0.005630969140085694\n",
      "train loss:0.008533880136542531\n",
      "train loss:0.0103481170589547\n",
      "train loss:0.004449860465388481\n",
      "train loss:0.020309742552432352\n",
      "train loss:0.013888129307840289\n",
      "train loss:0.00703166842993431\n",
      "train loss:0.011653256514091792\n",
      "train loss:0.016650672600869623\n",
      "train loss:0.06747826022015457\n",
      "train loss:0.03655110153511142\n",
      "train loss:0.005724919445771099\n",
      "train loss:0.01988039268651344\n",
      "train loss:0.024658286820495413\n",
      "train loss:0.024516808583500502\n",
      "train loss:0.029610337006085286\n",
      "train loss:0.017397956869455788\n",
      "train loss:0.013915823982359266\n",
      "train loss:0.008364400620737837\n",
      "train loss:0.011527730005102679\n",
      "train loss:0.002624526654743623\n",
      "train loss:0.021629758132269517\n",
      "train loss:0.01321876973873959\n",
      "train loss:0.009536069061217501\n",
      "train loss:0.007576308358509059\n",
      "train loss:0.023247492071819083\n",
      "train loss:0.002644950229093513\n",
      "train loss:0.005960535169909147\n",
      "train loss:0.003910427315104269\n",
      "train loss:0.006878962489006348\n",
      "train loss:0.009985291392822707\n",
      "train loss:0.008916948483514044\n",
      "train loss:0.011415487484671511\n",
      "train loss:0.016957727363773212\n",
      "train loss:0.004732690221029892\n",
      "train loss:0.013471387572006794\n",
      "train loss:0.0012443632013076325\n",
      "train loss:0.021282892014944776\n",
      "train loss:0.01789942398713186\n",
      "train loss:0.01531609880185655\n",
      "train loss:0.009657246179286738\n",
      "train loss:0.0016292302291278777\n",
      "train loss:0.004855117214964007\n",
      "train loss:0.11072420659417222\n",
      "train loss:0.009134889822927055\n",
      "train loss:0.0023947656197900835\n",
      "train loss:0.05479609936516206\n",
      "train loss:0.009821434238407398\n",
      "train loss:0.05431420946039674\n",
      "train loss:0.009159367932776353\n",
      "train loss:0.00473305793759018\n",
      "train loss:0.0065761574388300895\n",
      "train loss:0.01726361973721143\n",
      "train loss:0.004369986622498005\n",
      "train loss:0.007888283069705173\n",
      "train loss:0.004744824276313983\n",
      "train loss:0.0247208078317428\n",
      "train loss:0.030767016989740284\n",
      "train loss:0.014337182532766527\n",
      "train loss:0.018615916561074444\n",
      "train loss:0.010284021798674579\n",
      "train loss:0.055394064839859605\n",
      "train loss:0.008904509060835208\n",
      "train loss:0.0074166183811280725\n",
      "train loss:0.029508790133224702\n",
      "train loss:0.039035905109119404\n",
      "train loss:0.010211629806953686\n",
      "train loss:0.007606039970920659\n",
      "train loss:0.019226922389929655\n",
      "train loss:0.018264970261306145\n",
      "train loss:0.0015937493619304457\n",
      "train loss:0.01857103558723626\n",
      "train loss:0.01530438493996305\n",
      "train loss:0.02250419686203304\n",
      "train loss:0.004444554507754286\n",
      "train loss:0.029784534246460367\n",
      "train loss:0.0151649508361125\n",
      "train loss:0.008248760646460853\n",
      "train loss:0.027958347858033257\n",
      "train loss:0.015678878104919757\n",
      "train loss:0.0036068258580334326\n",
      "train loss:0.009269507709044758\n",
      "train loss:0.009703388123714247\n",
      "train loss:0.014500372935956917\n",
      "train loss:0.013910771935705547\n",
      "train loss:0.00396603950628722\n",
      "train loss:0.0023223550695252986\n",
      "train loss:0.028859610139963153\n",
      "train loss:0.006365630225497751\n",
      "train loss:0.01384446086662777\n",
      "train loss:0.017590241680869555\n",
      "train loss:0.005260563116671142\n",
      "train loss:0.005962817034760929\n",
      "train loss:0.017273554807479427\n",
      "train loss:0.00034155551256159517\n",
      "train loss:0.014530817069716737\n",
      "train loss:0.007771419259234855\n",
      "train loss:0.017688444586683626\n",
      "train loss:0.005388641775875724\n",
      "train loss:0.009555015513221568\n",
      "train loss:0.002747017930051178\n",
      "train loss:0.010337014920573701\n",
      "train loss:0.0039089430269350686\n",
      "train loss:0.005005074384267901\n",
      "train loss:0.0007576028096351535\n",
      "train loss:0.016426417316102775\n",
      "train loss:0.004187011864710616\n",
      "train loss:0.007344876117127629\n",
      "train loss:0.018408584305041838\n",
      "train loss:0.008101266771026483\n",
      "train loss:0.023188729441622792\n",
      "train loss:0.009218657710306166\n",
      "train loss:0.027434818252888994\n",
      "train loss:0.005473838393334651\n",
      "train loss:0.00491177179621676\n",
      "train loss:0.009141115586488948\n",
      "train loss:0.009402857752423098\n",
      "train loss:0.04315335847371725\n",
      "train loss:0.00388114280183831\n",
      "train loss:0.013894166065650524\n",
      "train loss:0.002634273332521069\n",
      "train loss:0.007228293421770291\n",
      "train loss:0.006147065671850173\n",
      "train loss:0.008297684038170472\n",
      "train loss:0.006452859870222649\n",
      "train loss:0.002079345602579708\n",
      "train loss:0.004330276999943724\n",
      "train loss:0.015117287463441632\n",
      "train loss:0.08443327839789547\n",
      "train loss:0.008548653424276796\n",
      "train loss:0.023686360583655204\n",
      "train loss:0.0062338780707773276\n",
      "train loss:0.006468598064576132\n",
      "train loss:0.005798306126538925\n",
      "train loss:7.794727046886178e-05\n",
      "train loss:0.016071491091945438\n",
      "train loss:0.0023203553514823307\n",
      "train loss:0.027029120362924184\n",
      "train loss:0.010027471940868518\n",
      "train loss:0.004454299161334927\n",
      "train loss:0.0048788691814052754\n",
      "train loss:0.0004895718443644455\n",
      "train loss:0.01870369565447427\n",
      "train loss:0.05154785168405389\n",
      "train loss:0.003737919261213977\n",
      "train loss:0.00974930463405233\n",
      "train loss:0.0025018852296407827\n",
      "train loss:0.003493118954663401\n",
      "train loss:0.018005586649903937\n",
      "train loss:0.0018036601141055274\n",
      "train loss:0.0031824994110188232\n",
      "train loss:0.008378744676996478\n",
      "train loss:0.0037184830962092236\n",
      "train loss:0.009772957043752967\n",
      "train loss:0.008848283867060063\n",
      "train loss:0.034543172487706025\n",
      "train loss:0.0025412903045175057\n",
      "train loss:0.0025962792656932577\n",
      "train loss:0.0009816043529770038\n",
      "train loss:0.032345345585223076\n",
      "train loss:0.005939945843536866\n",
      "train loss:0.026563589414458275\n",
      "train loss:0.012197993043131062\n",
      "train loss:0.009738091319348793\n",
      "train loss:0.006077131001264748\n",
      "train loss:0.0015317657807917583\n",
      "train loss:0.016506384613928546\n",
      "train loss:0.01404884846195875\n",
      "train loss:0.008096789711246957\n",
      "train loss:0.00664974731580966\n",
      "train loss:0.010604812866202485\n",
      "train loss:0.047034921619423915\n",
      "train loss:0.016284912922310456\n",
      "train loss:0.0070192338198492845\n",
      "train loss:0.007478662568100102\n",
      "train loss:0.0054210884206035695\n",
      "train loss:0.010358907629227976\n",
      "train loss:0.0038258331884797104\n",
      "train loss:0.003389119408303532\n",
      "train loss:0.008812933266181816\n",
      "train loss:0.008284307238956122\n",
      "train loss:0.02186339369800313\n",
      "train loss:0.0049942799964255635\n",
      "train loss:0.0038022999747533187\n",
      "train loss:0.006050956039688502\n",
      "train loss:0.011370016322457205\n",
      "train loss:0.0075732613443592715\n",
      "train loss:0.01628730938199478\n",
      "train loss:0.0029451736008289063\n",
      "train loss:0.005358842281988404\n",
      "train loss:0.008697052968742545\n",
      "train loss:0.022956631403620666\n",
      "train loss:0.0030385257526862846\n",
      "train loss:0.0039590626243825615\n",
      "train loss:0.06090055382889203\n",
      "train loss:0.0055005640482475035\n",
      "train loss:0.029017973780870964\n",
      "train loss:0.008893257149641747\n",
      "train loss:0.002456296540627855\n",
      "train loss:0.009969481978769217\n",
      "train loss:0.0012956011357495966\n",
      "train loss:0.006242088741308228\n",
      "train loss:0.006280915020274783\n",
      "train loss:0.006844472616674341\n",
      "train loss:0.01532491495666971\n",
      "train loss:0.000596630599640422\n",
      "train loss:0.009746161381280457\n",
      "train loss:0.009126582746822046\n",
      "train loss:0.014843368329407428\n",
      "train loss:0.023235210141033722\n",
      "train loss:0.010795072947881498\n",
      "train loss:0.024087957999967637\n",
      "train loss:0.004814454707452095\n",
      "train loss:0.005978252014251851\n",
      "train loss:0.017849158419764834\n",
      "train loss:0.030115459099743315\n",
      "train loss:0.016905281251773503\n",
      "train loss:0.010802785442752974\n",
      "train loss:0.005903023957136544\n",
      "train loss:0.033333696243007915\n",
      "train loss:0.03434269319906782\n",
      "train loss:0.029368734414618736\n",
      "train loss:0.007804504194436023\n",
      "train loss:0.007990969204590158\n",
      "train loss:0.005298560019377562\n",
      "train loss:0.04423743900938965\n",
      "train loss:0.00743639051013544\n",
      "train loss:0.0632894261450903\n",
      "train loss:0.012872582337366276\n",
      "train loss:0.0025228393474585093\n",
      "train loss:0.004879477690571899\n",
      "train loss:0.013806448875146394\n",
      "train loss:0.016183534204328318\n",
      "train loss:0.019644896117457\n",
      "train loss:0.004109616835642929\n",
      "train loss:0.01930028042078325\n",
      "train loss:0.006146716198108305\n",
      "train loss:0.03311556593932272\n",
      "train loss:0.008837860739908037\n",
      "train loss:0.01484952763366274\n",
      "train loss:0.009918671317809156\n",
      "train loss:0.006780822148416411\n",
      "train loss:0.04149396559090033\n",
      "train loss:0.010481598546967038\n",
      "train loss:0.00797169918903254\n",
      "train loss:0.07598118019015296\n",
      "train loss:0.0071000749337345416\n",
      "train loss:0.011260650091711484\n",
      "train loss:0.006065666136596327\n",
      "train loss:0.004751885015109646\n",
      "train loss:0.001594075413718868\n",
      "train loss:0.009896353845871701\n",
      "train loss:0.04125619908317155\n",
      "train loss:0.008229801332666344\n",
      "train loss:0.00916304811100084\n",
      "train loss:0.018945651405741983\n",
      "train loss:0.007158371910537542\n",
      "train loss:0.02588654338678315\n",
      "train loss:0.05348483895150876\n",
      "train loss:0.006186751639809367\n",
      "train loss:0.0036580179408784897\n",
      "train loss:0.0025520405035258614\n",
      "train loss:0.01672520326046834\n",
      "train loss:0.03498298283347172\n",
      "train loss:0.007495289663767019\n",
      "train loss:0.00995673045300912\n",
      "train loss:0.011685420185467978\n",
      "train loss:0.004807551695822995\n",
      "train loss:0.004985037189037458\n",
      "train loss:0.010382948027757292\n",
      "train loss:0.0162490901542276\n",
      "train loss:0.008940642369370536\n",
      "train loss:0.020463605053335797\n",
      "train loss:0.014901430820683747\n",
      "train loss:0.01038530286016351\n",
      "train loss:0.01633893725814289\n",
      "train loss:0.003592937433036159\n",
      "train loss:0.03823892581553787\n",
      "train loss:0.0029865070727941884\n",
      "train loss:0.013422007069729347\n",
      "train loss:0.0046014718047267915\n",
      "train loss:0.00521404686802226\n",
      "train loss:0.003072712082451797\n",
      "train loss:0.051500457631573224\n",
      "train loss:0.03786252522314149\n",
      "train loss:0.00922718761743296\n",
      "train loss:0.009853814588811271\n",
      "train loss:0.0008136363056161541\n",
      "train loss:0.00293550003396924\n",
      "train loss:0.0028209711992116544\n",
      "train loss:0.004292425974278189\n",
      "train loss:0.011041537911616\n",
      "train loss:0.005526771785981471\n",
      "train loss:0.029729067928533633\n",
      "train loss:0.002320052507228798\n",
      "train loss:0.0029794246903825155\n",
      "train loss:0.003917285226868246\n",
      "train loss:0.011508093009883076\n",
      "train loss:0.012339800250762986\n",
      "train loss:0.009622359269914187\n",
      "train loss:0.0065371421593976445\n",
      "train loss:0.004518860866782725\n",
      "train loss:0.010034359593141069\n",
      "train loss:0.012218130132069025\n",
      "train loss:0.01199515012178467\n",
      "train loss:0.00324789195029304\n",
      "train loss:0.01612391699834701\n",
      "train loss:0.008313767481324343\n",
      "train loss:0.014979342907552172\n",
      "train loss:0.0029707726232100808\n",
      "train loss:0.006516636009481028\n",
      "train loss:0.044155893039531165\n",
      "train loss:0.049222410262044346\n",
      "train loss:0.029337181959787344\n",
      "train loss:0.014115768948027376\n",
      "train loss:0.0013895315614300186\n",
      "train loss:0.03496204443341512\n",
      "train loss:0.013519430287532434\n",
      "train loss:0.006439625835498685\n",
      "train loss:0.02178678385490409\n",
      "train loss:0.005971730410641807\n",
      "train loss:0.01785540990825038\n",
      "train loss:0.0048403423749382194\n",
      "train loss:0.03132777619811347\n",
      "train loss:0.005180829698444964\n",
      "train loss:0.007067951312073079\n",
      "train loss:0.004647011569747787\n",
      "train loss:0.005957142466736211\n",
      "train loss:0.00633897642620807\n",
      "train loss:0.005760990999506034\n",
      "train loss:0.0045014836218258795\n",
      "train loss:0.013621274429621804\n",
      "train loss:0.0009652611118098578\n",
      "train loss:0.007286367654839355\n",
      "train loss:0.0019332991050285018\n",
      "train loss:0.0040968732592369\n",
      "train loss:0.005362451553368254\n",
      "train loss:0.001211965334319315\n",
      "train loss:0.003910499112572476\n",
      "train loss:0.006017375633459887\n",
      "train loss:0.00145567185148423\n",
      "train loss:0.005750087812948501\n",
      "train loss:0.008380715060243224\n",
      "train loss:0.013763765734102018\n",
      "train loss:0.004702683741899631\n",
      "train loss:0.0018329033796159425\n",
      "train loss:0.0028661971931132385\n",
      "train loss:0.005017567882695841\n",
      "train loss:0.0032681132112152757\n",
      "train loss:0.026795881446593565\n",
      "train loss:0.021935225934418136\n",
      "train loss:0.0019568479656301495\n",
      "train loss:0.0010843176708693805\n",
      "train loss:0.01460323647022793\n",
      "train loss:0.003944298063605712\n",
      "train loss:0.041425338062165556\n",
      "train loss:0.003863023428271261\n",
      "train loss:0.015284864872176675\n",
      "train loss:0.002969205268675037\n",
      "train loss:0.009466927181658554\n",
      "train loss:0.02376859541012575\n",
      "train loss:0.01120881248020822\n",
      "train loss:0.0012130805021183016\n",
      "train loss:0.005860237287975434\n",
      "train loss:0.0034174378277084818\n",
      "train loss:0.0032285186144552637\n",
      "train loss:0.002825168845182005\n",
      "train loss:0.005964044141933565\n",
      "train loss:0.01017815902450077\n",
      "train loss:0.006311620680567599\n",
      "train loss:0.010123163603094681\n",
      "train loss:0.00681796900185683\n",
      "train loss:0.006505439792347202\n",
      "train loss:0.011387033523144529\n",
      "train loss:0.0033877087686728907\n",
      "train loss:0.023212083752339793\n",
      "train loss:0.006328088819478808\n",
      "train loss:0.010583973702772339\n",
      "train loss:0.010776735121740079\n",
      "train loss:0.006340068234663716\n",
      "train loss:0.0033425997571648186\n",
      "train loss:0.0028899902682026236\n",
      "train loss:0.006580231029427001\n",
      "train loss:0.0006784171991147631\n",
      "train loss:0.021284072281366927\n",
      "train loss:0.021345044988469914\n",
      "train loss:0.002915461029217946\n",
      "train loss:0.0016534148793686302\n",
      "train loss:0.009464736693300266\n",
      "train loss:0.00976208556425135\n",
      "train loss:0.00866040574363614\n",
      "train loss:0.02834108646714415\n",
      "train loss:0.0033203445148223005\n",
      "train loss:0.009714660200264263\n",
      "train loss:0.0015039035094658632\n",
      "train loss:0.002646879671785259\n",
      "train loss:0.007585129819767929\n",
      "train loss:0.009297531922366294\n",
      "train loss:0.0014915749121549747\n",
      "train loss:0.013453079852297245\n",
      "train loss:0.004638743711659047\n",
      "train loss:0.028251800295264164\n",
      "train loss:0.00961986039712614\n",
      "train loss:0.003846681962815504\n",
      "train loss:0.0011572698870627868\n",
      "train loss:0.002846773278161366\n",
      "train loss:0.0060186509491785755\n",
      "train loss:0.014544605597947836\n",
      "train loss:0.015168360637719201\n",
      "train loss:0.0028851317256296527\n",
      "train loss:0.0054484028924048356\n",
      "train loss:0.023730591743370808\n",
      "train loss:0.004272868341330496\n",
      "train loss:0.008626553833090227\n",
      "train loss:0.09492251992170647\n",
      "train loss:0.0012928433576024278\n",
      "train loss:0.009236199683922723\n",
      "train loss:0.007817321809140852\n",
      "train loss:0.007522528954699463\n",
      "train loss:0.008521276232675078\n",
      "train loss:0.023003146477866897\n",
      "train loss:0.006331136360844988\n",
      "train loss:0.0033272941216804736\n",
      "train loss:0.00879986258137361\n",
      "train loss:0.01892596533715835\n",
      "train loss:0.0022002936972302283\n",
      "train loss:0.005514110868593181\n",
      "train loss:0.0038453608238518244\n",
      "train loss:0.057289183862833326\n",
      "train loss:0.01855557008591983\n",
      "train loss:0.003980527471766745\n",
      "train loss:0.004448805421495009\n",
      "train loss:0.015366064912460047\n",
      "train loss:0.0053124166457296056\n",
      "train loss:0.002390113545159688\n",
      "train loss:0.024383402902307238\n",
      "train loss:0.007353716243593976\n",
      "train loss:0.0007329547801155365\n",
      "train loss:0.002423698047962501\n",
      "train loss:0.03918736197405541\n",
      "train loss:0.014232129195897076\n",
      "train loss:0.00360086941124219\n",
      "train loss:0.002984610852206487\n",
      "train loss:0.003115545647206428\n",
      "train loss:0.02301771681332812\n",
      "train loss:0.0031423505551840653\n",
      "train loss:0.005460450805109939\n",
      "train loss:0.009602846133357282\n",
      "train loss:0.0018013482221785034\n",
      "train loss:0.012547326704360939\n",
      "train loss:0.00676878688566485\n",
      "train loss:0.0039059649402789966\n",
      "train loss:0.039816140456059364\n",
      "train loss:0.03240616490969546\n",
      "train loss:0.009323433265268539\n",
      "train loss:0.009779381465014045\n",
      "train loss:0.03378958405282038\n",
      "train loss:0.0013268206570312678\n",
      "train loss:0.0004934761961366124\n",
      "train loss:0.011703576082538721\n",
      "train loss:0.005324048407494905\n",
      "=== epoch:9, train acc:0.99, test acc:0.986 ===\n",
      "train loss:0.013576465058601977\n",
      "train loss:0.00993922266476172\n",
      "train loss:0.007867253138589684\n",
      "train loss:0.0021281109577499556\n",
      "train loss:0.011153363968256658\n",
      "train loss:0.022872272789863753\n",
      "train loss:0.008679079586475949\n",
      "train loss:0.01767003147116047\n",
      "train loss:0.0016837863632835884\n",
      "train loss:0.10287642899527022\n",
      "train loss:0.004922079748016184\n",
      "train loss:0.0037824002614052273\n",
      "train loss:0.0057468203081110945\n",
      "train loss:0.007648635154887381\n",
      "train loss:0.008474556170740942\n",
      "train loss:0.0023444138371990306\n",
      "train loss:0.008910582573989806\n",
      "train loss:0.002483073917701951\n",
      "train loss:0.014225293218997468\n",
      "train loss:0.01133752907285386\n",
      "train loss:0.019025774013013357\n",
      "train loss:0.004074461856572909\n",
      "train loss:0.03757939757792118\n",
      "train loss:0.07091768822556244\n",
      "train loss:0.04374143399571669\n",
      "train loss:0.005485984115285677\n",
      "train loss:0.007596203673954153\n",
      "train loss:0.043220608883862956\n",
      "train loss:0.004146519752581783\n",
      "train loss:0.017074830938145102\n",
      "train loss:0.017463476192694186\n",
      "train loss:0.01117785606673339\n",
      "train loss:0.003533138237203387\n",
      "train loss:0.014273968350815592\n",
      "train loss:0.004956684927033125\n",
      "train loss:0.008334586919504823\n",
      "train loss:0.012584934414201769\n",
      "train loss:0.006052342389112203\n",
      "train loss:0.02775428676399995\n",
      "train loss:0.003980075954264414\n",
      "train loss:0.0005438375040894176\n",
      "train loss:0.0008396269145576203\n",
      "train loss:0.014255055323601449\n",
      "train loss:0.0022943375559848407\n",
      "train loss:0.0027458353397164376\n",
      "train loss:0.012973509199220594\n",
      "train loss:0.0012359731721693305\n",
      "train loss:0.018406657006965838\n",
      "train loss:0.004290950859864102\n",
      "train loss:0.0195847705928621\n",
      "train loss:0.034730050760628316\n",
      "train loss:0.004579449794764867\n",
      "train loss:0.008089254809739089\n",
      "train loss:0.003742941557852492\n",
      "train loss:0.005599945995833573\n",
      "train loss:0.02536428575187126\n",
      "train loss:0.0076528644802217076\n",
      "train loss:0.010117020400227012\n",
      "train loss:0.006009893195895701\n",
      "train loss:0.003763862609961432\n",
      "train loss:0.017369154988629218\n",
      "train loss:0.014260479898553986\n",
      "train loss:0.00337899613471009\n",
      "train loss:0.014698349229860191\n",
      "train loss:0.026559849061843256\n",
      "train loss:0.011147689275178292\n",
      "train loss:0.008860669819973141\n",
      "train loss:0.005186744809287196\n",
      "train loss:0.011460470732979025\n",
      "train loss:0.042725154204596015\n",
      "train loss:0.035967368228420185\n",
      "train loss:0.011989127638789652\n",
      "train loss:0.005274554958607141\n",
      "train loss:0.03319372097148344\n",
      "train loss:0.0005943092869821988\n",
      "train loss:0.0061635281925276515\n",
      "train loss:0.013644926199337213\n",
      "train loss:0.016815349842098502\n",
      "train loss:0.003624289422562446\n",
      "train loss:0.006315357773852515\n",
      "train loss:0.011634631829919261\n",
      "train loss:0.00476540594319231\n",
      "train loss:0.015761122497326776\n",
      "train loss:0.017828031441464873\n",
      "train loss:0.05205182730811452\n",
      "train loss:0.009857920275677364\n",
      "train loss:0.002000646435226423\n",
      "train loss:0.008301236501866765\n",
      "train loss:0.011888857146904994\n",
      "train loss:0.0068704672180505385\n",
      "train loss:0.002260495127132621\n",
      "train loss:0.008862066305971989\n",
      "train loss:0.0068594319013617466\n",
      "train loss:0.013817842419921433\n",
      "train loss:0.009783328418562358\n",
      "train loss:0.0011407174024001656\n",
      "train loss:0.005215216820692121\n",
      "train loss:0.0028490517684621854\n",
      "train loss:0.0032013125578924054\n",
      "train loss:0.015431837214506099\n",
      "train loss:0.006574048634690509\n",
      "train loss:0.016149532102858237\n",
      "train loss:0.009344853860493696\n",
      "train loss:0.001798243912417197\n",
      "train loss:0.010752288035695824\n",
      "train loss:0.006568206436219383\n",
      "train loss:0.009577481499375217\n",
      "train loss:0.0037587761568463524\n",
      "train loss:0.0037461974960030873\n",
      "train loss:0.0006832892309808238\n",
      "train loss:0.012692169118757209\n",
      "train loss:0.016866424243805512\n",
      "train loss:0.006458226112568562\n",
      "train loss:0.00942687752237585\n",
      "train loss:0.009098734441023042\n",
      "train loss:0.0007797294462694797\n",
      "train loss:0.005271736454609421\n",
      "train loss:0.005846031868657506\n",
      "train loss:0.004925855866746321\n",
      "train loss:0.02254210328639452\n",
      "train loss:0.019541134764233095\n",
      "train loss:0.013009132713363826\n",
      "train loss:0.0029642632148140446\n",
      "train loss:0.0015156913923488732\n",
      "train loss:0.021198728720326222\n",
      "train loss:0.010528399949817063\n",
      "train loss:0.023512097277208047\n",
      "train loss:0.0164430580622235\n",
      "train loss:0.0074025860041088375\n",
      "train loss:0.03151750471000513\n",
      "train loss:0.06157236720125965\n",
      "train loss:0.00586997682950889\n",
      "train loss:0.017326526449097355\n",
      "train loss:0.00041270875819867227\n",
      "train loss:0.055430956413937596\n",
      "train loss:0.04312578941484701\n",
      "train loss:0.05274930488921365\n",
      "train loss:0.017499108235291307\n",
      "train loss:0.00506266780415985\n",
      "train loss:0.003733460632331433\n",
      "train loss:0.014220814735545834\n",
      "train loss:0.0071858759470818465\n",
      "train loss:0.009678567692527508\n",
      "train loss:0.0016117680329657568\n",
      "train loss:0.017008279637507822\n",
      "train loss:0.006186234914378077\n",
      "train loss:0.013299111809563896\n",
      "train loss:0.004986262244028762\n",
      "train loss:0.0014505722796257412\n",
      "train loss:0.009273567965534862\n",
      "train loss:0.018888253581497996\n",
      "train loss:0.010806566334815327\n",
      "train loss:0.006160610764191413\n",
      "train loss:0.018392279260216807\n",
      "train loss:0.014941448857451236\n",
      "train loss:0.008038564785122635\n",
      "train loss:0.04176468083491477\n",
      "train loss:0.011206106639718359\n",
      "train loss:0.003498184159983113\n",
      "train loss:0.019137943523775985\n",
      "train loss:0.009874435489745715\n",
      "train loss:0.003624223044566148\n",
      "train loss:0.0851588320209487\n",
      "train loss:0.013937431685248247\n",
      "train loss:0.0042339447940566936\n",
      "train loss:0.016234269683535966\n",
      "train loss:0.002923360762223778\n",
      "train loss:0.001954675734621171\n",
      "train loss:0.004170504543647688\n",
      "train loss:0.02919129173460703\n",
      "train loss:0.028223696353483065\n",
      "train loss:0.0074576772738113375\n",
      "train loss:0.003981090954631424\n",
      "train loss:0.0064239206309075\n",
      "train loss:0.004679280686634641\n",
      "train loss:0.015553593643495572\n",
      "train loss:0.032033757468237826\n",
      "train loss:0.01678003204762281\n",
      "train loss:0.010422539465764132\n",
      "train loss:0.0009193793205883543\n",
      "train loss:0.020434149630074193\n",
      "train loss:0.0006888970418335855\n",
      "train loss:0.026430175601444478\n",
      "train loss:0.017597466910957062\n",
      "train loss:0.006504854961470876\n",
      "train loss:0.0048131597701169156\n",
      "train loss:0.031347947439370645\n",
      "train loss:0.011334328349039273\n",
      "train loss:0.008289687759938609\n",
      "train loss:0.10337291260005003\n",
      "train loss:0.015847648643200117\n",
      "train loss:0.001530714473014371\n",
      "train loss:0.0200520557635557\n",
      "train loss:0.000601922936272618\n",
      "train loss:0.008762303348642678\n",
      "train loss:0.01288929926620822\n",
      "train loss:0.014785152106125356\n",
      "train loss:0.005032589604869161\n",
      "train loss:0.018157735322592208\n",
      "train loss:0.00641272393861009\n",
      "train loss:0.0072594818169635445\n",
      "train loss:0.011141243845254245\n",
      "train loss:0.011086928205444593\n",
      "train loss:0.04821000865735588\n",
      "train loss:0.01834695623145775\n",
      "train loss:0.03190573547544352\n",
      "train loss:0.014084905549319127\n",
      "train loss:0.0072474561677931025\n",
      "train loss:0.00842407092055458\n",
      "train loss:0.013751124456248865\n",
      "train loss:0.009746201618725643\n",
      "train loss:0.007900025868186398\n",
      "train loss:0.025882519821233033\n",
      "train loss:0.004218117429764492\n",
      "train loss:0.003953209815766382\n",
      "train loss:0.0059773742243771485\n",
      "train loss:0.0034599276108314824\n",
      "train loss:0.011403512422594585\n",
      "train loss:0.041448248822374564\n",
      "train loss:0.013790003509220222\n",
      "train loss:0.010934841386346095\n",
      "train loss:0.004805590528746812\n",
      "train loss:0.014063764038742805\n",
      "train loss:0.0011700100504404502\n",
      "train loss:0.00571334389508207\n",
      "train loss:0.0024807399891428345\n",
      "train loss:0.010789092409494523\n",
      "train loss:0.010800379363462766\n",
      "train loss:0.001988550515192164\n",
      "train loss:0.00632098503852214\n",
      "train loss:0.0038552404675411545\n",
      "train loss:0.0007950715288718808\n",
      "train loss:0.020217898656972714\n",
      "train loss:0.006708488177022871\n",
      "train loss:0.000915201414600959\n",
      "train loss:0.010297222539886686\n",
      "train loss:0.014710970320701186\n",
      "train loss:0.030404328521209535\n",
      "train loss:0.001874745379841051\n",
      "train loss:0.022735408025187392\n",
      "train loss:0.003365130812763244\n",
      "train loss:0.017224718272078835\n",
      "train loss:0.01821166187104312\n",
      "train loss:0.0056549396779587155\n",
      "train loss:0.005209013392512235\n",
      "train loss:0.019168714862965775\n",
      "train loss:0.00607008593348065\n",
      "train loss:0.014826138996684161\n",
      "train loss:0.009981474629597507\n",
      "train loss:0.005498312859453308\n",
      "train loss:0.031224864232973437\n",
      "train loss:0.009113796492224735\n",
      "train loss:0.007306787999183183\n",
      "train loss:0.005579722928746807\n",
      "train loss:0.003976348037682924\n",
      "train loss:0.010261044235437104\n",
      "train loss:0.0009176482478303237\n",
      "train loss:0.0065659186223758804\n",
      "train loss:0.0006284517126000841\n",
      "train loss:0.007950305820564808\n",
      "train loss:0.005055109516817911\n",
      "train loss:0.02218442201607764\n",
      "train loss:0.010194730102017976\n",
      "train loss:0.007778712426787365\n",
      "train loss:0.018101695556890382\n",
      "train loss:0.010119498983339123\n",
      "train loss:0.010448025939828767\n",
      "train loss:0.0010381290924325736\n",
      "train loss:0.004681933839438557\n",
      "train loss:0.00950837297914542\n",
      "train loss:0.02974884278911232\n",
      "train loss:0.008603472450358392\n",
      "train loss:0.016159096618928608\n",
      "train loss:0.0053991258009594935\n",
      "train loss:0.06021592293435329\n",
      "train loss:0.007434401098145674\n",
      "train loss:0.003888275884044128\n",
      "train loss:0.013444089898985743\n",
      "train loss:0.0017907659183085491\n",
      "train loss:0.005953829859849531\n",
      "train loss:0.004599399991053965\n",
      "train loss:0.018252959783839288\n",
      "train loss:0.013538300412148185\n",
      "train loss:0.010168226189987117\n",
      "train loss:0.009467101857076126\n",
      "train loss:0.0004791536700976706\n",
      "train loss:0.006179640767583688\n",
      "train loss:0.004413298759690261\n",
      "train loss:0.01662233373246726\n",
      "train loss:0.008054166999828338\n",
      "train loss:0.0007141244821196979\n",
      "train loss:0.007508134721594967\n",
      "train loss:0.02477134496644465\n",
      "train loss:0.021182884800946065\n",
      "train loss:0.061413370841252785\n",
      "train loss:0.003473028448373072\n",
      "train loss:0.003433570633569465\n",
      "train loss:0.0038457498681981742\n",
      "train loss:0.006862805880910994\n",
      "train loss:0.003069710915678035\n",
      "train loss:0.0031676208194985583\n",
      "train loss:0.01729236600760577\n",
      "train loss:0.006144711767664801\n",
      "train loss:0.011496752773299755\n",
      "train loss:0.04090558895585548\n",
      "train loss:0.01905197115504925\n",
      "train loss:0.008291297861430652\n",
      "train loss:0.023196041181876004\n",
      "train loss:0.009339458062758557\n",
      "train loss:0.0018476855772882864\n",
      "train loss:0.03225785477264602\n",
      "train loss:0.026058048845605458\n",
      "train loss:0.00728110487671035\n",
      "train loss:0.010963797158667273\n",
      "train loss:0.02735390262371533\n",
      "train loss:0.0022692535220240253\n",
      "train loss:0.002458839493992118\n",
      "train loss:0.00886163451616531\n",
      "train loss:0.011030175129143574\n",
      "train loss:0.012679022293570567\n",
      "train loss:0.024333568295759545\n",
      "train loss:0.009012702307751242\n",
      "train loss:0.034336703467813076\n",
      "train loss:0.028670823898290437\n",
      "train loss:0.04368106903080103\n",
      "train loss:0.008534061165252467\n",
      "train loss:0.056941505398338685\n",
      "train loss:0.0072264854944899745\n",
      "train loss:0.007079404858395526\n",
      "train loss:0.011036996365337419\n",
      "train loss:0.025785049116885412\n",
      "train loss:0.003231046442951838\n",
      "train loss:0.011811972284514409\n",
      "train loss:0.005101975135967449\n",
      "train loss:0.0007159728901613096\n",
      "train loss:0.013976872791801266\n",
      "train loss:0.02224185773547519\n",
      "train loss:0.0020389500449011517\n",
      "train loss:0.012898685600770817\n",
      "train loss:0.0056805266064437225\n",
      "train loss:0.017494633290281232\n",
      "train loss:0.04096819127997429\n",
      "train loss:0.0032859591680331935\n",
      "train loss:0.17525307062272993\n",
      "train loss:0.006029379388193781\n",
      "train loss:0.007555933780195959\n",
      "train loss:0.008861448204051027\n",
      "train loss:0.024830893910127046\n",
      "train loss:0.014151812975768792\n",
      "train loss:0.012773119231640569\n",
      "train loss:0.003179298904769883\n",
      "train loss:0.005251280691692473\n",
      "train loss:0.024794389464366854\n",
      "train loss:0.0071171682684699625\n",
      "train loss:0.032234597851429465\n",
      "train loss:0.0056861526116485485\n",
      "train loss:0.005359644412909097\n",
      "train loss:0.0018520171268143818\n",
      "train loss:0.005585773179385685\n",
      "train loss:0.002701912412242273\n",
      "train loss:0.04380899820542008\n",
      "train loss:0.013254280048636626\n",
      "train loss:0.01177643055720622\n",
      "train loss:0.0012634312838107353\n",
      "train loss:0.0019866079200423787\n",
      "train loss:0.026447128352998027\n",
      "train loss:0.022091302563028544\n",
      "train loss:0.02540742027157723\n",
      "train loss:0.0025311485085526175\n",
      "train loss:0.002873215392868087\n",
      "train loss:0.0044259235999118286\n",
      "train loss:0.007061704248533629\n",
      "train loss:0.004312138717575619\n",
      "train loss:0.005993285149515646\n",
      "train loss:0.005981438434584377\n",
      "train loss:0.0019990062981371246\n",
      "train loss:0.009845473949920483\n",
      "train loss:0.012424151961148959\n",
      "train loss:0.006415120821547431\n",
      "train loss:0.03883530272175288\n",
      "train loss:0.004186812865515749\n",
      "train loss:0.009882575024860382\n",
      "train loss:0.002518067283681972\n",
      "train loss:0.0018525914804897452\n",
      "train loss:0.02124805004517441\n",
      "train loss:0.007135983881877988\n",
      "train loss:0.0010839663540832608\n",
      "train loss:0.012007403263124338\n",
      "train loss:0.00939926941615296\n",
      "train loss:0.01271338059518558\n",
      "train loss:0.006068711038059149\n",
      "train loss:0.017245211027644022\n",
      "train loss:0.021164639725868937\n",
      "train loss:0.002653443318612608\n",
      "train loss:0.0046273899924930235\n",
      "train loss:0.012009465825785753\n",
      "train loss:0.013413623659732086\n",
      "train loss:0.007376906317565099\n",
      "train loss:0.013362537405099155\n",
      "train loss:0.0028903282668867112\n",
      "train loss:0.005054103021605943\n",
      "train loss:0.00511342112082069\n",
      "train loss:0.0027941288367500524\n",
      "train loss:0.012553200774505896\n",
      "train loss:0.016269174080312976\n",
      "train loss:0.0068293329221240096\n",
      "train loss:0.0023481823104781556\n",
      "train loss:0.009342379279494871\n",
      "train loss:0.0015855402909585598\n",
      "train loss:0.00530741285311984\n",
      "train loss:0.008655316204799006\n",
      "train loss:0.008217408902123299\n",
      "train loss:0.007977797809219329\n",
      "train loss:0.004813612156647059\n",
      "train loss:0.008588980504780928\n",
      "train loss:0.01964076569724784\n",
      "train loss:0.005140429700467144\n",
      "train loss:0.005292357203485112\n",
      "train loss:0.0031291339744906003\n",
      "train loss:0.07461962606542138\n",
      "train loss:0.006787610923531243\n",
      "train loss:0.006881629791531227\n",
      "train loss:0.0033681924866416435\n",
      "train loss:0.0025087261622062486\n",
      "train loss:0.012306969660241297\n",
      "train loss:0.0019521620751568641\n",
      "train loss:0.0021412089821430487\n",
      "train loss:0.006134039995861106\n",
      "train loss:0.011167896695868667\n",
      "train loss:0.023754922306972234\n",
      "train loss:0.006524483068196434\n",
      "train loss:0.00253042987303247\n",
      "train loss:0.01702686755302133\n",
      "train loss:0.001198483128536017\n",
      "train loss:0.0026451383938155965\n",
      "train loss:0.0034084545959537087\n",
      "train loss:0.01095598899556928\n",
      "train loss:0.0054945634194097735\n",
      "train loss:0.02668924049016895\n",
      "train loss:0.002065042633013599\n",
      "train loss:0.004908214348990155\n",
      "train loss:0.003458881514472363\n",
      "train loss:0.03254802769292998\n",
      "train loss:0.0433922215515292\n",
      "train loss:0.0031499537334346963\n",
      "train loss:0.0056481587663552435\n",
      "train loss:0.0021299558017235216\n",
      "train loss:0.009726640998341234\n",
      "train loss:0.01227576434674912\n",
      "train loss:0.008378305297430068\n",
      "train loss:0.009527088691682615\n",
      "train loss:0.020234343771713438\n",
      "train loss:0.0033916092423421362\n",
      "train loss:0.00989916933034966\n",
      "train loss:0.007034205487086223\n",
      "train loss:0.008267599768028015\n",
      "train loss:0.0008664438170490495\n",
      "train loss:0.0007880205635355887\n",
      "train loss:0.009427546186505688\n",
      "train loss:0.013120112301033673\n",
      "train loss:0.020719378155812363\n",
      "train loss:0.006311221874752519\n",
      "train loss:0.004155451811596525\n",
      "train loss:0.009206667669365506\n",
      "train loss:0.018854027800968894\n",
      "train loss:0.004590051013445134\n",
      "train loss:0.0006317575073754924\n",
      "train loss:0.0015106662664629638\n",
      "train loss:0.03571485505252933\n",
      "train loss:0.0005935540888035136\n",
      "train loss:0.02397402942514274\n",
      "train loss:0.004694838960102262\n",
      "train loss:0.006269983397324388\n",
      "train loss:0.013544738734441347\n",
      "train loss:0.008410013345717617\n",
      "train loss:0.020858383693914198\n",
      "train loss:0.007075666861965851\n",
      "train loss:0.012210380152037887\n",
      "train loss:0.008193252117302409\n",
      "train loss:0.004145357131025214\n",
      "train loss:0.00786674192838538\n",
      "train loss:0.003978100649178875\n",
      "train loss:0.0037519299201897575\n",
      "train loss:0.028246138683346488\n",
      "train loss:0.0026091878594075614\n",
      "train loss:0.006406258655054048\n",
      "train loss:0.007380702671280073\n",
      "train loss:0.001557949292758068\n",
      "train loss:0.004269261448437229\n",
      "train loss:0.030481181333351134\n",
      "train loss:0.002558023559391468\n",
      "train loss:0.001216113921907186\n",
      "train loss:0.0018875318907939087\n",
      "train loss:0.007727062374600297\n",
      "train loss:0.0033986120041691907\n",
      "train loss:0.0036134429522423728\n",
      "train loss:0.007749150000682551\n",
      "train loss:0.0054760466549513675\n",
      "train loss:0.008273373903995806\n",
      "train loss:0.06622789047043753\n",
      "train loss:0.002114427108398498\n",
      "train loss:0.018551205958011616\n",
      "train loss:0.021528660004151386\n",
      "train loss:0.00371601050452164\n",
      "train loss:0.001474030341236022\n",
      "train loss:0.018697420583953333\n",
      "train loss:0.0026399864451668903\n",
      "train loss:0.004393674629261103\n",
      "train loss:0.007606922653469607\n",
      "train loss:0.007350147014839567\n",
      "train loss:0.0033818530414589944\n",
      "train loss:0.007647706688321637\n",
      "train loss:0.0016039694108466028\n",
      "train loss:0.011522668151355284\n",
      "train loss:0.005147784544046261\n",
      "train loss:0.003348001655527229\n",
      "train loss:0.0032313614524772994\n",
      "train loss:0.017013988391896294\n",
      "train loss:0.03294523593644645\n",
      "train loss:0.0013527488860161356\n",
      "train loss:0.024081206669463683\n",
      "train loss:0.02593207924435516\n",
      "train loss:0.003360912444119717\n",
      "train loss:0.001536633377552154\n",
      "train loss:0.0047945967174008045\n",
      "train loss:0.0013907852270817467\n",
      "train loss:0.06882863904059719\n",
      "train loss:0.001181057821670756\n",
      "train loss:0.003213171443239345\n",
      "train loss:0.0009651824401317059\n",
      "train loss:0.0014436754803192633\n",
      "train loss:0.010732529722149492\n",
      "train loss:0.02100101390974418\n",
      "train loss:0.0032356777933454377\n",
      "train loss:0.0028810190801918595\n",
      "train loss:0.006923096590593944\n",
      "train loss:0.017029436174223943\n",
      "train loss:0.02418136972443349\n",
      "train loss:0.0013293540344243202\n",
      "train loss:0.0030723677539273514\n",
      "train loss:0.00170056676096389\n",
      "train loss:0.019799895555789357\n",
      "train loss:0.004641685403679571\n",
      "train loss:0.0033068275241592104\n",
      "train loss:0.010180601953011665\n",
      "train loss:0.012027797308834132\n",
      "train loss:0.004138896702125386\n",
      "train loss:0.002762202529917549\n",
      "train loss:0.013277594071218623\n",
      "train loss:0.011538581601418033\n",
      "train loss:0.001064802766513938\n",
      "train loss:0.006239475179473142\n",
      "train loss:0.009731564526495418\n",
      "train loss:0.005665611584673007\n",
      "train loss:0.03604786740847894\n",
      "train loss:0.004356036548029079\n",
      "train loss:0.003028598536123866\n",
      "train loss:0.008576605565197896\n",
      "train loss:0.020178586274560893\n",
      "train loss:0.002110680797110226\n",
      "train loss:0.004249595445050355\n",
      "train loss:0.009721352244422131\n",
      "train loss:0.006278381374638196\n",
      "train loss:0.03296063753668728\n",
      "train loss:0.002641616380655012\n",
      "train loss:0.0010995472352045255\n",
      "train loss:0.002753553202723167\n",
      "train loss:0.02708407905274992\n",
      "train loss:0.005908249208693167\n",
      "train loss:0.014819723084063778\n",
      "train loss:0.002826308904668215\n",
      "train loss:0.0013523218801373695\n",
      "train loss:0.023897097631564314\n",
      "train loss:0.004968797511941165\n",
      "train loss:0.01671229569039969\n",
      "train loss:0.0032653580167838953\n",
      "train loss:0.00922450860006665\n",
      "train loss:0.007224792459144949\n",
      "train loss:0.0024226926740743377\n",
      "train loss:0.0014196500225815136\n",
      "train loss:0.012073511984806232\n",
      "train loss:0.05173343772111192\n",
      "train loss:0.00043963304794624245\n",
      "train loss:0.005237265561575784\n",
      "train loss:0.004762492790837079\n",
      "train loss:0.009012238834754951\n",
      "train loss:0.007163548530226633\n",
      "train loss:0.004079861431801479\n",
      "train loss:0.013373336881149955\n",
      "train loss:0.0011213023759386346\n",
      "train loss:0.0042165387916016815\n",
      "train loss:0.0045493283855194125\n",
      "train loss:0.0022006131320107193\n",
      "train loss:0.006463224251871947\n",
      "train loss:0.0885893673777843\n",
      "train loss:0.0038108930074100904\n",
      "train loss:0.03441674358948497\n",
      "train loss:0.001755985890545577\n",
      "train loss:0.01138328866642519\n",
      "train loss:0.0013937430253521312\n",
      "=== epoch:10, train acc:0.995, test acc:0.99 ===\n",
      "train loss:0.0009327546991807305\n",
      "train loss:0.001204952174057036\n",
      "train loss:0.003956540701804378\n",
      "train loss:0.006671398437532958\n",
      "train loss:0.0014839089280849404\n",
      "train loss:0.0014267910740093418\n",
      "train loss:0.004356877461948467\n",
      "train loss:0.0018332387208763274\n",
      "train loss:0.004254966994289383\n",
      "train loss:0.007443838215163906\n",
      "train loss:0.0131043375844679\n",
      "train loss:0.0007258851633649846\n",
      "train loss:0.0033940879510333255\n",
      "train loss:0.0052840969115028015\n",
      "train loss:0.00043119190660585076\n",
      "train loss:0.016948045972614515\n",
      "train loss:0.0482664473891872\n",
      "train loss:0.0029091809221730275\n",
      "train loss:0.014602717223299115\n",
      "train loss:0.013357287008721082\n",
      "train loss:0.0030648722996764934\n",
      "train loss:0.008245307709512342\n",
      "train loss:0.010187882948872115\n",
      "train loss:0.007366006825747991\n",
      "train loss:0.008877328629046905\n",
      "train loss:0.003319849886857336\n",
      "train loss:0.005635721782406528\n",
      "train loss:0.004169735164805755\n",
      "train loss:0.005121126705295323\n",
      "train loss:0.005843775228543886\n",
      "train loss:0.01699953309894006\n",
      "train loss:0.005733401893741346\n",
      "train loss:0.0035851572564306864\n",
      "train loss:0.0028216499658114485\n",
      "train loss:0.004554524792872836\n",
      "train loss:0.003867473489366325\n",
      "train loss:0.02807174068090753\n",
      "train loss:0.0054594391185196055\n",
      "train loss:0.001971796987240353\n",
      "train loss:0.0006243024566607964\n",
      "train loss:0.02675948944068491\n",
      "train loss:0.01663979230549212\n",
      "train loss:0.0012871993770230217\n",
      "train loss:0.000480819960091978\n",
      "train loss:0.018534744278782676\n",
      "train loss:0.007898179043935383\n",
      "train loss:0.002838189659496162\n",
      "train loss:0.00081859092200535\n",
      "train loss:0.005560732839619593\n",
      "train loss:0.007965831733593687\n",
      "train loss:0.0035513824385361233\n",
      "train loss:0.01422356999908022\n",
      "train loss:0.0020167103401090344\n",
      "train loss:0.007696782187579585\n",
      "train loss:0.004628065187335186\n",
      "train loss:0.0013474627800279981\n",
      "train loss:0.002964016522463348\n",
      "train loss:0.00209017221079478\n",
      "train loss:0.0077971098721082545\n",
      "train loss:0.01945887751494371\n",
      "train loss:0.0037011511948672354\n",
      "train loss:0.0035738901006910338\n",
      "train loss:0.0054388299846591095\n",
      "train loss:0.002361220234814153\n",
      "train loss:0.00420457700706857\n",
      "train loss:0.0007367428551667784\n",
      "train loss:0.0051671061617022665\n",
      "train loss:0.029175600304661188\n",
      "train loss:0.0025402485990651817\n",
      "train loss:0.0020388534991356284\n",
      "train loss:0.009325543663342569\n",
      "train loss:0.008797491539992514\n",
      "train loss:0.04446487522915203\n",
      "train loss:0.0029458587079335074\n",
      "train loss:0.004587858095010411\n",
      "train loss:0.004027632206516509\n",
      "train loss:0.010252303391914692\n",
      "train loss:0.011961736149684295\n",
      "train loss:0.039788502592084635\n",
      "train loss:0.03369549628278874\n",
      "train loss:0.006657101427340135\n",
      "train loss:0.009242474513691116\n",
      "train loss:0.0020044772491027256\n",
      "train loss:0.004698919694313341\n",
      "train loss:0.030687839693920065\n",
      "train loss:0.008859206010073766\n",
      "train loss:0.010233838506662332\n",
      "train loss:0.003983047139148614\n",
      "train loss:0.021633260802258317\n",
      "train loss:0.0027405725676505027\n",
      "train loss:0.007035327361425434\n",
      "train loss:0.0447059137325041\n",
      "train loss:0.010998777566610446\n",
      "train loss:0.0025389818889716563\n",
      "train loss:0.0005241441686978688\n",
      "train loss:0.008836164050542685\n",
      "train loss:0.033687907842482254\n",
      "train loss:0.009863664673932545\n",
      "train loss:0.008897150090213248\n",
      "train loss:0.0026793724184576402\n",
      "train loss:0.012137635853096677\n",
      "train loss:0.002425338570745869\n",
      "train loss:0.035746829715018205\n",
      "train loss:0.0038388365937236215\n",
      "train loss:0.004952523966253841\n",
      "train loss:0.0029207513635806674\n",
      "train loss:0.005181940852296704\n",
      "train loss:0.011412998469805112\n",
      "train loss:0.0010388831819106905\n",
      "train loss:0.00970471280261902\n",
      "train loss:0.005315112401328443\n",
      "train loss:0.01680975520299056\n",
      "train loss:0.0041954464940713665\n",
      "train loss:0.012601011251236992\n",
      "train loss:0.003524267090816333\n",
      "train loss:0.0018943135181475562\n",
      "train loss:0.014691227281626975\n",
      "train loss:0.005727363840681751\n",
      "train loss:0.036275552279324975\n",
      "train loss:0.004800021318968667\n",
      "train loss:0.00792222526568643\n",
      "train loss:0.004105662469294578\n",
      "train loss:0.006089963861805223\n",
      "train loss:0.0034124306453677455\n",
      "train loss:0.0036776727474191933\n",
      "train loss:0.0017992206120594578\n",
      "train loss:0.008353181911179463\n",
      "train loss:0.004013657227692054\n",
      "train loss:0.012624570104280489\n",
      "train loss:0.002858388227559017\n",
      "train loss:0.003957477668106883\n",
      "train loss:0.0031300996309589525\n",
      "train loss:0.028224732353687415\n",
      "train loss:0.003774685191079124\n",
      "train loss:0.0054715965245516985\n",
      "train loss:0.0003410301583208774\n",
      "train loss:0.0045930340368733015\n",
      "train loss:0.0035922292692346035\n",
      "train loss:0.0039017087666025053\n",
      "train loss:0.03445962772542154\n",
      "train loss:0.00246388172130589\n",
      "train loss:0.0006651861217922174\n",
      "train loss:0.00038740620702629056\n",
      "train loss:0.0026759342383310264\n",
      "train loss:0.006779230443926958\n",
      "train loss:0.018589478404057433\n",
      "train loss:0.0015558468614688794\n",
      "train loss:0.01692994392362992\n",
      "train loss:0.003415026953109328\n",
      "train loss:0.007349355817535885\n",
      "train loss:0.004473909932327899\n",
      "train loss:0.030335047796448023\n",
      "train loss:0.007384587285841901\n",
      "train loss:0.00039434401427867647\n",
      "train loss:0.004099219405545977\n",
      "train loss:0.00299202231423433\n",
      "train loss:0.0014127142893705378\n",
      "train loss:0.01734688318105045\n",
      "train loss:0.011822685407102122\n",
      "train loss:0.003400910449761417\n",
      "train loss:0.0030763664930898256\n",
      "train loss:0.002492096933020775\n",
      "train loss:0.0006088985910677699\n",
      "train loss:0.0018539927945719077\n",
      "train loss:0.0023004116615260137\n",
      "train loss:0.006811079166716999\n",
      "train loss:0.009609980669263213\n",
      "train loss:0.010011668179847118\n",
      "train loss:0.0047527341102692085\n",
      "train loss:0.002306470459099837\n",
      "train loss:0.07830755206836673\n",
      "train loss:0.0025885179768319624\n",
      "train loss:0.00822826392805651\n",
      "train loss:0.005217953659307277\n",
      "train loss:0.0027954730080562394\n",
      "train loss:0.006025549114325626\n",
      "train loss:0.0008167474953992489\n",
      "train loss:0.0019051950932036528\n",
      "train loss:0.0055717489559990166\n",
      "train loss:0.014237664501734843\n",
      "train loss:0.04493946062305602\n",
      "train loss:0.018641443070192828\n",
      "train loss:0.003746715372650233\n",
      "train loss:0.003076365571442658\n",
      "train loss:0.0020328557098541395\n",
      "train loss:0.0044669647598303145\n",
      "train loss:0.014205569536196751\n",
      "train loss:0.0023105251242573\n",
      "train loss:0.000651714171571934\n",
      "train loss:0.004132566093829753\n",
      "train loss:0.009362285911377811\n",
      "train loss:0.002278592911419975\n",
      "train loss:0.006090434959808912\n",
      "train loss:0.0020814451788483605\n",
      "train loss:0.004449604201041182\n",
      "train loss:0.003836166509561133\n",
      "train loss:0.009398770974831511\n",
      "train loss:0.005818365393052799\n",
      "train loss:0.05555740671093564\n",
      "train loss:0.004524066896328091\n",
      "train loss:0.002707266100271888\n",
      "train loss:0.044763420531110897\n",
      "train loss:0.009100734319323279\n",
      "train loss:0.006999053904446892\n",
      "train loss:0.009264703735786485\n",
      "train loss:0.004384549982923925\n",
      "train loss:0.01612644597972702\n",
      "train loss:0.0029295431618152445\n",
      "train loss:0.002885635801429532\n",
      "train loss:0.005453092328098377\n",
      "train loss:0.004792680462632486\n",
      "train loss:0.0019325978241845837\n",
      "train loss:0.004554102583682423\n",
      "train loss:0.010724296867531433\n",
      "train loss:0.014860089481341907\n",
      "train loss:0.011664533108908075\n",
      "train loss:0.05058626196617229\n",
      "train loss:0.0042476969007555635\n",
      "train loss:0.04202634719703066\n",
      "train loss:0.0048305407039301975\n",
      "train loss:0.007144402535785034\n",
      "train loss:0.009615938232051962\n",
      "train loss:0.0021848772685966102\n",
      "train loss:0.01029242617720945\n",
      "train loss:0.0053564296556895645\n",
      "train loss:0.046619662645894344\n",
      "train loss:0.00496721752515443\n",
      "train loss:0.0006643962982133028\n",
      "train loss:0.024446357984181873\n",
      "train loss:0.0032152849635437904\n",
      "train loss:0.0007672217094007227\n",
      "train loss:0.002145474393879256\n",
      "train loss:0.006098196391124111\n",
      "train loss:0.024944500702217563\n",
      "train loss:0.0016272612853852652\n",
      "train loss:0.021028138249900036\n",
      "train loss:0.004842184148146594\n",
      "train loss:0.014520924541602526\n",
      "train loss:0.0013289571168815912\n",
      "train loss:0.012403789259458056\n",
      "train loss:0.006159729180656338\n",
      "train loss:0.002628673155359842\n",
      "train loss:0.003263205319553125\n",
      "train loss:0.01594842579476283\n",
      "train loss:0.008020171778722435\n",
      "train loss:0.014727727595552116\n",
      "train loss:0.01636408268901366\n",
      "train loss:0.012625489952978844\n",
      "train loss:0.00567025828286017\n",
      "train loss:0.0106257774305371\n",
      "train loss:0.0031548091676634805\n",
      "train loss:0.008897742469661933\n",
      "train loss:0.004039238769379671\n",
      "train loss:0.0006284986198110497\n",
      "train loss:0.013442292120165565\n",
      "train loss:0.025907364751082948\n",
      "train loss:0.026120180730920876\n",
      "train loss:0.004788404304407658\n",
      "train loss:0.01463356137092246\n",
      "train loss:0.026386160297327464\n",
      "train loss:0.01240067451591586\n",
      "train loss:0.0042777557365003055\n",
      "train loss:0.004378710916970134\n",
      "train loss:0.019941961227046945\n",
      "train loss:0.0008058632388485698\n",
      "train loss:0.007478437948311304\n",
      "train loss:0.0070601399093181945\n",
      "train loss:0.003240424796691227\n",
      "train loss:0.005406061731824282\n",
      "train loss:0.06285819159840315\n",
      "train loss:0.03816259905824731\n",
      "train loss:0.006134687839403018\n",
      "train loss:0.05823993125171967\n",
      "train loss:0.00699782272399524\n",
      "train loss:0.001976157473187009\n",
      "train loss:0.010049073298177008\n",
      "train loss:0.01205515696925427\n",
      "train loss:0.0022359146037614903\n",
      "train loss:0.0028493459904039486\n",
      "train loss:0.005111170747843261\n",
      "train loss:0.004846723080952827\n",
      "train loss:0.00821433451584644\n",
      "train loss:0.004350272629528166\n",
      "train loss:0.005088043110739637\n",
      "train loss:0.030259150179415637\n",
      "train loss:0.019119202419823123\n",
      "train loss:0.0026879672461142586\n",
      "train loss:0.007761860056887408\n",
      "train loss:0.000745589046069426\n",
      "train loss:0.023205241512774852\n",
      "train loss:0.0028301307994846557\n",
      "train loss:0.015673505102371497\n",
      "train loss:0.03171287059686\n",
      "train loss:0.0034606823737844405\n",
      "train loss:0.0057583263726819385\n",
      "train loss:0.006702487222297532\n",
      "train loss:0.00380671399887748\n",
      "train loss:0.0015286074332051547\n",
      "train loss:0.003114164093216321\n",
      "train loss:0.003437479151200403\n",
      "train loss:0.029553474863708618\n",
      "train loss:0.009666113173145951\n",
      "train loss:0.016600943886585437\n",
      "train loss:0.0014600276063366975\n",
      "train loss:0.0033449033048359588\n",
      "train loss:0.0032098517997790994\n",
      "train loss:0.0037206795862686267\n",
      "train loss:0.006818725652508994\n",
      "train loss:0.003127037827507909\n",
      "train loss:0.021414037250251924\n",
      "train loss:0.0029498168030209783\n",
      "train loss:0.007388350726813363\n",
      "train loss:0.012350221103971628\n",
      "train loss:0.00532255572408054\n",
      "train loss:0.006536778365935887\n",
      "train loss:0.0008364898243843218\n",
      "train loss:0.00573704747092098\n",
      "train loss:0.006129871199911141\n",
      "train loss:0.003474204743057721\n",
      "train loss:0.012062574845654017\n",
      "train loss:0.037795021391209924\n",
      "train loss:0.003210096983493368\n",
      "train loss:0.003550327044666392\n",
      "train loss:0.006485249818704249\n",
      "train loss:0.015939312994144406\n",
      "train loss:0.0021531512627484943\n",
      "train loss:0.009293599454900768\n",
      "train loss:0.0020422120456138433\n",
      "train loss:0.002288981161230915\n",
      "train loss:0.002106314719728777\n",
      "train loss:0.034362533451860314\n",
      "train loss:0.0039070394724191205\n",
      "train loss:0.009921739547364042\n",
      "train loss:0.009065029362848597\n",
      "train loss:0.009583917885862514\n",
      "train loss:0.007529292229627033\n",
      "train loss:0.004786429466433898\n",
      "train loss:0.015338580223370892\n",
      "train loss:0.019130710168217067\n",
      "train loss:0.026964132316627355\n",
      "train loss:0.024303903915004813\n",
      "train loss:0.004863171971078568\n",
      "train loss:0.008371244202391494\n",
      "train loss:0.015710651843221176\n",
      "train loss:0.003237451053224942\n",
      "train loss:0.0014790151507631825\n",
      "train loss:0.010946784739000263\n",
      "train loss:0.0011764200439146712\n",
      "train loss:0.012991996041358726\n",
      "train loss:0.035163580349445334\n",
      "train loss:0.0025763764826047197\n",
      "train loss:0.015491408958577764\n",
      "train loss:0.0014723933407530402\n",
      "train loss:0.011236238763604817\n",
      "train loss:0.011130278360625188\n",
      "train loss:0.01641012469878058\n",
      "train loss:0.0023074199997922084\n",
      "train loss:0.017388927248571148\n",
      "train loss:0.005581621050590026\n",
      "train loss:0.0027267537272466845\n",
      "train loss:0.0070319346244257916\n",
      "train loss:0.0178587522323797\n",
      "train loss:0.0004048934875031409\n",
      "train loss:0.0002706576961158487\n",
      "train loss:0.0025847716714684195\n",
      "train loss:0.031760517220320084\n",
      "train loss:0.0027340075307572087\n",
      "train loss:0.020808675923868626\n",
      "train loss:0.0003800626956454372\n",
      "train loss:0.007046906984439128\n",
      "train loss:0.003947652091821427\n",
      "train loss:0.0024712240365228335\n",
      "train loss:0.001529786233789536\n",
      "train loss:0.01389598152874877\n",
      "train loss:0.002383892104534076\n",
      "train loss:0.015802513451295398\n",
      "train loss:0.010166835177850775\n",
      "train loss:0.0011811826721945665\n",
      "train loss:0.019336152511436302\n",
      "train loss:0.0018108011500685458\n",
      "train loss:0.008036191226405837\n",
      "train loss:0.0037656259262827106\n",
      "train loss:0.013306935462859533\n",
      "train loss:0.00043713066813007274\n",
      "train loss:0.004878680163691718\n",
      "train loss:0.024861964857589656\n",
      "train loss:0.012263031694559622\n",
      "train loss:0.0016708817205222893\n",
      "train loss:0.0020471227370577505\n",
      "train loss:0.0029369831854896503\n",
      "train loss:0.01173877393901559\n",
      "train loss:0.006782229577498923\n",
      "train loss:0.0014494048141512645\n",
      "train loss:0.008773628866620675\n",
      "train loss:0.0022364193592429578\n",
      "train loss:0.007276684604237361\n",
      "train loss:0.002479161587539991\n",
      "train loss:0.0027909432248807335\n",
      "train loss:0.0054539319618782545\n",
      "train loss:0.0038262297299489607\n",
      "train loss:0.007066508864112458\n",
      "train loss:0.006436851280563349\n",
      "train loss:0.004092679543197279\n",
      "train loss:0.008366301709417798\n",
      "train loss:0.0005830212642149863\n",
      "train loss:0.016172291767554053\n",
      "train loss:0.010669584710266767\n",
      "train loss:0.0003919600819227033\n",
      "train loss:0.019701183134358397\n",
      "train loss:0.0010387680372267573\n",
      "train loss:0.01523498752635595\n",
      "train loss:0.022721029794391702\n",
      "train loss:0.0033206968993169193\n",
      "train loss:0.004685003023297466\n",
      "train loss:0.008974408450733426\n",
      "train loss:0.006237662392823766\n",
      "train loss:0.012033839467325351\n",
      "train loss:0.0007289442436718507\n",
      "train loss:0.0032003555415258222\n",
      "train loss:0.007426999901783756\n",
      "train loss:0.0007500243265084691\n",
      "train loss:0.011988078621237122\n",
      "train loss:0.0032654208348683084\n",
      "train loss:0.0035194509006539654\n",
      "train loss:0.0034073684876122974\n",
      "train loss:0.0021523363230973275\n",
      "train loss:0.012739914870128249\n",
      "train loss:0.03249652652765389\n",
      "train loss:0.0017626012543813052\n",
      "train loss:0.010251236149567163\n",
      "train loss:0.0017106952973600363\n",
      "train loss:0.003724677912005034\n",
      "train loss:0.017444649193919245\n",
      "train loss:0.0027251217469348404\n",
      "train loss:0.007080590041094038\n",
      "train loss:0.002063380932932426\n",
      "train loss:0.011449655483275346\n",
      "train loss:0.018201909404049466\n",
      "train loss:0.006456410656592637\n",
      "train loss:0.06763317156424678\n",
      "train loss:0.004942480199325475\n",
      "train loss:0.0008845123558318822\n",
      "train loss:0.004421835518390622\n",
      "train loss:0.0087363761136733\n",
      "train loss:0.013790484202188003\n",
      "train loss:0.020698506912770694\n",
      "train loss:0.0029176533645115543\n",
      "train loss:0.001979832548769234\n",
      "train loss:0.009164693837336618\n",
      "train loss:0.0015883675549530072\n",
      "train loss:0.003899950600771228\n",
      "train loss:0.003435948380245481\n",
      "train loss:0.018924496949645736\n",
      "train loss:0.0015969784653225066\n",
      "train loss:0.017168048452590037\n",
      "train loss:0.008059175854997977\n",
      "train loss:0.002238871518503496\n",
      "train loss:0.0058848501690246776\n",
      "train loss:0.012741647931408225\n",
      "train loss:0.014063056739542137\n",
      "train loss:0.005620604008377533\n",
      "train loss:0.0018082565791243531\n",
      "train loss:0.018172495325865245\n",
      "train loss:0.004422705081493423\n",
      "train loss:0.006501444832643153\n",
      "train loss:0.007779779538913655\n",
      "train loss:0.007578683632763392\n",
      "train loss:0.005940404115527671\n",
      "train loss:0.0038009996376567635\n",
      "train loss:0.0005733498138839274\n",
      "train loss:0.014386159188326999\n",
      "train loss:0.004906496389680613\n",
      "train loss:0.011994943073353238\n",
      "train loss:0.0077209835743236276\n",
      "train loss:0.0027076648941536646\n",
      "train loss:0.002926314621902523\n",
      "train loss:0.012880005695110457\n",
      "train loss:0.0009914869582232175\n",
      "train loss:0.00640611739334199\n",
      "train loss:0.002588660145989951\n",
      "train loss:0.0018149284827167142\n",
      "train loss:0.007222172008868135\n",
      "train loss:0.011005862584762265\n",
      "train loss:0.006751386117474333\n",
      "train loss:0.006962725064469034\n",
      "train loss:0.0036171225337344355\n",
      "train loss:0.012342248406391788\n",
      "train loss:0.004877119053351112\n",
      "train loss:0.006958109250132445\n",
      "train loss:0.0036452629639715808\n",
      "train loss:0.001957171479863303\n",
      "train loss:0.005548612737230108\n",
      "train loss:0.02240490262067145\n",
      "train loss:0.005167118155820261\n",
      "train loss:0.004831591908482496\n",
      "train loss:0.002885691228548557\n",
      "train loss:0.030778852037154546\n",
      "train loss:0.008600185300213949\n",
      "train loss:0.014653496251553827\n",
      "train loss:0.009820759112168015\n",
      "train loss:0.002443596749628764\n",
      "train loss:0.004955917555611931\n",
      "train loss:0.006863456133236923\n",
      "train loss:0.0006372211068991861\n",
      "train loss:0.03655464557189967\n",
      "train loss:0.007508110889398555\n",
      "train loss:0.0014463259327759047\n",
      "train loss:0.017902592263438353\n",
      "train loss:0.009820445045171964\n",
      "train loss:0.002041180798786717\n",
      "train loss:0.0012345878923566214\n",
      "train loss:0.0726001509917503\n",
      "train loss:0.001387986547075812\n",
      "train loss:0.011008504659662053\n",
      "train loss:0.0059901702316092435\n",
      "train loss:0.0034552106228090294\n",
      "train loss:0.012371999407954758\n",
      "train loss:0.007922282580083602\n",
      "train loss:0.0064848778971581846\n",
      "train loss:0.007502743422261558\n",
      "train loss:0.004521307230508479\n",
      "train loss:0.001501389988764133\n",
      "train loss:0.014361553012067194\n",
      "train loss:0.0029647942982821886\n",
      "train loss:0.012230456235325746\n",
      "train loss:0.0012756713131909761\n",
      "train loss:0.00870648447666482\n",
      "train loss:0.0011220173936636222\n",
      "train loss:0.019642383603240478\n",
      "train loss:0.0032219813878677957\n",
      "train loss:0.010528535553412148\n",
      "train loss:0.00524696226620699\n",
      "train loss:0.011797387618152515\n",
      "train loss:0.0015928479554501874\n",
      "train loss:0.0022679771816543526\n",
      "train loss:0.009667559135223793\n",
      "train loss:0.004387024696455453\n",
      "train loss:0.00406972594923596\n",
      "train loss:0.0036323942882497233\n",
      "train loss:0.0021289913033187666\n",
      "train loss:0.005988377610124782\n",
      "train loss:0.00548034193354456\n",
      "train loss:0.017507351472974216\n",
      "train loss:0.01046742941135815\n",
      "train loss:0.0020989952884097832\n",
      "train loss:0.03703150776147345\n",
      "train loss:0.008421087416998634\n",
      "train loss:0.006547594438921222\n",
      "train loss:0.00830975989430662\n",
      "train loss:0.008750780184795045\n",
      "train loss:0.01073016272793102\n",
      "train loss:0.0060313365359384785\n",
      "train loss:0.012184561524987368\n",
      "train loss:0.0102463836660715\n",
      "train loss:0.008143601839129832\n",
      "train loss:0.011233039313985317\n",
      "train loss:0.00426106716605447\n",
      "train loss:0.0005529044528657979\n",
      "train loss:0.001989710979793778\n",
      "train loss:0.07582772031039203\n",
      "train loss:0.0027101386464304057\n",
      "train loss:0.00937530105137712\n",
      "train loss:0.006000958420771228\n",
      "train loss:0.0019102740062494055\n",
      "train loss:0.0016978502404310584\n",
      "train loss:0.004961936525962659\n",
      "train loss:0.0099132376907369\n",
      "train loss:0.0008184899869430635\n",
      "train loss:0.002806735345570579\n",
      "train loss:0.006888335509830135\n",
      "train loss:0.003602105760975287\n",
      "train loss:0.01093308988236582\n",
      "train loss:0.0058558863448608615\n",
      "train loss:0.006369041984044337\n",
      "train loss:0.009782597442610286\n",
      "train loss:0.0027004134553925497\n",
      "train loss:0.003568722409933867\n",
      "train loss:0.004012818779013225\n",
      "train loss:0.002756952096927701\n",
      "train loss:0.0007985435484161471\n",
      "train loss:0.010056365533503224\n",
      "train loss:0.00416949453256008\n",
      "train loss:0.014155152301908171\n",
      "train loss:0.031200443915649755\n",
      "train loss:0.0011054565766958385\n",
      "train loss:0.008727629113760613\n",
      "train loss:0.0009777058611321057\n",
      "train loss:0.049029298288708\n",
      "train loss:0.007515289624425322\n",
      "train loss:0.0023743755423137904\n",
      "train loss:0.0013476057991760198\n",
      "train loss:0.0016443279747874017\n",
      "train loss:0.0036450818652066265\n",
      "train loss:0.000819228863941418\n",
      "train loss:0.00379767159412999\n",
      "train loss:0.0009476976361243072\n",
      "train loss:0.009360427778345871\n",
      "train loss:0.05568930674643999\n",
      "train loss:0.003894230693471662\n",
      "train loss:0.044800665823654565\n",
      "=== epoch:11, train acc:0.996, test acc:0.986 ===\n",
      "train loss:0.012847364402924739\n",
      "train loss:0.008264819680397955\n",
      "train loss:0.0073949487968756064\n",
      "train loss:0.009397624389178549\n",
      "train loss:0.016618042421556945\n",
      "train loss:0.014269548155282061\n",
      "train loss:0.004706228114945695\n",
      "train loss:0.009814700607903232\n",
      "train loss:0.002433269523126497\n",
      "train loss:0.0013927022230476502\n",
      "train loss:0.0033402380291509045\n",
      "train loss:0.04587182329263412\n",
      "train loss:0.005527407967621562\n",
      "train loss:0.0022865081321887027\n",
      "train loss:0.0007298610058712835\n",
      "train loss:0.0005640181429713375\n",
      "train loss:0.04084560666156463\n",
      "train loss:0.015910027926411822\n",
      "train loss:0.0038762191899158805\n",
      "train loss:0.01965245459801343\n",
      "train loss:0.0004848857525938319\n",
      "train loss:0.006570426381690399\n",
      "train loss:0.026400671044680343\n",
      "train loss:0.015713350185788556\n",
      "train loss:0.007485787156294919\n",
      "train loss:0.006636127236917537\n",
      "train loss:0.002300600321780881\n",
      "train loss:0.0031025147573868128\n",
      "train loss:0.007839186233092866\n",
      "train loss:0.008069276143826416\n",
      "train loss:0.009406007733663369\n",
      "train loss:0.01166730825280297\n",
      "train loss:0.0020233644775433227\n",
      "train loss:0.008699026945851914\n",
      "train loss:0.006810663098982721\n",
      "train loss:0.0035247409270551698\n",
      "train loss:0.0007602927297982769\n",
      "train loss:0.002743252379931246\n",
      "train loss:0.013539788229418299\n",
      "train loss:0.0094919867283348\n",
      "train loss:0.0011642194421468011\n",
      "train loss:0.06548521870197792\n",
      "train loss:0.007425603789852766\n",
      "train loss:0.0036832914319992623\n",
      "train loss:0.004270604164217243\n",
      "train loss:0.02934371082676815\n",
      "train loss:0.015608233640516502\n",
      "train loss:0.014908529207597672\n",
      "train loss:0.010056262423778207\n",
      "train loss:0.001497370851228547\n",
      "train loss:0.005954992158812246\n",
      "train loss:0.006439171312045051\n",
      "train loss:0.003519840862347957\n",
      "train loss:0.0037014717263129915\n",
      "train loss:0.012498301501789271\n",
      "train loss:0.002517106958979705\n",
      "train loss:0.005096733264945531\n",
      "train loss:0.002005481985875655\n",
      "train loss:0.002965582559257982\n",
      "train loss:0.004123410935966739\n",
      "train loss:0.0010063829225341878\n",
      "train loss:0.00962501711110364\n",
      "train loss:0.07423885957849156\n",
      "train loss:0.027872321647769004\n",
      "train loss:0.007171013307291695\n",
      "train loss:0.0030244878178945557\n",
      "train loss:0.022522160460850565\n",
      "train loss:0.006215287137139848\n",
      "train loss:0.012323406384806055\n",
      "train loss:0.005481276064164184\n",
      "train loss:0.00330424141101754\n",
      "train loss:0.012283089757011999\n",
      "train loss:0.008802341365688462\n",
      "train loss:0.002428009500925092\n",
      "train loss:0.0062635626745571075\n",
      "train loss:0.012682686217071325\n",
      "train loss:0.0026474127275120263\n",
      "train loss:0.0066911925442808004\n",
      "train loss:0.002712854935556918\n",
      "train loss:0.007997051595709513\n",
      "train loss:0.0012050165455793932\n",
      "train loss:0.009058812101639679\n",
      "train loss:0.0012240987681611732\n",
      "train loss:0.0013178901244922847\n",
      "train loss:0.002552502134289036\n",
      "train loss:0.021395759593436193\n",
      "train loss:0.0017752068082948632\n",
      "train loss:0.0069632877700680695\n",
      "train loss:0.004490997568087971\n",
      "train loss:0.002561380717946349\n",
      "train loss:0.005279861118151997\n",
      "train loss:0.0008996737712857525\n",
      "train loss:0.0006692556245594831\n",
      "train loss:0.008102692444301058\n",
      "train loss:0.003665381746074231\n",
      "train loss:0.007753149041162066\n",
      "train loss:0.003443212062542732\n",
      "train loss:0.0022847552692670824\n",
      "train loss:0.0029736176625479705\n",
      "train loss:0.0006478378628253248\n",
      "train loss:0.006640808348731983\n",
      "train loss:0.0084541988767187\n",
      "train loss:0.007683484149696309\n",
      "train loss:0.0016733402592133439\n",
      "train loss:0.0016641566993188417\n",
      "train loss:0.007179127946162112\n",
      "train loss:0.005400399580928914\n",
      "train loss:0.004875307761710785\n",
      "train loss:0.010239121594977767\n",
      "train loss:0.004595758921492887\n",
      "train loss:0.0024850264615087756\n",
      "train loss:0.001739998078495199\n",
      "train loss:0.006237780375614033\n",
      "train loss:0.0019399771511877367\n",
      "train loss:0.0006049150026150911\n",
      "train loss:0.002040644873947066\n",
      "train loss:0.006296175680637239\n",
      "train loss:0.001137538829925064\n",
      "train loss:0.02917304040301886\n",
      "train loss:0.0038786216932461194\n",
      "train loss:0.0012666029506778161\n",
      "train loss:0.0019537564717383737\n",
      "train loss:0.018839164047977638\n",
      "train loss:0.003341508204022701\n",
      "train loss:0.00799089721979495\n",
      "train loss:0.0028557825710863118\n",
      "train loss:0.005331086228229057\n",
      "train loss:0.007065774107043784\n",
      "train loss:0.001356528262685462\n",
      "train loss:0.0037857503664898194\n",
      "train loss:0.002496619724958361\n",
      "train loss:0.0030166592137870064\n",
      "train loss:0.005677677146733928\n",
      "train loss:0.010859231493942285\n",
      "train loss:0.011748497705098665\n",
      "train loss:0.021880340594441793\n",
      "train loss:0.001655424468169111\n",
      "train loss:0.0038011141610483066\n",
      "train loss:0.00966228347868797\n",
      "train loss:0.003658443465054534\n",
      "train loss:0.00027290875265151667\n",
      "train loss:0.002623026152263215\n",
      "train loss:0.0018455342693696544\n",
      "train loss:0.0041786834415268115\n",
      "train loss:0.008367742403409793\n",
      "train loss:0.0012541157617542325\n",
      "train loss:0.004323860644428858\n",
      "train loss:0.008267441075252416\n",
      "train loss:0.006703437495622463\n",
      "train loss:0.021640112385927193\n",
      "train loss:0.001104059694491174\n",
      "train loss:0.007654802461105959\n",
      "train loss:0.015320399837723642\n",
      "train loss:0.0068040680889257\n",
      "train loss:0.013591678153515023\n",
      "train loss:0.0060812198225011485\n",
      "train loss:0.0016672414840581517\n",
      "train loss:0.013347653835260489\n",
      "train loss:0.0026671653227899154\n",
      "train loss:0.01098870198315909\n",
      "train loss:0.004772984538099327\n",
      "train loss:0.014536632341196795\n",
      "train loss:0.001539369724914835\n",
      "train loss:0.0027780497288112244\n",
      "train loss:0.001609627262345985\n",
      "train loss:0.0005615409261877221\n",
      "train loss:0.0055591415352769305\n",
      "train loss:0.0027427412322853857\n",
      "train loss:0.017525860476908586\n",
      "train loss:0.006400141968564116\n",
      "train loss:0.001175199535748353\n",
      "train loss:0.015823524746222943\n",
      "train loss:0.010975475263892769\n",
      "train loss:0.002428339088670097\n",
      "train loss:0.01998549753284061\n",
      "train loss:0.005743360597521733\n",
      "train loss:0.007984209932970984\n",
      "train loss:0.00431940313643982\n",
      "train loss:0.016479758392918365\n",
      "train loss:0.013735596682145426\n",
      "train loss:0.003292537723635432\n",
      "train loss:0.0319379983905699\n",
      "train loss:0.009551364006133906\n",
      "train loss:0.002915423014583935\n",
      "train loss:0.0018939164845866864\n",
      "train loss:0.001419440262840235\n",
      "train loss:0.0012515742425374947\n",
      "train loss:0.009778522306248068\n",
      "train loss:0.004340762145269129\n",
      "train loss:0.003819607075129237\n",
      "train loss:0.007699273040947986\n",
      "train loss:0.043845500275764816\n",
      "train loss:0.0025968771629526783\n",
      "train loss:0.00036398790342301963\n",
      "train loss:0.0024370013610648003\n",
      "train loss:0.0033017489039221214\n",
      "train loss:0.010413314442845274\n",
      "train loss:0.0035385573548840695\n",
      "train loss:0.007317301094738461\n",
      "train loss:0.004027420282998228\n",
      "train loss:0.01882598605379936\n",
      "train loss:0.0028440984986162444\n",
      "train loss:0.007536373616905698\n",
      "train loss:0.0003320712630741155\n",
      "train loss:0.011760943168706538\n",
      "train loss:0.00824918632014163\n",
      "train loss:0.001234449004223423\n",
      "train loss:0.009924995560454848\n",
      "train loss:0.005004413138281614\n",
      "train loss:0.0033838008790159787\n",
      "train loss:0.004057981404493184\n",
      "train loss:0.002861177888806233\n",
      "train loss:0.014712510746398922\n",
      "train loss:0.0038730060689829383\n",
      "train loss:0.02549596299222553\n",
      "train loss:0.001569181332128178\n",
      "train loss:0.016642246035678643\n",
      "train loss:0.0013206231364605164\n",
      "train loss:0.004759314150571849\n",
      "train loss:0.0011218794354889172\n",
      "train loss:0.022281301889598874\n",
      "train loss:0.003676058037309628\n",
      "train loss:0.0017020078704313754\n",
      "train loss:0.0011949933934584142\n",
      "train loss:0.0009184807322430367\n",
      "train loss:0.00568358265344902\n",
      "train loss:0.0013031645719982924\n",
      "train loss:0.0010403124041456762\n",
      "train loss:0.003533195347456743\n",
      "train loss:0.0013089722820144388\n",
      "train loss:0.005532542586363408\n",
      "train loss:0.0034094968321822395\n",
      "train loss:0.005734125752941977\n",
      "train loss:0.002011758074116752\n",
      "train loss:0.00951801130921804\n",
      "train loss:0.0015352143544523267\n",
      "train loss:0.0053823800547185266\n",
      "train loss:0.007826255443939114\n",
      "train loss:0.0035575398860288165\n",
      "train loss:0.00521020795185909\n",
      "train loss:0.008776573598686093\n",
      "train loss:0.0006053230695096115\n",
      "train loss:0.0024732597377363514\n",
      "train loss:0.007791925061078464\n",
      "train loss:0.0010816921214731202\n",
      "train loss:0.0003636747683200262\n",
      "train loss:0.004360897221306474\n",
      "train loss:0.0070990382789825925\n",
      "train loss:0.0025751271214702297\n",
      "train loss:0.007564237816305342\n",
      "train loss:0.0009709081619757846\n",
      "train loss:0.0019157968814852275\n",
      "train loss:0.004578299329486611\n",
      "train loss:0.003805295891037546\n",
      "train loss:0.015867154579179997\n",
      "train loss:0.0004801967068480478\n",
      "train loss:0.003368841198891882\n",
      "train loss:0.003767055876119074\n",
      "train loss:0.002759766443423594\n",
      "train loss:0.0025019006555593902\n",
      "train loss:0.004274933752157554\n",
      "train loss:0.024518414441226447\n",
      "train loss:0.002959686898279939\n",
      "train loss:0.007123286595936132\n",
      "train loss:0.0008859668472545523\n",
      "train loss:0.0316926748353064\n",
      "train loss:0.004712048571018054\n",
      "train loss:0.002233445917571313\n",
      "train loss:0.0029085044619518844\n",
      "train loss:0.0014134664203410447\n",
      "train loss:0.00175502892910485\n",
      "train loss:0.009071727172923774\n",
      "train loss:0.0009408891100067331\n",
      "train loss:0.003103463346597714\n",
      "train loss:0.0018623391883486535\n",
      "train loss:0.009871706650648264\n",
      "train loss:0.008225636073462831\n",
      "train loss:0.00340279093740247\n",
      "train loss:0.0023464311048506463\n",
      "train loss:0.015720823502031932\n",
      "train loss:0.004782942447840508\n",
      "train loss:0.013172112280356162\n",
      "train loss:0.013205898668128538\n",
      "train loss:0.0067967108245356065\n",
      "train loss:0.0006273065100208172\n",
      "train loss:0.004085492414180594\n",
      "train loss:0.0021200225979971774\n",
      "train loss:0.003564534920910103\n",
      "train loss:0.0019189717286294155\n",
      "train loss:0.0016911383140099613\n",
      "train loss:0.005551205178255239\n",
      "train loss:0.0013239272707360466\n",
      "train loss:0.0030642848149984012\n",
      "train loss:0.00024256172404098155\n",
      "train loss:0.0009003904155944815\n",
      "train loss:0.01410870120929958\n",
      "train loss:0.0018443970133005547\n",
      "train loss:0.0036511852936234822\n",
      "train loss:0.004923377355500214\n",
      "train loss:0.010185440691240086\n",
      "train loss:0.0049486350175647585\n",
      "train loss:0.0019577410537961713\n",
      "train loss:0.0015773021511969472\n",
      "train loss:0.0024747210310068256\n",
      "train loss:0.008810507331465506\n",
      "train loss:0.0006664357988289339\n",
      "train loss:0.006755943249609065\n",
      "train loss:0.0009737385589440603\n",
      "train loss:0.0003345107898665017\n",
      "train loss:0.0004729758651935938\n",
      "train loss:0.0017743338876216302\n",
      "train loss:0.00748328934353047\n",
      "train loss:0.024354959863069906\n",
      "train loss:0.0012570639951816846\n",
      "train loss:0.0018648013786127687\n",
      "train loss:0.003836488603881749\n",
      "train loss:0.0073913604280684\n",
      "train loss:0.004228961852554966\n",
      "train loss:0.0054179455329366345\n",
      "train loss:0.0010216851449013416\n",
      "train loss:0.0020896526916060667\n",
      "train loss:0.018046915802266597\n",
      "train loss:0.003645717214446331\n",
      "train loss:0.0003197132253579615\n",
      "train loss:0.005827644000609173\n",
      "train loss:0.004293733022782998\n",
      "train loss:0.0019833155164014926\n",
      "train loss:0.0012895434329739395\n",
      "train loss:0.008637301321205321\n",
      "train loss:0.0009450090623448719\n",
      "train loss:0.013317471191530412\n",
      "train loss:0.008147423600817568\n",
      "train loss:0.0053583488366938945\n",
      "train loss:0.006195767841223888\n",
      "train loss:0.0048388176229068\n",
      "train loss:0.005441382232352647\n",
      "train loss:0.0007715583978279219\n",
      "train loss:0.003970150567402289\n",
      "train loss:0.001523249623455619\n",
      "train loss:0.04518081950903264\n",
      "train loss:0.0019397213136739643\n",
      "train loss:0.0015794301117164735\n",
      "train loss:0.011214663521529616\n",
      "train loss:0.01003951652624919\n",
      "train loss:0.000783003341281029\n",
      "train loss:0.016209297735553314\n",
      "train loss:0.0068286126189693215\n",
      "train loss:0.011809114998945092\n",
      "train loss:0.01424078615704139\n",
      "train loss:0.0008334101312216924\n",
      "train loss:0.004253030590446059\n",
      "train loss:0.05903059000606208\n",
      "train loss:0.00722554625187101\n",
      "train loss:0.008554123742913316\n",
      "train loss:0.0005854699720606714\n",
      "train loss:0.0038392238853644715\n",
      "train loss:0.01181309921317386\n",
      "train loss:0.0005241924881677749\n",
      "train loss:0.012466561982246988\n",
      "train loss:0.0038602185142353794\n",
      "train loss:0.0022674818666206255\n",
      "train loss:0.005568853206553056\n",
      "train loss:0.003102149726892676\n",
      "train loss:0.006907161137398886\n",
      "train loss:0.002347994682590068\n",
      "train loss:0.0012525564630161876\n",
      "train loss:0.004140766258057741\n",
      "train loss:0.009704875530981913\n",
      "train loss:0.0006273344863593064\n",
      "train loss:0.006008004424986565\n",
      "train loss:0.001730256099798807\n",
      "train loss:0.0073036333330500055\n",
      "train loss:0.003526956078737509\n",
      "train loss:0.005704026469582465\n",
      "train loss:0.002075317089389615\n",
      "train loss:0.002073550433320564\n",
      "train loss:0.017157263940408316\n",
      "train loss:0.002678906003029328\n",
      "train loss:0.0007080411611052462\n",
      "train loss:0.0006058245365969055\n",
      "train loss:0.00034171725272601824\n",
      "train loss:0.001065220588225109\n",
      "train loss:0.0050514843897387085\n",
      "train loss:0.022319802529835363\n",
      "train loss:0.011030006069234898\n",
      "train loss:0.0029204422653309357\n",
      "train loss:0.00234591793041566\n",
      "train loss:0.0011212182836270181\n",
      "train loss:0.029000970335590712\n",
      "train loss:0.0038509406762133377\n",
      "train loss:0.006560579876810481\n",
      "train loss:0.0015173164589045896\n",
      "train loss:0.01567057023406379\n",
      "train loss:0.003351779618550653\n",
      "train loss:0.00556558779108258\n",
      "train loss:0.0009274174458951047\n",
      "train loss:0.00108301741324418\n",
      "train loss:0.006307412539337897\n",
      "train loss:0.001090374391847404\n",
      "train loss:0.006182528238239219\n",
      "train loss:0.0004534332642139001\n",
      "train loss:0.004715172377134108\n",
      "train loss:0.00026789092835757697\n",
      "train loss:0.00991553407325664\n",
      "train loss:0.0065012385448370335\n",
      "train loss:0.004221424644941369\n",
      "train loss:0.007327960506545721\n",
      "train loss:0.001255540390275709\n",
      "train loss:0.0036843245855183652\n",
      "train loss:0.0003256822768967317\n",
      "train loss:0.002009443395531535\n",
      "train loss:0.003197549295289817\n",
      "train loss:0.011674862105636088\n",
      "train loss:0.0005655903159977409\n",
      "train loss:0.0018055144648227955\n",
      "train loss:0.0034560190696049063\n",
      "train loss:0.02962311782081999\n",
      "train loss:0.0030168629745247555\n",
      "train loss:0.004763464972274989\n",
      "train loss:0.00114048647795782\n",
      "train loss:0.0067779537662999035\n",
      "train loss:0.0833734529737852\n",
      "train loss:0.0024473724931493306\n",
      "train loss:0.007469781008198815\n",
      "train loss:0.03154214683891455\n",
      "train loss:0.007284875982953485\n",
      "train loss:0.002751502864560119\n",
      "train loss:0.011302579163039903\n",
      "train loss:0.014770911593018252\n",
      "train loss:0.0008948087836631272\n",
      "train loss:0.007517978430108025\n",
      "train loss:0.004870957929363371\n",
      "train loss:0.001730732260095855\n",
      "train loss:0.002632929915956402\n",
      "train loss:0.004389153922336511\n",
      "train loss:0.00329606548535939\n",
      "train loss:0.0004398818735607231\n",
      "train loss:0.001924058059019059\n",
      "train loss:0.018583116702037043\n",
      "train loss:0.002166899892012041\n",
      "train loss:0.00228612149036724\n",
      "train loss:0.007476508313246797\n",
      "train loss:0.015901959486724494\n",
      "train loss:0.004174147933275152\n",
      "train loss:0.009557615034876054\n",
      "train loss:0.005105480905154922\n",
      "train loss:0.001014813641015173\n",
      "train loss:0.003993179956845411\n",
      "train loss:0.0039639101745294215\n",
      "train loss:0.0017724849609774016\n",
      "train loss:0.009092103037562436\n",
      "train loss:0.0014264639878369737\n",
      "train loss:0.006429272876188893\n",
      "train loss:0.001979404601354671\n",
      "train loss:0.002062187311079517\n",
      "train loss:0.0014465451395766337\n",
      "train loss:0.0033215185928265312\n",
      "train loss:0.005261400085608925\n",
      "train loss:0.005839799195413603\n",
      "train loss:0.006719508580232545\n",
      "train loss:0.00268838876235457\n",
      "train loss:0.0010584155036335715\n",
      "train loss:0.006766420328434999\n",
      "train loss:0.0020751807735911705\n",
      "train loss:0.0011679187310319338\n",
      "train loss:0.006161429712115582\n",
      "train loss:0.005292622602606188\n",
      "train loss:0.01887683298554772\n",
      "train loss:0.0009815286211261385\n",
      "train loss:0.0038846283351478784\n",
      "train loss:0.002799254742733243\n",
      "train loss:0.00045444038486788655\n",
      "train loss:0.0030524448496407415\n",
      "train loss:0.018136425783896157\n",
      "train loss:0.0011687452306949587\n",
      "train loss:0.0005009996840666133\n",
      "train loss:0.0011880937357741173\n",
      "train loss:0.023392583511115093\n",
      "train loss:0.001796415352251424\n",
      "train loss:0.011213341622244442\n",
      "train loss:0.0021541244376869213\n",
      "train loss:0.005910475546554122\n",
      "train loss:0.0055342584574339234\n",
      "train loss:0.003332619479909581\n",
      "train loss:0.004782018711843476\n",
      "train loss:0.0031350408757959826\n",
      "train loss:0.0017155865634653857\n",
      "train loss:0.0009040960546670848\n",
      "train loss:0.018482664827898745\n",
      "train loss:0.004909599155220623\n",
      "train loss:0.002487391203585323\n",
      "train loss:0.003562463430436094\n",
      "train loss:0.0007245839885589886\n",
      "train loss:0.002620586800936507\n",
      "train loss:0.02164383553623651\n",
      "train loss:0.005557490064905706\n",
      "train loss:0.0018209657184630804\n",
      "train loss:0.03739471962832691\n",
      "train loss:0.002377503895486269\n",
      "train loss:0.00047534433002244546\n",
      "train loss:0.002647260343213172\n",
      "train loss:0.004636897598117118\n",
      "train loss:0.008618813590777862\n",
      "train loss:0.0036486121377479457\n",
      "train loss:0.012811270669891621\n",
      "train loss:0.015085839247098791\n",
      "train loss:0.006992305019614776\n",
      "train loss:0.004560844890611389\n",
      "train loss:0.0013588158690791899\n",
      "train loss:0.0063861908405854605\n",
      "train loss:0.004329616431672089\n",
      "train loss:0.0025131709755523684\n",
      "train loss:0.005615448037796907\n",
      "train loss:0.0008652065813295365\n",
      "train loss:0.0026500314155725175\n",
      "train loss:0.003821377660518121\n",
      "train loss:0.0051073284951390525\n",
      "train loss:0.0045546167860507185\n",
      "train loss:0.004305623235152494\n",
      "train loss:0.008605325557220688\n",
      "train loss:0.008818579236885546\n",
      "train loss:0.004083256383529417\n",
      "train loss:0.0062775298946050806\n",
      "train loss:0.0055325998991526204\n",
      "train loss:0.0026919383760103427\n",
      "train loss:0.0022026327116789466\n",
      "train loss:0.003853271958190468\n",
      "train loss:0.008167305756324648\n",
      "train loss:0.0009420325637473881\n",
      "train loss:0.00431234007128017\n",
      "train loss:0.002747761874445973\n",
      "train loss:0.005385744245694216\n",
      "train loss:0.0075323056837742685\n",
      "train loss:0.0013994909077464104\n",
      "train loss:0.0012405921440716017\n",
      "train loss:0.0038284360311979676\n",
      "train loss:0.004025148141329945\n",
      "train loss:0.0002927013677548135\n",
      "train loss:0.0041171693685724585\n",
      "train loss:0.002251210932464078\n",
      "train loss:0.001972682736824535\n",
      "train loss:0.0015450841218992927\n",
      "train loss:0.004678691604922494\n",
      "train loss:0.0016724583931804276\n",
      "train loss:0.004287279826564764\n",
      "train loss:0.0036417527651743014\n",
      "train loss:0.001542181114322056\n",
      "train loss:0.00130618787347397\n",
      "train loss:0.0004274793700944753\n",
      "train loss:0.002347850950985881\n",
      "train loss:0.007388622508378522\n",
      "train loss:0.0061686427435576445\n",
      "train loss:0.002251727990213959\n",
      "train loss:0.0010133634868253914\n",
      "train loss:0.00763626680784112\n",
      "train loss:0.00018942976371908672\n",
      "train loss:0.0019683136046500633\n",
      "train loss:0.01581633367582077\n",
      "train loss:0.00541840884948396\n",
      "train loss:0.005807765561629264\n",
      "train loss:0.002670769152512109\n",
      "train loss:0.0031570417618493126\n",
      "train loss:0.0006524517275678733\n",
      "train loss:0.001994106843548228\n",
      "train loss:0.0006197484358963648\n",
      "train loss:0.005788961933434221\n",
      "train loss:0.0011127082646024637\n",
      "train loss:0.0027387782048096136\n",
      "train loss:0.0015281315886274982\n",
      "train loss:0.001067763001308703\n",
      "train loss:0.004074301079578266\n",
      "train loss:0.002456430992280827\n",
      "train loss:0.003007874071771275\n",
      "train loss:0.0018778793522054645\n",
      "train loss:0.0009525438674941527\n",
      "train loss:0.009935913150360152\n",
      "train loss:0.0020807474477985023\n",
      "train loss:0.0017923917401387623\n",
      "train loss:0.001246580056646309\n",
      "train loss:0.02814663427064503\n",
      "train loss:0.0026479165260125826\n",
      "train loss:0.01123040453091926\n",
      "train loss:0.004738132359408951\n",
      "train loss:0.0009326548453175625\n",
      "train loss:0.0012673877471979255\n",
      "train loss:0.0045305325827745095\n",
      "train loss:0.006514652622664207\n",
      "train loss:0.0005996726617510193\n",
      "train loss:0.0011468688578560359\n",
      "train loss:0.010082181160539428\n",
      "train loss:0.0009012899413334781\n",
      "train loss:0.0009739705477587904\n",
      "train loss:0.001920229589458067\n",
      "train loss:0.0014743662652521078\n",
      "train loss:0.017236594334948258\n",
      "train loss:0.007246382740375257\n",
      "train loss:0.0019274226119573453\n",
      "train loss:0.007197156431686047\n",
      "train loss:0.001822313932076647\n",
      "train loss:0.0054118090763887175\n",
      "=== epoch:12, train acc:0.996, test acc:0.984 ===\n",
      "train loss:0.008818312410621752\n",
      "train loss:0.003543679091585755\n",
      "train loss:0.003016373420167915\n",
      "train loss:0.0035603265084782746\n",
      "train loss:0.0016810087779254215\n",
      "train loss:0.0012111064920883594\n",
      "train loss:0.002732238323771374\n",
      "train loss:0.007409769821931867\n",
      "train loss:0.007848267823002235\n",
      "train loss:0.000995408846621961\n",
      "train loss:0.022067532237387052\n",
      "train loss:0.006772197618825908\n",
      "train loss:0.0022504166688453744\n",
      "train loss:0.006900037549924543\n",
      "train loss:0.008050732523808866\n",
      "train loss:0.008905932844766984\n",
      "train loss:0.0005967428866995746\n",
      "train loss:0.0037953001237977994\n",
      "train loss:0.009851596591249937\n",
      "train loss:0.0012879435946928836\n",
      "train loss:0.0038955274942794626\n",
      "train loss:0.0026631787058292067\n",
      "train loss:0.0171925421345756\n",
      "train loss:0.02122797167057072\n",
      "train loss:0.0011112350499705326\n",
      "train loss:0.005430724610590596\n",
      "train loss:0.012586197817086782\n",
      "train loss:0.002288584522832586\n",
      "train loss:0.0014772081695775242\n",
      "train loss:0.011172614117115644\n",
      "train loss:0.0010641329739333523\n",
      "train loss:0.013467963239873005\n",
      "train loss:0.000936654339034783\n",
      "train loss:0.0028089337275082438\n",
      "train loss:0.001788070933442675\n",
      "train loss:0.0027283490135016435\n",
      "train loss:0.0013057615481661844\n",
      "train loss:0.00783949033596558\n",
      "train loss:0.00249863031550586\n",
      "train loss:0.0014435831293589793\n",
      "train loss:0.0036478409574773065\n",
      "train loss:0.0031487272470512146\n",
      "train loss:0.00253056470886285\n",
      "train loss:0.018331921832694664\n",
      "train loss:0.0027594872263490143\n",
      "train loss:0.004318979553138726\n",
      "train loss:0.0009190910943263297\n",
      "train loss:0.021377801421002852\n",
      "train loss:0.008277854016987848\n",
      "train loss:0.001644442232406119\n",
      "train loss:0.0002815571269191465\n",
      "train loss:0.0013568223280635296\n",
      "train loss:0.007088698081750076\n",
      "train loss:0.00271994829117349\n",
      "train loss:0.0010030494634889551\n",
      "train loss:0.0029110355481737037\n",
      "train loss:0.0023112332457161295\n",
      "train loss:0.0024586305979156695\n",
      "train loss:0.0021102983320135374\n",
      "train loss:0.0012506530117699765\n",
      "train loss:0.022605781481549455\n",
      "train loss:0.03412694330833409\n",
      "train loss:0.0032276247313089896\n",
      "train loss:0.0018953625160565257\n",
      "train loss:0.0024643503836765295\n",
      "train loss:0.002737356535318253\n",
      "train loss:0.00041339800645949205\n",
      "train loss:0.011607217189738108\n",
      "train loss:0.0010224593592092087\n",
      "train loss:0.003833622467908922\n",
      "train loss:0.030737389594707395\n",
      "train loss:0.014305295010898661\n",
      "train loss:0.0003586432969330044\n",
      "train loss:0.014241861649587679\n",
      "train loss:0.0022276689900063863\n",
      "train loss:0.004766743435734091\n",
      "train loss:0.011647901294636689\n",
      "train loss:0.008519989708211006\n",
      "train loss:0.008189209485994591\n",
      "train loss:0.0038407826857492972\n",
      "train loss:0.020538477641024927\n",
      "train loss:0.011467965377512315\n",
      "train loss:0.008733636656303154\n",
      "train loss:0.0016533243560957111\n",
      "train loss:0.0013654483436661947\n",
      "train loss:0.022413596337273446\n",
      "train loss:0.0027153262335092663\n",
      "train loss:0.03160568412854747\n",
      "train loss:0.0034699565701716752\n",
      "train loss:0.028939864700524103\n",
      "train loss:0.002605576682974471\n",
      "train loss:0.004486927839721076\n",
      "train loss:0.001999439638495567\n",
      "train loss:0.006309577971339722\n",
      "train loss:0.0077744639368915545\n",
      "train loss:0.0008571661374969093\n",
      "train loss:0.011904110250319648\n",
      "train loss:0.0013744356436790112\n",
      "train loss:0.00023917523731012446\n",
      "train loss:0.003335471070356572\n",
      "train loss:0.0010821179631256503\n",
      "train loss:0.006263729893355989\n",
      "train loss:0.011626082513686837\n",
      "train loss:0.008328771359290919\n",
      "train loss:0.0051232873140101606\n",
      "train loss:0.006323104567737545\n",
      "train loss:0.0007561708088541859\n",
      "train loss:0.005875766455952716\n",
      "train loss:0.02691160796990538\n",
      "train loss:0.01095161119235505\n",
      "train loss:0.0008500332081649232\n",
      "train loss:0.0023181749124345542\n",
      "train loss:0.015903336159718142\n",
      "train loss:0.0011963901504695657\n",
      "train loss:0.0064167860346936315\n",
      "train loss:0.005894721356266099\n",
      "train loss:0.0002852520145357494\n",
      "train loss:0.004057663363057319\n",
      "train loss:0.002805822630304157\n",
      "train loss:0.0020450612348410072\n",
      "train loss:0.013446555758905923\n",
      "train loss:0.019380268109420954\n",
      "train loss:0.0009828897380942598\n",
      "train loss:0.0009109658881516887\n",
      "train loss:0.00108019549382621\n",
      "train loss:0.0032527497788953064\n",
      "train loss:0.012625015201707874\n",
      "train loss:0.0015531529206481926\n",
      "train loss:0.00866055428588564\n",
      "train loss:0.0004341652874090588\n",
      "train loss:0.0047263059303766335\n",
      "train loss:0.0012264887136025882\n",
      "train loss:0.006008350624236377\n",
      "train loss:0.0004691906465502368\n",
      "train loss:0.003859652801814574\n",
      "train loss:0.002278101159707647\n",
      "train loss:0.005063422375426308\n",
      "train loss:0.00992307170167848\n",
      "train loss:0.0017914378233623244\n",
      "train loss:0.0040332817844607564\n",
      "train loss:0.0011746990299973848\n",
      "train loss:0.006289694354032971\n",
      "train loss:0.0024591311466551132\n",
      "train loss:0.0028503881241812522\n",
      "train loss:0.005088288611461235\n",
      "train loss:0.0017954945265953202\n",
      "train loss:0.014491305190947665\n",
      "train loss:0.0041100746180551765\n",
      "train loss:0.0042221595804492525\n",
      "train loss:0.0008702590482410794\n",
      "train loss:0.002185870303722301\n",
      "train loss:0.002376510751382213\n",
      "train loss:0.005380442196999548\n",
      "train loss:0.006189306553328155\n",
      "train loss:0.0006124260485614549\n",
      "train loss:0.008907401403371545\n",
      "train loss:0.010403356432626201\n",
      "train loss:0.0013147014662635403\n",
      "train loss:0.002316391348733874\n",
      "train loss:0.0018837825349912784\n",
      "train loss:0.0056538432774708705\n",
      "train loss:0.008673256845991259\n",
      "train loss:0.0006414096180487655\n",
      "train loss:0.008455712325140611\n",
      "train loss:0.0022841102284659703\n",
      "train loss:0.00026222386721130094\n",
      "train loss:0.003762168271203487\n",
      "train loss:0.0009945006133377473\n",
      "train loss:0.003457367420668803\n",
      "train loss:0.0033417856173262204\n",
      "train loss:0.0003366524151954025\n",
      "train loss:0.0019984837106834068\n",
      "train loss:0.005904049916408369\n",
      "train loss:0.009317297219517432\n",
      "train loss:0.0002419552955208676\n",
      "train loss:0.0006150560914116603\n",
      "train loss:0.00015440248790348448\n",
      "train loss:0.016752850337900752\n",
      "train loss:0.018582702731363674\n",
      "train loss:0.002764927443662986\n",
      "train loss:0.007108663359986811\n",
      "train loss:0.004264579315384557\n",
      "train loss:0.0013417641859960925\n",
      "train loss:0.0010214910325740776\n",
      "train loss:0.0019216017030078106\n",
      "train loss:0.002880341080254693\n",
      "train loss:0.000849985218529159\n",
      "train loss:0.03294818989813815\n",
      "train loss:0.0055101493235319475\n",
      "train loss:0.0014858314596032074\n",
      "train loss:0.0006037093188019599\n",
      "train loss:0.001082461307009163\n",
      "train loss:0.0021603234023567703\n",
      "train loss:0.0003883458270443447\n",
      "train loss:0.004291754259195628\n",
      "train loss:0.0013728308867929173\n",
      "train loss:0.0035391015861808693\n",
      "train loss:0.004628500369250869\n",
      "train loss:0.0057752620313007786\n",
      "train loss:0.0063816869139633895\n",
      "train loss:0.005491383744531662\n",
      "train loss:0.0036580991363595167\n",
      "train loss:0.006932201958766239\n",
      "train loss:0.0025681348444730532\n",
      "train loss:0.0026789622477667724\n",
      "train loss:0.0004379839476872149\n",
      "train loss:0.0010168277689593071\n",
      "train loss:0.0009538973149068559\n",
      "train loss:0.0008010865769088492\n",
      "train loss:0.0006950998547485619\n",
      "train loss:0.0030594678341706733\n",
      "train loss:0.0018766695278899458\n",
      "train loss:0.007137736991960142\n",
      "train loss:0.00389186370801041\n",
      "train loss:0.0037230565732441893\n",
      "train loss:0.00046828247561585214\n",
      "train loss:0.01723770571111534\n",
      "train loss:0.0069168070056035555\n",
      "train loss:0.0007770209488616204\n",
      "train loss:0.001664132212412694\n",
      "train loss:0.006369265446214705\n",
      "train loss:0.0015765656509565172\n",
      "train loss:0.0013448531685765176\n",
      "train loss:0.0018220651635632056\n",
      "train loss:0.0010034888968237046\n",
      "train loss:0.005329217972794431\n",
      "train loss:0.0027625580343132385\n",
      "train loss:0.0018113733149610234\n",
      "train loss:0.005206960307562419\n",
      "train loss:0.0039038179207419254\n",
      "train loss:0.0045764477557465105\n",
      "train loss:0.0019316602239078853\n",
      "train loss:0.0033889336628970114\n",
      "train loss:0.0018689461198021123\n",
      "train loss:0.0029473674133923765\n",
      "train loss:0.011382887461512704\n",
      "train loss:0.00945412136395637\n",
      "train loss:0.01856529596037908\n",
      "train loss:0.001158245111638844\n",
      "train loss:0.0011261442278007686\n",
      "train loss:0.015247368661108741\n",
      "train loss:0.03116653675062718\n",
      "train loss:0.00011868406391410067\n",
      "train loss:0.013342950106862317\n",
      "train loss:0.001817063896061208\n",
      "train loss:0.013925579043259867\n",
      "train loss:0.001502642430066611\n",
      "train loss:0.0015694988365794846\n",
      "train loss:0.0011786201682922525\n",
      "train loss:0.005518246890399358\n",
      "train loss:0.0018559719199953894\n",
      "train loss:0.009093623052520698\n",
      "train loss:0.002224121107058534\n",
      "train loss:0.03293447772210695\n",
      "train loss:0.00496994749912031\n",
      "train loss:0.003794976992956455\n",
      "train loss:0.0006501919937802638\n",
      "train loss:0.003684851840146148\n",
      "train loss:0.002745628783496208\n",
      "train loss:0.003407858881631886\n",
      "train loss:0.004777363422303398\n",
      "train loss:0.011788367076780349\n",
      "train loss:0.004290389952350752\n",
      "train loss:0.013110117614287754\n",
      "train loss:0.001289832016876279\n",
      "train loss:0.0009248755065041206\n",
      "train loss:0.0003993514719895351\n",
      "train loss:0.0005573939072602678\n",
      "train loss:0.0032168442250350253\n",
      "train loss:0.0025573252408907293\n",
      "train loss:0.00023189687438505995\n",
      "train loss:0.0016149442873919517\n",
      "train loss:0.0029580908499623314\n",
      "train loss:0.0019493284960617379\n",
      "train loss:0.011231447169327464\n",
      "train loss:0.06560065792009745\n",
      "train loss:0.0009002461748152376\n",
      "train loss:0.007098107428006162\n",
      "train loss:0.0040565118626687225\n",
      "train loss:0.0014179621343386112\n",
      "train loss:0.0030945091504864406\n",
      "train loss:0.003100043386293722\n",
      "train loss:0.0018542416622454741\n",
      "train loss:0.00987720962648012\n",
      "train loss:0.01293734610597365\n",
      "train loss:0.003467188335453121\n",
      "train loss:0.007140577617308799\n",
      "train loss:0.0012227535642563468\n",
      "train loss:0.009809558324021634\n",
      "train loss:0.0067564451907431225\n",
      "train loss:0.008362424679523878\n",
      "train loss:0.002777285118481087\n",
      "train loss:0.0026266636823468557\n",
      "train loss:0.009533963697395797\n",
      "train loss:0.005670873908349293\n",
      "train loss:0.0039414331218022396\n",
      "train loss:0.008665030964162525\n",
      "train loss:0.0031626108843258467\n",
      "train loss:0.0017953863153637626\n",
      "train loss:0.008008128925090901\n",
      "train loss:0.002778371873557776\n",
      "train loss:0.0028796197476570625\n",
      "train loss:0.00014832062016982957\n",
      "train loss:0.0031613282394291316\n",
      "train loss:0.0023327505027248715\n",
      "train loss:0.0018778257567003264\n",
      "train loss:0.021185469691978015\n",
      "train loss:0.0021486947642089527\n",
      "train loss:0.0027158710425448794\n",
      "train loss:0.0025237949375208333\n",
      "train loss:0.005039465599416121\n",
      "train loss:0.009915145601120157\n",
      "train loss:0.003045601668670806\n",
      "train loss:0.0021256812686161067\n",
      "train loss:0.004567741403070113\n",
      "train loss:0.0015997858051476034\n",
      "train loss:0.0072358076857901065\n",
      "train loss:0.008043142419639757\n",
      "train loss:0.00026922137569960553\n",
      "train loss:0.008094677875577605\n",
      "train loss:0.006328997470888672\n",
      "train loss:0.0010776131102127242\n",
      "train loss:0.0018828535695524984\n",
      "train loss:0.0009572658237121384\n",
      "train loss:0.0016113569424950583\n",
      "train loss:0.000328447542075418\n",
      "train loss:0.0047110159396710335\n",
      "train loss:0.0008607649550386765\n",
      "train loss:0.004248752193549171\n",
      "train loss:0.010540496965261228\n",
      "train loss:0.003355938306971582\n",
      "train loss:0.005169759641172889\n",
      "train loss:0.007194585132853097\n",
      "train loss:0.04370550293448087\n",
      "train loss:0.007187889209202426\n",
      "train loss:0.017790248598150794\n",
      "train loss:0.0014329843048500367\n",
      "train loss:0.00034844976721972094\n",
      "train loss:0.0013523561647999962\n",
      "train loss:0.017155389797708812\n",
      "train loss:0.00020171473019603943\n",
      "train loss:0.0039835199635905195\n",
      "train loss:0.005087779249507492\n",
      "train loss:0.005717806117930768\n",
      "train loss:0.006433214316900044\n",
      "train loss:0.006662395222434347\n",
      "train loss:0.004259814007340283\n",
      "train loss:0.008099686207445422\n",
      "train loss:0.0032724027702518738\n",
      "train loss:0.005393767310735217\n",
      "train loss:0.001518643603490905\n",
      "train loss:0.009739327343399452\n",
      "train loss:0.00285742583084825\n",
      "train loss:0.004542717931712595\n",
      "train loss:0.005141388274762706\n",
      "train loss:0.005491417734345199\n",
      "train loss:0.004910456859530547\n",
      "train loss:0.0011152729444381046\n",
      "train loss:0.0013237555978571844\n",
      "train loss:0.0037651047612334548\n",
      "train loss:0.0019747107356799603\n",
      "train loss:0.0004962307990711612\n",
      "train loss:0.009801720630688097\n",
      "train loss:0.001143739850977514\n",
      "train loss:0.0009716277080443754\n",
      "train loss:0.002462874695113674\n",
      "train loss:0.0026459463288792066\n",
      "train loss:0.0035145418883706326\n",
      "train loss:0.0006858386371056765\n",
      "train loss:0.0027597663437178123\n",
      "train loss:0.002503363233978778\n",
      "train loss:0.00246643195492487\n",
      "train loss:0.0012021328975658\n",
      "train loss:0.034855620303727484\n",
      "train loss:0.0001331019256130771\n",
      "train loss:0.000591660534204336\n",
      "train loss:0.002389200904694428\n",
      "train loss:0.00031810909843952467\n",
      "train loss:0.0031569063555598313\n",
      "train loss:0.0015913375890558312\n",
      "train loss:0.0008506536079989723\n",
      "train loss:0.0018121779624807619\n",
      "train loss:0.0035251655396332394\n",
      "train loss:0.0021311211095703633\n",
      "train loss:0.01516061287334041\n",
      "train loss:0.0005169394243247766\n",
      "train loss:0.0035447815840358486\n",
      "train loss:0.0021989208520523844\n",
      "train loss:0.0020247094224433797\n",
      "train loss:0.0026518649825748126\n",
      "train loss:0.006626293690927465\n",
      "train loss:0.0017626749220696315\n",
      "train loss:0.0034322292299005007\n",
      "train loss:0.0008572651693958726\n",
      "train loss:0.00058887640485273\n",
      "train loss:0.067082298974491\n",
      "train loss:0.0035929601251351827\n",
      "train loss:0.0017679131680061542\n",
      "train loss:0.003293783250898129\n",
      "train loss:0.005612266730463747\n",
      "train loss:0.00026007112143870307\n",
      "train loss:0.0013305049412307818\n",
      "train loss:0.00540959499807721\n",
      "train loss:0.01561409611501984\n",
      "train loss:0.001546435886887646\n",
      "train loss:0.0009255791308367535\n",
      "train loss:0.001588273882225397\n",
      "train loss:0.019035692492614085\n",
      "train loss:0.03742506089164733\n",
      "train loss:0.004818705024812081\n",
      "train loss:0.0023485089341363967\n",
      "train loss:0.00481209502153747\n",
      "train loss:0.007986346873363\n",
      "train loss:0.007074202088721138\n",
      "train loss:0.0021476022793979373\n",
      "train loss:0.006625677049973541\n",
      "train loss:0.0005754180609159678\n",
      "train loss:0.005330987819701611\n",
      "train loss:0.022308839282741185\n",
      "train loss:0.004473593129525253\n",
      "train loss:0.004844879601233554\n",
      "train loss:0.0036228432587444353\n",
      "train loss:0.000665722451701439\n",
      "train loss:0.001287749954031689\n",
      "train loss:0.0036687562115641685\n",
      "train loss:0.003305405040002996\n",
      "train loss:0.008450524344228254\n",
      "train loss:0.0007411763101497263\n",
      "train loss:0.0031022444575798795\n",
      "train loss:0.014612519705520449\n",
      "train loss:0.0024160432949795548\n",
      "train loss:0.005479840061846261\n",
      "train loss:0.003217254439871338\n",
      "train loss:0.0027807974170401977\n",
      "train loss:0.000781327992012284\n",
      "train loss:0.04443320195080624\n",
      "train loss:0.0021304493278197136\n",
      "train loss:0.010049139384601885\n",
      "train loss:0.004088555162839598\n",
      "train loss:0.0010734331251660808\n",
      "train loss:0.0004269589994467357\n",
      "train loss:0.00045328948194119816\n",
      "train loss:0.0003602789682142264\n",
      "train loss:0.003013633641245213\n",
      "train loss:0.0003233759187935742\n",
      "train loss:0.004259326176640526\n",
      "train loss:0.0019462518341609148\n",
      "train loss:0.00791843547839924\n",
      "train loss:0.004595919245894147\n",
      "train loss:0.0025656781053013515\n",
      "train loss:0.0031936238972177294\n",
      "train loss:0.005939684713312702\n",
      "train loss:0.002760124244526887\n",
      "train loss:0.005339660174735116\n",
      "train loss:0.002241998886876474\n",
      "train loss:0.16371846760596245\n",
      "train loss:0.013440391818432379\n",
      "train loss:0.0011118718895950952\n",
      "train loss:0.01674819192710342\n",
      "train loss:0.0043008553400881175\n",
      "train loss:0.020953850507276897\n",
      "train loss:0.0021227566466334035\n",
      "train loss:0.0014492523317914474\n",
      "train loss:0.004634892931489384\n",
      "train loss:0.011908272108595443\n",
      "train loss:0.0025362359141238413\n",
      "train loss:0.0006366311728026812\n",
      "train loss:0.009625944293326288\n",
      "train loss:0.0005369356442202227\n",
      "train loss:0.01751507827502817\n",
      "train loss:0.0072014330039566025\n",
      "train loss:0.009218118240479687\n",
      "train loss:0.002445091256795061\n",
      "train loss:0.0002459244015638957\n",
      "train loss:0.006800519992995758\n",
      "train loss:0.002511329506608752\n",
      "train loss:0.0007199049438953307\n",
      "train loss:0.0064759155361808875\n",
      "train loss:0.004812323963470003\n",
      "train loss:0.015186515468158236\n",
      "train loss:0.0005572620409289655\n",
      "train loss:0.0006956858307278576\n",
      "train loss:0.0013428997977988652\n",
      "train loss:0.0006165278625628666\n",
      "train loss:0.0029326479847062103\n",
      "train loss:0.0003927852001766385\n",
      "train loss:0.002681198179719915\n",
      "train loss:0.0019732672371817355\n",
      "train loss:0.020879646620001996\n",
      "train loss:0.0013966367224407025\n",
      "train loss:0.007626391333542553\n",
      "train loss:0.012951871198934864\n",
      "train loss:0.004258719336001795\n",
      "train loss:0.008782357605792896\n",
      "train loss:0.0009776749812883533\n",
      "train loss:0.006509734766556156\n",
      "train loss:0.00034374709920830367\n",
      "train loss:0.0017227375069699628\n",
      "train loss:0.0040661645343188476\n",
      "train loss:0.0024839322075924353\n",
      "train loss:0.009299467826683653\n",
      "train loss:0.0014751820145420605\n",
      "train loss:0.0017556399518647854\n",
      "train loss:0.001293406608260275\n",
      "train loss:0.005027598525107178\n",
      "train loss:0.006167313877499082\n",
      "train loss:0.0011877682553646442\n",
      "train loss:0.002000045521229553\n",
      "train loss:0.0002594228042049456\n",
      "train loss:0.004727453337114511\n",
      "train loss:0.003394563342131509\n",
      "train loss:0.005834514389591607\n",
      "train loss:0.00043048032585388035\n",
      "train loss:0.005637436266505271\n",
      "train loss:0.007879621158586264\n",
      "train loss:0.000886062841453174\n",
      "train loss:0.0015027445077049181\n",
      "train loss:0.0009661504093855014\n",
      "train loss:0.0031671187523385227\n",
      "train loss:0.0013629448429522053\n",
      "train loss:0.0013486693806949496\n",
      "train loss:0.0002639752932019043\n",
      "train loss:0.005685175334068609\n",
      "train loss:0.008296824850048746\n",
      "train loss:0.0008358125247156941\n",
      "train loss:0.0031981533170440763\n",
      "train loss:0.010939298484670401\n",
      "train loss:0.009432180611784517\n",
      "train loss:0.0008562006353502658\n",
      "train loss:0.004595921128489349\n",
      "train loss:0.005297504007233527\n",
      "train loss:0.0023821000398362366\n",
      "train loss:0.0004818632585173143\n",
      "train loss:0.0032155129924219678\n",
      "train loss:0.00517116468371073\n",
      "train loss:0.0015149713493666863\n",
      "train loss:0.0013900076368610237\n",
      "train loss:0.0011533428324336805\n",
      "train loss:0.0032873225826666047\n",
      "train loss:0.0009510850329795549\n",
      "train loss:0.004596122986196003\n",
      "train loss:0.0018602160970773761\n",
      "train loss:0.024049717248784307\n",
      "train loss:0.014300067086034597\n",
      "train loss:0.004526821372021721\n",
      "train loss:0.006249756877881957\n",
      "train loss:0.0004419804056747499\n",
      "train loss:0.0060134867102231\n",
      "train loss:0.004015944789864714\n",
      "train loss:0.00044365980399871835\n",
      "train loss:0.0014885746289027804\n",
      "train loss:0.0036364791198559214\n",
      "train loss:0.003456095597424746\n",
      "train loss:0.0014047128384472249\n",
      "train loss:0.0022995118044664095\n",
      "train loss:0.0013204738131578277\n",
      "train loss:0.00037441721844748866\n",
      "train loss:0.004234332700587937\n",
      "train loss:0.0020251906104318423\n",
      "train loss:0.003861244613615733\n",
      "train loss:0.0028277839579537428\n",
      "train loss:0.005082940903433737\n",
      "train loss:0.00782475672979736\n",
      "train loss:0.0008084261400876973\n",
      "train loss:0.0031053837903808044\n",
      "train loss:0.0023484424973279515\n",
      "train loss:0.020147390313703113\n",
      "train loss:0.004473842396964139\n",
      "train loss:0.0003423390232799916\n",
      "train loss:0.003956400028303347\n",
      "train loss:0.06271852823622366\n",
      "train loss:0.0010611975939986097\n",
      "train loss:0.0009966643793415239\n",
      "train loss:0.0027240691941818017\n",
      "train loss:0.0019576586489884885\n",
      "train loss:0.0002940674668108376\n",
      "train loss:0.003624135739079015\n",
      "train loss:0.001236463763087873\n",
      "train loss:0.027207251746083717\n",
      "train loss:0.011859558985541967\n",
      "train loss:0.0021342166280500117\n",
      "train loss:0.004763333762978512\n",
      "train loss:0.0002662300369699457\n",
      "train loss:0.0031048001454646455\n",
      "train loss:0.0011851847155904833\n",
      "train loss:0.014193264468311881\n",
      "train loss:0.000749870711056455\n",
      "train loss:0.002687418250720811\n",
      "train loss:0.007238719361085003\n",
      "train loss:0.03011889544803135\n",
      "train loss:0.004639377669252004\n",
      "train loss:0.0014457620610136318\n",
      "train loss:0.00262057702108352\n",
      "train loss:0.004180301097365924\n",
      "train loss:0.0024988988010483317\n",
      "train loss:0.0012265240309183689\n",
      "train loss:0.0058460459317470174\n",
      "train loss:0.001009725618958073\n",
      "train loss:0.007489535277403956\n",
      "train loss:0.005990210801221734\n",
      "=== epoch:13, train acc:0.995, test acc:0.99 ===\n",
      "train loss:0.013681711497907248\n",
      "train loss:0.0007625232371652607\n",
      "train loss:0.005670124550892119\n",
      "train loss:0.005734027511039182\n",
      "train loss:0.004487553093972083\n",
      "train loss:0.00942985449322161\n",
      "train loss:0.006582517500358003\n",
      "train loss:0.043643818339690804\n",
      "train loss:0.04525389899646202\n",
      "train loss:0.006973378498562829\n",
      "train loss:0.015116026738687107\n",
      "train loss:0.0016283326277301737\n",
      "train loss:0.037088563732666946\n",
      "train loss:0.015267358258411426\n",
      "train loss:0.0013438286086578958\n",
      "train loss:0.0016658188204946725\n",
      "train loss:0.003331692572112524\n",
      "train loss:0.01258127180469621\n",
      "train loss:0.006110652547010202\n",
      "train loss:0.00387639402852332\n",
      "train loss:0.006156041121148013\n",
      "train loss:0.0043384892357166424\n",
      "train loss:0.0003885757206056609\n",
      "train loss:0.0007200666214664199\n",
      "train loss:0.0016040930359122432\n",
      "train loss:0.006185561740198886\n",
      "train loss:0.0008001777269806418\n",
      "train loss:0.010630826742629234\n",
      "train loss:0.0036030651835259352\n",
      "train loss:0.0030154710386517784\n",
      "train loss:0.003574013131402757\n",
      "train loss:0.00807357982801998\n",
      "train loss:0.004723165915023938\n",
      "train loss:0.0019857042109575926\n",
      "train loss:0.0042233787900099575\n",
      "train loss:0.001080124643311832\n",
      "train loss:0.008812692762329604\n",
      "train loss:0.004216518125488613\n",
      "train loss:0.003225717922908282\n",
      "train loss:0.003571301926114561\n",
      "train loss:0.0022694779749767883\n",
      "train loss:0.04358016655506007\n",
      "train loss:0.007336520959581959\n",
      "train loss:0.000667398077870492\n",
      "train loss:0.0033858363309835554\n",
      "train loss:0.004263826416977545\n",
      "train loss:0.0007433297994748403\n",
      "train loss:0.014576506998045238\n",
      "train loss:0.009155244736929387\n",
      "train loss:0.014659708162954413\n",
      "train loss:0.0012235890013643684\n",
      "train loss:0.0013259998889202267\n",
      "train loss:0.0038571230413401033\n",
      "train loss:0.004683817909672848\n",
      "train loss:0.011925797354600807\n",
      "train loss:0.058792286603763386\n",
      "train loss:0.0037533668112888436\n",
      "train loss:0.00045820828890478566\n",
      "train loss:0.003160091662128533\n",
      "train loss:0.0031901847590978176\n",
      "train loss:0.005444641822024494\n",
      "train loss:0.0023319875148637066\n",
      "train loss:0.009576970753628112\n",
      "train loss:0.015771851691748485\n",
      "train loss:0.0026085594906565005\n",
      "train loss:0.0005158782815309404\n",
      "train loss:0.002680884106375815\n",
      "train loss:0.0008953417138892441\n",
      "train loss:0.00878654318710957\n",
      "train loss:0.017511813776087567\n",
      "train loss:0.009583728284911231\n",
      "train loss:0.0017371811215176943\n",
      "train loss:0.011665451863754258\n",
      "train loss:0.006736376334275678\n",
      "train loss:0.004736039445695558\n",
      "train loss:0.0023210099157782134\n",
      "train loss:0.007993697550798855\n",
      "train loss:0.0005770695106298499\n",
      "train loss:0.0011286886218876341\n",
      "train loss:0.003384541096433877\n",
      "train loss:0.008681037556190013\n",
      "train loss:0.0028295889398581454\n",
      "train loss:0.0026499159422224113\n",
      "train loss:0.004631400293540206\n",
      "train loss:0.0030251403716128732\n",
      "train loss:0.005542476041408596\n",
      "train loss:0.044262719521800296\n",
      "train loss:0.003920020563545832\n",
      "train loss:0.0012373831874003807\n",
      "train loss:0.014396603811982127\n",
      "train loss:0.006592081808584721\n",
      "train loss:0.005487811457442247\n",
      "train loss:0.010836800367480756\n",
      "train loss:0.0015120077147652558\n",
      "train loss:0.0010814515986145275\n",
      "train loss:0.0046662132375156155\n",
      "train loss:0.0010735778377468142\n",
      "train loss:0.0017344277709334382\n",
      "train loss:0.0002115167193847905\n",
      "train loss:0.0011870269154298102\n",
      "train loss:0.0015005422401267734\n",
      "train loss:0.0003142186698861543\n",
      "train loss:0.0019212960040772963\n",
      "train loss:0.006451891492073809\n",
      "train loss:0.008436474431305886\n",
      "train loss:0.012734195549941709\n",
      "train loss:0.0028009607614857697\n",
      "train loss:0.00010358555072925295\n",
      "train loss:0.005077742338816813\n",
      "train loss:0.0019628827776598397\n",
      "train loss:0.00749768524800865\n",
      "train loss:0.0033880922199451967\n",
      "train loss:0.00045103152589164475\n",
      "train loss:0.0027869646138181164\n",
      "train loss:0.013859864859020531\n",
      "train loss:0.004151844099463613\n",
      "train loss:0.0011397039956233266\n",
      "train loss:0.001038463872554704\n",
      "train loss:0.00014549287820909957\n",
      "train loss:0.0009709785610536468\n",
      "train loss:0.00853796057167233\n",
      "train loss:0.002643580833679704\n",
      "train loss:0.0010090111313094683\n",
      "train loss:0.0014380548706645953\n",
      "train loss:0.0010840566452599975\n",
      "train loss:0.012964852187766576\n",
      "train loss:0.024757187782388876\n",
      "train loss:0.004696135651219323\n",
      "train loss:0.0037629132416616778\n",
      "train loss:0.005979313249660657\n",
      "train loss:0.005777490741506569\n",
      "train loss:0.0003229309649137769\n",
      "train loss:0.007810110685135969\n",
      "train loss:0.0015598370790136779\n",
      "train loss:0.004712524158457109\n",
      "train loss:0.0009614206137662972\n",
      "train loss:0.0029070407904664385\n",
      "train loss:0.010908462693596988\n",
      "train loss:0.0023430942407412256\n",
      "train loss:0.0025091495229778555\n",
      "train loss:0.013288252263151423\n",
      "train loss:0.00028312076961764304\n",
      "train loss:0.0037342044650596953\n",
      "train loss:0.010911676667094855\n",
      "train loss:0.0045273809166679726\n",
      "train loss:0.0025620742757586027\n",
      "train loss:0.0026499607075052257\n",
      "train loss:0.006203048331673824\n",
      "train loss:0.0035714393874389833\n",
      "train loss:0.001394617901127376\n",
      "train loss:0.0051146874863818\n",
      "train loss:0.00530229972915616\n",
      "train loss:0.0023396288951915995\n",
      "train loss:0.003763532969302071\n",
      "train loss:0.0005167301256146394\n",
      "train loss:0.0006093780897215505\n",
      "train loss:0.016221688860784157\n",
      "train loss:0.0008080256065992727\n",
      "train loss:0.002136968254336595\n",
      "train loss:0.0005887413702949921\n",
      "train loss:0.0035301976767691478\n",
      "train loss:0.00198363615612939\n",
      "train loss:0.017502868225216697\n",
      "train loss:0.00132433096867654\n",
      "train loss:0.0013720729974491524\n",
      "train loss:0.009143737959858372\n",
      "train loss:0.0017238888485838488\n",
      "train loss:0.0028797404175032355\n",
      "train loss:0.006984823865707482\n",
      "train loss:0.0040558779457826755\n",
      "train loss:0.0004336531524562929\n",
      "train loss:0.011352778672675064\n",
      "train loss:0.0002496196004630453\n",
      "train loss:0.0013859752164639089\n",
      "train loss:0.0045248176009020195\n",
      "train loss:0.0030145763157737888\n",
      "train loss:0.007193791033946248\n",
      "train loss:0.002189250245931248\n",
      "train loss:0.0010787603835889644\n",
      "train loss:0.001712853526636583\n",
      "train loss:0.021870920816351894\n",
      "train loss:0.0006738031095983514\n",
      "train loss:0.0008681460605093294\n",
      "train loss:0.001965288481335146\n",
      "train loss:0.0016496421327904884\n",
      "train loss:0.005880132349325752\n",
      "train loss:0.0884090299023124\n",
      "train loss:0.0038877501914092566\n",
      "train loss:0.005553041142207829\n",
      "train loss:0.001401384444356189\n",
      "train loss:0.0006248177510696825\n",
      "train loss:0.0034600067137991086\n",
      "train loss:0.008408942479769801\n",
      "train loss:0.006170580763596136\n",
      "train loss:0.0023861249806669356\n",
      "train loss:0.0011410676860298047\n",
      "train loss:0.010142434766959358\n",
      "train loss:0.010412558324440865\n",
      "train loss:0.01751901074448919\n",
      "train loss:0.0023027150515761933\n",
      "train loss:0.003681963424781656\n",
      "train loss:0.0017681561512337017\n",
      "train loss:0.04487551422522969\n",
      "train loss:0.0003597592299781102\n",
      "train loss:0.016469944633442472\n",
      "train loss:0.010133385650680388\n",
      "train loss:0.0044705876484993105\n",
      "train loss:0.0008224758597828989\n",
      "train loss:0.000646351030271812\n",
      "train loss:0.0002886690469663294\n",
      "train loss:0.0013244713786601677\n",
      "train loss:0.003972542758323511\n",
      "train loss:0.009854837054804994\n",
      "train loss:0.0046699567684578445\n",
      "train loss:0.00040219073071694797\n",
      "train loss:0.00966036935426403\n",
      "train loss:0.004847419970172033\n",
      "train loss:0.007966708042088783\n",
      "train loss:0.0066131291876551\n",
      "train loss:0.0037027305233465176\n",
      "train loss:0.011875046864700603\n",
      "train loss:0.002945798948362777\n",
      "train loss:0.0016273009095755322\n",
      "train loss:0.0029565417333101774\n",
      "train loss:0.0032966778311749045\n",
      "train loss:0.0016466075432146612\n",
      "train loss:0.003483735998950698\n",
      "train loss:0.006517640733175411\n",
      "train loss:0.01080358235537751\n",
      "train loss:0.0007393734969808599\n",
      "train loss:0.0005668535929182368\n",
      "train loss:0.010099624431347987\n",
      "train loss:0.00033634153574431933\n",
      "train loss:0.009546040874229736\n",
      "train loss:0.0008499460300665833\n",
      "train loss:0.016728443081664535\n",
      "train loss:0.003226494346183542\n",
      "train loss:0.000578955160417688\n",
      "train loss:0.0005400066215450919\n",
      "train loss:0.010094332685731839\n",
      "train loss:0.06804752054092308\n",
      "train loss:0.004804875650027463\n",
      "train loss:0.00238564629693266\n",
      "train loss:5.399341738726804e-05\n",
      "train loss:0.0007466080409800226\n",
      "train loss:0.003532160091866967\n",
      "train loss:0.00015517920176948662\n",
      "train loss:0.00069849398484463\n",
      "train loss:0.0046145982509673\n",
      "train loss:0.0008584567641046784\n",
      "train loss:0.0037020742860798955\n",
      "train loss:0.012937425495388444\n",
      "train loss:0.0003467982922043947\n",
      "train loss:0.0038601739001507714\n",
      "train loss:0.003334564569081488\n",
      "train loss:0.002979136617211709\n",
      "train loss:0.01634301180138465\n",
      "train loss:0.005410703516476526\n",
      "train loss:0.03041073381711181\n",
      "train loss:0.008102991638130528\n",
      "train loss:0.003505287630503078\n",
      "train loss:0.0029462742529005525\n",
      "train loss:0.0010916521763369134\n",
      "train loss:0.01207949012350441\n",
      "train loss:0.005131063027714215\n",
      "train loss:0.0022466048573009036\n",
      "train loss:0.0011069604228639259\n",
      "train loss:0.0036066392498965375\n",
      "train loss:0.00077354955932553\n",
      "train loss:0.004743692797249384\n",
      "train loss:0.02022097261277928\n",
      "train loss:0.0009208032894708668\n",
      "train loss:0.006310703133208268\n",
      "train loss:0.005401928916672751\n",
      "train loss:0.00036982452834036174\n",
      "train loss:0.0022248082906690695\n",
      "train loss:0.0011650980324472283\n",
      "train loss:0.010712011919380635\n",
      "train loss:0.0024644458404216125\n",
      "train loss:0.0006827912128341868\n",
      "train loss:0.0021073330378728484\n",
      "train loss:0.0007302375427008363\n",
      "train loss:0.0025500244760594958\n",
      "train loss:0.001328773583838383\n",
      "train loss:0.02183542693084887\n",
      "train loss:0.005591285358063961\n",
      "train loss:0.004181427438309764\n",
      "train loss:0.00442817145371425\n",
      "train loss:0.003968041824613822\n",
      "train loss:0.0028643272701250506\n",
      "train loss:0.05981437746171706\n",
      "train loss:0.0034139140971297687\n",
      "train loss:0.019995535975876035\n",
      "train loss:0.0029951043481570485\n",
      "train loss:0.000589304542696747\n",
      "train loss:0.0011483121766483718\n",
      "train loss:0.013722600207730928\n",
      "train loss:0.006167472606224628\n",
      "train loss:0.021646765954969522\n",
      "train loss:0.000755103314474153\n",
      "train loss:0.0021185840028485384\n",
      "train loss:0.00728506096491237\n",
      "train loss:0.0009859323605048494\n",
      "train loss:0.004435099892407451\n",
      "train loss:0.020809879179816197\n",
      "train loss:0.0009243930024769627\n",
      "train loss:0.005725390718766372\n",
      "train loss:0.0007861285764016194\n",
      "train loss:0.005834093205596247\n",
      "train loss:0.0005455643767122846\n",
      "train loss:0.0025347298796912017\n",
      "train loss:0.0031672475967198877\n",
      "train loss:0.024397374926210823\n",
      "train loss:0.02058551296392027\n",
      "train loss:0.004115840724184794\n",
      "train loss:0.002346183132492437\n",
      "train loss:0.013993831082940254\n",
      "train loss:0.006871880783824224\n",
      "train loss:0.05378323263276405\n",
      "train loss:0.0006089100478255432\n",
      "train loss:0.0019244420768869542\n",
      "train loss:0.0009364765864643797\n",
      "train loss:0.003527511952886767\n",
      "train loss:0.0009831307731567106\n",
      "train loss:0.000986684736547604\n",
      "train loss:0.0011150996908537896\n",
      "train loss:0.0008881067572787066\n",
      "train loss:0.0016016373088087476\n",
      "train loss:0.004332768152819396\n",
      "train loss:0.002961640059611321\n",
      "train loss:0.006005997102364573\n",
      "train loss:0.0008814423049410045\n",
      "train loss:0.0008267022743593211\n",
      "train loss:0.003633832735879318\n",
      "train loss:0.0007992770776007472\n",
      "train loss:0.005943241474795103\n",
      "train loss:0.002008911863448849\n",
      "train loss:0.0017855612483706983\n",
      "train loss:0.00805871013477219\n",
      "train loss:0.017306715572110394\n",
      "train loss:0.00038356207926788376\n",
      "train loss:0.0017413360310489221\n",
      "train loss:0.021829298944907177\n",
      "train loss:0.0010054227491453189\n",
      "train loss:0.002303182999488608\n",
      "train loss:0.0019648595865561728\n",
      "train loss:0.002575042722800425\n",
      "train loss:0.0006882675874490405\n",
      "train loss:0.00029293498338259753\n",
      "train loss:0.007527486633665141\n",
      "train loss:0.04360994675643684\n",
      "train loss:0.002330955613478802\n",
      "train loss:0.0010323199784418033\n",
      "train loss:0.003291228943815699\n",
      "train loss:0.0005086879539341022\n",
      "train loss:0.0027409113888966514\n",
      "train loss:0.007938608331440295\n",
      "train loss:0.000853693318310401\n",
      "train loss:0.0007631609075210407\n",
      "train loss:0.00095473018887261\n",
      "train loss:0.0323931732064681\n",
      "train loss:0.03893749948525291\n",
      "train loss:0.012590834228523808\n",
      "train loss:0.0008914192607160317\n",
      "train loss:0.0013568610116612544\n",
      "train loss:0.003784855935034274\n",
      "train loss:0.003305861028230272\n",
      "train loss:0.0013048135254467672\n",
      "train loss:0.005173183242720668\n",
      "train loss:0.004415701854465449\n",
      "train loss:0.009005462071129232\n",
      "train loss:0.001167917178743553\n",
      "train loss:0.0071177075187565005\n",
      "train loss:0.011786386150379982\n",
      "train loss:0.009699106406179577\n",
      "train loss:0.0014042566462208247\n",
      "train loss:0.0006475659714943727\n",
      "train loss:0.00024045186337130723\n",
      "train loss:0.004604123309833516\n",
      "train loss:0.0011287562557684544\n",
      "train loss:0.0061048458010856295\n",
      "train loss:0.002009812847552665\n",
      "train loss:0.001661720679939057\n",
      "train loss:0.000468047425598418\n",
      "train loss:0.0030421541726218565\n",
      "train loss:0.0008711886184151319\n",
      "train loss:0.004459215113470247\n",
      "train loss:0.003215134249265422\n",
      "train loss:0.0006160291742145456\n",
      "train loss:0.0035464029884598957\n",
      "train loss:0.002059662660538157\n",
      "train loss:0.0017708835932689693\n",
      "train loss:0.002794829488083255\n",
      "train loss:0.004968849593393906\n",
      "train loss:0.015899314947440894\n",
      "train loss:0.002349839161884537\n",
      "train loss:0.00354843683794906\n",
      "train loss:0.0010612219895207795\n",
      "train loss:0.0015894486400048536\n",
      "train loss:0.0019233394652566828\n",
      "train loss:0.0031528640385326207\n",
      "train loss:0.004857119790762109\n",
      "train loss:0.0006586980113913937\n",
      "train loss:0.008851984713897554\n",
      "train loss:0.004561045212105062\n",
      "train loss:0.006678201118747011\n",
      "train loss:0.010303710376809504\n",
      "train loss:0.0029013101292901045\n",
      "train loss:0.014126122502116105\n",
      "train loss:0.006033038348247442\n",
      "train loss:0.001721521436682201\n",
      "train loss:0.004462860382988306\n",
      "train loss:0.00201106259837181\n",
      "train loss:0.0015618552472063186\n",
      "train loss:0.0035803128146479507\n",
      "train loss:0.0049464376677323466\n",
      "train loss:0.005637941537822243\n",
      "train loss:0.009127109011855799\n",
      "train loss:0.001574213974382826\n",
      "train loss:0.000666886063642342\n",
      "train loss:0.011490202694096978\n",
      "train loss:0.003055925377114266\n",
      "train loss:0.0003240383392092896\n",
      "train loss:0.0014801645112663922\n",
      "train loss:0.0032100208274851542\n",
      "train loss:0.005817559993306877\n",
      "train loss:0.0013007727569080065\n",
      "train loss:0.0010145359983628618\n",
      "train loss:0.00034632061125184315\n",
      "train loss:0.014094514687488145\n",
      "train loss:0.0022835472171502187\n",
      "train loss:0.0047470154082771415\n",
      "train loss:0.003449014653323176\n",
      "train loss:0.0010986564099927825\n",
      "train loss:0.024417585177931526\n",
      "train loss:0.0006266390963919388\n",
      "train loss:0.0013768199049119753\n",
      "train loss:0.00024170947351857263\n",
      "train loss:0.001199426054678453\n",
      "train loss:0.016044627443015946\n",
      "train loss:0.001134597516916331\n",
      "train loss:0.001967797780273544\n",
      "train loss:0.003533118701829103\n",
      "train loss:0.0011172238552304577\n",
      "train loss:0.0003152764855560775\n",
      "train loss:0.010505435992844201\n",
      "train loss:0.006301639690587859\n",
      "train loss:0.0017317598318952149\n",
      "train loss:0.011801021415055363\n",
      "train loss:0.0008747254844875493\n",
      "train loss:0.0068392322050772406\n",
      "train loss:0.0014105191853324788\n",
      "train loss:0.0027135506489511262\n",
      "train loss:0.0015221757580430437\n",
      "train loss:0.001456761437609664\n",
      "train loss:0.00740666665949738\n",
      "train loss:0.013404065879386237\n",
      "train loss:0.011628673446638451\n",
      "train loss:0.003272769340593628\n",
      "train loss:0.0006148928590040023\n",
      "train loss:0.00036584627484393344\n",
      "train loss:0.005051649130763491\n",
      "train loss:0.005912648201015875\n",
      "train loss:0.0031273004273730943\n",
      "train loss:0.00038288411011959747\n",
      "train loss:0.0068772434917157696\n",
      "train loss:0.001394864084199877\n",
      "train loss:0.00024364659974095075\n",
      "train loss:0.001250685237030458\n",
      "train loss:0.003276546266578576\n",
      "train loss:0.0007800984521486073\n",
      "train loss:0.0013954900848945817\n",
      "train loss:0.0016863512662844006\n",
      "train loss:0.002564435533303603\n",
      "train loss:0.0008027269501907072\n",
      "train loss:0.0026284102033222443\n",
      "train loss:0.0008074818748613934\n",
      "train loss:0.0005440809667955246\n",
      "train loss:0.0053045069124663365\n",
      "train loss:0.007413849950535777\n",
      "train loss:0.0052971771456850805\n",
      "train loss:0.00026088130565174255\n",
      "train loss:0.0004310967057369532\n",
      "train loss:0.016243242777513687\n",
      "train loss:0.00011549901681273308\n",
      "train loss:0.00604894701845082\n",
      "train loss:0.008266067151186005\n",
      "train loss:0.0030878817095963086\n",
      "train loss:0.005868026531829869\n",
      "train loss:0.007993396190104324\n",
      "train loss:0.0031687909375380597\n",
      "train loss:0.010710885804473564\n",
      "train loss:0.0062353619081372234\n",
      "train loss:0.008464449250418065\n",
      "train loss:0.0039030981416945266\n",
      "train loss:0.004056153522924233\n",
      "train loss:0.00046220621703039496\n",
      "train loss:0.004094704595579869\n",
      "train loss:0.005666412187243927\n",
      "train loss:0.0015840420521670813\n",
      "train loss:0.0013044244085583287\n",
      "train loss:0.003262200020708576\n",
      "train loss:0.0004362589055909759\n",
      "train loss:0.001394820554827289\n",
      "train loss:0.002165180943778318\n",
      "train loss:0.004610767077574979\n",
      "train loss:0.01361286440172763\n",
      "train loss:0.0020101148428310497\n",
      "train loss:0.0007597504684073646\n",
      "train loss:0.00112376909184215\n",
      "train loss:0.004224492899112354\n",
      "train loss:0.003324924876670107\n",
      "train loss:0.0013798790506468742\n",
      "train loss:0.0010784603931998905\n",
      "train loss:0.002645664658799929\n",
      "train loss:0.0011265440531338434\n",
      "train loss:0.003281217217818282\n",
      "train loss:0.0006977868186272228\n",
      "train loss:0.00276530724555281\n",
      "train loss:0.0010227270559062264\n",
      "train loss:0.00839292522201635\n",
      "train loss:0.0025642103608175325\n",
      "train loss:0.003118517505856519\n",
      "train loss:0.001065158300315959\n",
      "train loss:0.00654847790806638\n",
      "train loss:0.0004945864533768466\n",
      "train loss:0.0041579678906996705\n",
      "train loss:0.006565630198332226\n",
      "train loss:0.0011325086910422386\n",
      "train loss:0.0013113443821380077\n",
      "train loss:0.002204557779420596\n",
      "train loss:0.0052067275119336235\n",
      "train loss:0.00020086133275705876\n",
      "train loss:0.0037869229388382917\n",
      "train loss:0.012243900121068674\n",
      "train loss:0.006769216710266868\n",
      "train loss:0.0024502468919619337\n",
      "train loss:0.0012874994504768894\n",
      "train loss:0.0007228696603927885\n",
      "train loss:0.0004013438948655922\n",
      "train loss:0.0003441019643075523\n",
      "train loss:0.0003691900377930081\n",
      "train loss:0.0012329960704400838\n",
      "train loss:0.0020622203983159306\n",
      "train loss:0.00678597260772427\n",
      "train loss:0.040594994808611125\n",
      "train loss:0.0031376188936843973\n",
      "train loss:0.012687246377294088\n",
      "train loss:0.0005303939371543385\n",
      "train loss:0.006244230542170323\n",
      "train loss:0.0021596605354454643\n",
      "train loss:0.0006398294591747284\n",
      "train loss:0.015664393374239697\n",
      "train loss:0.000285599437171833\n",
      "train loss:0.0028583552248252837\n",
      "train loss:0.0010076962746168197\n",
      "train loss:0.01728964353640072\n",
      "train loss:0.03143757015204406\n",
      "train loss:0.004844554764856437\n",
      "train loss:0.00426844315550105\n",
      "train loss:0.0031570670673532904\n",
      "train loss:0.006881036554940843\n",
      "train loss:0.008651075290323239\n",
      "train loss:0.009048268351186481\n",
      "train loss:0.010566320330363659\n",
      "train loss:0.009800608609477643\n",
      "train loss:0.0013092987155242564\n",
      "train loss:0.0011040926067882905\n",
      "train loss:0.02527374601031321\n",
      "train loss:0.008227798505817535\n",
      "train loss:0.0014355136561652736\n",
      "train loss:0.005884328607961674\n",
      "train loss:0.0011088853632189896\n",
      "train loss:0.0021827668274037345\n",
      "train loss:0.006614831953121475\n",
      "train loss:0.006216600174903081\n",
      "train loss:0.010875145165166395\n",
      "train loss:0.003353451092578297\n",
      "train loss:0.0034632283885715908\n",
      "train loss:0.0006359255163518716\n",
      "train loss:0.0022816875498972216\n",
      "train loss:0.008883529371615816\n",
      "train loss:0.03621707454329731\n",
      "train loss:0.007671775238797891\n",
      "train loss:0.0024297780871909273\n",
      "train loss:0.0037431746555757557\n",
      "train loss:0.0035815498159199543\n",
      "train loss:0.0024362296689844137\n",
      "train loss:0.009764382546304845\n",
      "train loss:0.0025759207913101404\n",
      "train loss:0.008537385379387455\n",
      "train loss:0.00046686728002010936\n",
      "train loss:0.001447410244502497\n",
      "train loss:0.0017331718345066163\n",
      "train loss:0.005337915682877149\n",
      "train loss:0.0036143782514603436\n",
      "train loss:0.0009041991975394127\n",
      "train loss:0.0031543996122603097\n",
      "train loss:0.0011510324141044705\n",
      "train loss:0.0020270021226249713\n",
      "=== epoch:14, train acc:1.0, test acc:0.991 ===\n",
      "train loss:0.004093362585395563\n",
      "train loss:0.005632310453459621\n",
      "train loss:0.0019599088941910305\n",
      "train loss:0.0018956935219422264\n",
      "train loss:0.006121518574100828\n",
      "train loss:0.0047751583269867726\n",
      "train loss:0.009912920419557974\n",
      "train loss:0.0044133033001135715\n",
      "train loss:0.0018895555325244946\n",
      "train loss:0.0033507145200283896\n",
      "train loss:0.0011352634665274506\n",
      "train loss:0.007398132408552068\n",
      "train loss:0.0036440059685909658\n",
      "train loss:0.001632418545499998\n",
      "train loss:0.004520644595322888\n",
      "train loss:0.002548184299130219\n",
      "train loss:0.00025699773684956923\n",
      "train loss:0.0010372120356313848\n",
      "train loss:0.0024955341866313746\n",
      "train loss:0.00027521215859731995\n",
      "train loss:0.0008625522233736747\n",
      "train loss:0.014378545149126194\n",
      "train loss:0.011152192745876706\n",
      "train loss:0.019233364964836807\n",
      "train loss:0.002213379571027818\n",
      "train loss:0.000330090502795341\n",
      "train loss:0.0009990698642731223\n",
      "train loss:0.023862820708038614\n",
      "train loss:0.003958525935922493\n",
      "train loss:0.0009479689175871667\n",
      "train loss:0.0005413039957829306\n",
      "train loss:0.0011391996680123098\n",
      "train loss:0.0008495516474039929\n",
      "train loss:0.0042715954818445786\n",
      "train loss:0.00844973745387333\n",
      "train loss:0.0009751706341940752\n",
      "train loss:0.0008585470077304692\n",
      "train loss:0.0006268122498586076\n",
      "train loss:0.008775148760092819\n",
      "train loss:0.0008028777869156644\n",
      "train loss:0.003941853725506229\n",
      "train loss:0.00963353804138973\n",
      "train loss:0.010633424865856743\n",
      "train loss:0.00044998995501998045\n",
      "train loss:0.005099704397893051\n",
      "train loss:0.0013246193382981347\n",
      "train loss:0.007474903450333823\n",
      "train loss:0.013123295192988594\n",
      "train loss:0.003146379513438306\n",
      "train loss:0.004200467837789341\n",
      "train loss:0.04766510989905883\n",
      "train loss:0.0018902242614234968\n",
      "train loss:0.001669192769716968\n",
      "train loss:0.00955974769242286\n",
      "train loss:0.0021075767928070118\n",
      "train loss:0.0009961661967065767\n",
      "train loss:0.01577698416393768\n",
      "train loss:0.0019900452441277802\n",
      "train loss:0.015308949603046929\n",
      "train loss:0.0010133368034875947\n",
      "train loss:0.004940005981675464\n",
      "train loss:0.0013632785863182107\n",
      "train loss:0.0013125265989083626\n",
      "train loss:0.009611529652067975\n",
      "train loss:0.0004584715180639918\n",
      "train loss:0.003954854122475325\n",
      "train loss:0.0009934678530492667\n",
      "train loss:0.0033484019966953677\n",
      "train loss:0.0011611487923736499\n",
      "train loss:0.03177880438463451\n",
      "train loss:0.0004200757560969823\n",
      "train loss:0.00033152035941195465\n",
      "train loss:0.05464762265289986\n",
      "train loss:0.009658801288884616\n",
      "train loss:0.00026324393648625744\n",
      "train loss:0.00739109874319047\n",
      "train loss:0.005519958211281933\n",
      "train loss:0.01996059785828813\n",
      "train loss:0.0161971671216711\n",
      "train loss:0.0040951444982763804\n",
      "train loss:0.001849885528122698\n",
      "train loss:0.004674942484443628\n",
      "train loss:0.08132570517664289\n",
      "train loss:0.0017161477329609989\n",
      "train loss:0.004273212914110241\n",
      "train loss:0.00022438029454349847\n",
      "train loss:0.0003527811514281332\n",
      "train loss:0.004975387209469686\n",
      "train loss:0.00738759406276139\n",
      "train loss:0.003671240927242497\n",
      "train loss:0.00040379367191199496\n",
      "train loss:0.018692356469694554\n",
      "train loss:0.00576127121904292\n",
      "train loss:0.0018993077647460005\n",
      "train loss:0.027123676891477086\n",
      "train loss:0.00179198693877464\n",
      "train loss:0.0018669791207068323\n",
      "train loss:0.0025943137290357636\n",
      "train loss:0.005558889064420806\n",
      "train loss:0.00048005515539200814\n",
      "train loss:0.006374846884809389\n",
      "train loss:0.0011604153073914109\n",
      "train loss:0.002972343969905502\n",
      "train loss:0.0011135529039408855\n",
      "train loss:0.006005799865223363\n",
      "train loss:0.0007387703240287795\n",
      "train loss:0.01967922502915397\n",
      "train loss:0.002445964838802074\n",
      "train loss:0.0012826273156496897\n",
      "train loss:0.003204772041914793\n",
      "train loss:0.0023107106858634855\n",
      "train loss:0.004957206486191308\n",
      "train loss:0.0010566322129541405\n",
      "train loss:0.0005319585068219942\n",
      "train loss:0.001267577599747409\n",
      "train loss:0.0027013086961121787\n",
      "train loss:0.00030988086947072524\n",
      "train loss:0.008673997972062343\n",
      "train loss:0.01074410901580345\n",
      "train loss:0.0006905165992956664\n",
      "train loss:0.004037766769739207\n",
      "train loss:0.00017558573247454922\n",
      "train loss:0.0009264524764552571\n",
      "train loss:0.0039009397923920595\n",
      "train loss:0.005717963257228651\n",
      "train loss:0.0027913830511618327\n",
      "train loss:0.004726489084159266\n",
      "train loss:0.0011085378319200738\n",
      "train loss:0.001752198786909631\n",
      "train loss:0.0006642723965387769\n",
      "train loss:0.0009061352608010949\n",
      "train loss:0.012728341521501962\n",
      "train loss:0.0048797987793324955\n",
      "train loss:0.000485452116182797\n",
      "train loss:0.002830524174080106\n",
      "train loss:0.0032418315830044326\n",
      "train loss:0.0017627872173098119\n",
      "train loss:0.0006107510222566228\n",
      "train loss:0.0018605523931981697\n",
      "train loss:0.004126139640016696\n",
      "train loss:0.002432952698750897\n",
      "train loss:0.001356229981602957\n",
      "train loss:0.001862328180039236\n",
      "train loss:0.00397739621881158\n",
      "train loss:0.03850187280057233\n",
      "train loss:0.0017518700041786524\n",
      "train loss:0.0035043874262323566\n",
      "train loss:0.0010111446856383903\n",
      "train loss:0.0009126992696040999\n",
      "train loss:0.000819260095407353\n",
      "train loss:0.001121973939395509\n",
      "train loss:0.001657288071525212\n",
      "train loss:0.00026310387468858967\n",
      "train loss:0.0017549007645384746\n",
      "train loss:0.005757279227512505\n",
      "train loss:0.002101141323276976\n",
      "train loss:0.0004159802024572416\n",
      "train loss:0.0024170150678166037\n",
      "train loss:0.01720258431339496\n",
      "train loss:0.0009761745798821882\n",
      "train loss:0.0007647649799175952\n",
      "train loss:0.003058943604494629\n",
      "train loss:0.0011458765852373825\n",
      "train loss:0.005454858532570942\n",
      "train loss:0.004831516332501581\n",
      "train loss:0.0499420286749918\n",
      "train loss:0.01599744031756265\n",
      "train loss:0.0015166714169459181\n",
      "train loss:0.001532262238621847\n",
      "train loss:0.014298028658208409\n",
      "train loss:0.0005049333305140294\n",
      "train loss:0.0035221737467248386\n",
      "train loss:0.005415290857626117\n",
      "train loss:0.0034169082399945778\n",
      "train loss:0.006657312874581496\n",
      "train loss:0.0007196632037159181\n",
      "train loss:0.0005868353321003058\n",
      "train loss:0.009516979921322275\n",
      "train loss:0.007024112530032296\n",
      "train loss:0.002537261648239984\n",
      "train loss:0.015576973462837436\n",
      "train loss:0.0038763745205982053\n",
      "train loss:0.02012365399397606\n",
      "train loss:0.00317314872283283\n",
      "train loss:0.004482058174913597\n",
      "train loss:0.0009907949446388378\n",
      "train loss:0.010601112564684805\n",
      "train loss:0.01045039533227364\n",
      "train loss:0.0002808815355192732\n",
      "train loss:0.0012548763436069574\n",
      "train loss:0.00271090036615304\n",
      "train loss:0.02361296505838729\n",
      "train loss:0.0039068486796679704\n",
      "train loss:0.0006751140673086609\n",
      "train loss:0.004411693005365949\n",
      "train loss:0.0029126071873061482\n",
      "train loss:0.0029478496487715584\n",
      "train loss:0.006050802701857982\n",
      "train loss:0.0002261381339054082\n",
      "train loss:0.0029588638389727766\n",
      "train loss:0.0014585059700628503\n",
      "train loss:0.003275656565856403\n",
      "train loss:0.0012866465121155208\n",
      "train loss:0.001042122567655651\n",
      "train loss:0.0013579737166886173\n",
      "train loss:0.009292720606032112\n",
      "train loss:0.00042407129218890457\n",
      "train loss:0.0074552094938825355\n",
      "train loss:0.004684266777381381\n",
      "train loss:0.0030312095363477037\n",
      "train loss:0.002509749547794906\n",
      "train loss:0.0004380929476226964\n",
      "train loss:0.003592602726925621\n",
      "train loss:0.0028261240972779476\n",
      "train loss:0.0004525230764692912\n",
      "train loss:0.0018200832378778803\n",
      "train loss:0.003846187979670393\n",
      "train loss:0.01084352521074862\n",
      "train loss:0.000860887167145443\n",
      "train loss:0.018711156511546603\n",
      "train loss:0.0009386966982058232\n",
      "train loss:0.0009940970743397084\n",
      "train loss:0.004701424844192997\n",
      "train loss:0.004610021651803824\n",
      "train loss:0.001205683863938868\n",
      "train loss:0.00678998028504221\n",
      "train loss:0.004229750976236669\n",
      "train loss:0.001271632408708042\n",
      "train loss:0.0007852847023847844\n",
      "train loss:0.008409869666528931\n",
      "train loss:0.0026263271438331415\n",
      "train loss:0.0001922266242802723\n",
      "train loss:0.005202797136243147\n",
      "train loss:0.0010988233085865238\n",
      "train loss:0.0012184167268128744\n",
      "train loss:0.004802250670177732\n",
      "train loss:0.0005255758124028209\n",
      "train loss:0.004116003362118315\n",
      "train loss:0.0017068492984335364\n",
      "train loss:0.000624712219092533\n",
      "train loss:0.00018536792004694366\n",
      "train loss:0.0011078500492078364\n",
      "train loss:0.0018798619755039845\n",
      "train loss:0.000632198855238779\n",
      "train loss:0.0003424416801022184\n",
      "train loss:0.0014226365665291959\n",
      "train loss:0.00037096682490262724\n",
      "train loss:0.0026242158410701992\n",
      "train loss:0.001970640393046018\n",
      "train loss:0.0007555875655725978\n",
      "train loss:0.0015428594758572682\n",
      "train loss:0.0005776430018660607\n",
      "train loss:0.0005563738062748218\n",
      "train loss:0.0008850545927531388\n",
      "train loss:0.0019347496168958733\n",
      "train loss:0.0011003150794158373\n",
      "train loss:0.013631892717883818\n",
      "train loss:0.004615804489580475\n",
      "train loss:0.011765396775833398\n",
      "train loss:0.0026959951174483354\n",
      "train loss:0.0011307646949701912\n",
      "train loss:0.0012942209406465721\n",
      "train loss:0.0001332099075870397\n",
      "train loss:0.00016757782309540466\n",
      "train loss:0.0017561001781018332\n",
      "train loss:0.0015488728800586832\n",
      "train loss:0.001419963660932794\n",
      "train loss:0.0018185136774078491\n",
      "train loss:0.00347158796473758\n",
      "train loss:0.002126533962611917\n",
      "train loss:0.0002845651079987198\n",
      "train loss:0.0009408775814459403\n",
      "train loss:0.0013462270143605071\n",
      "train loss:0.012996970338579503\n",
      "train loss:0.0011104253139592962\n",
      "train loss:0.005610124742120848\n",
      "train loss:0.01143648635832503\n",
      "train loss:0.003252041469011266\n",
      "train loss:0.0012125857488380911\n",
      "train loss:0.007383413263283812\n",
      "train loss:0.0013644899179477072\n",
      "train loss:0.008426999863116959\n",
      "train loss:0.003911242757187321\n",
      "train loss:0.0006637077979870747\n",
      "train loss:0.0017703026211215841\n",
      "train loss:0.0005146183849840599\n",
      "train loss:0.005234944549489448\n",
      "train loss:0.0009941324465825906\n",
      "train loss:0.004237998111041301\n",
      "train loss:0.0027115820174201345\n",
      "train loss:0.00429604619102685\n",
      "train loss:0.0020831797921266977\n",
      "train loss:5.682552419512938e-05\n",
      "train loss:0.00332434722424789\n",
      "train loss:0.0002842984970979432\n",
      "train loss:0.0014231989787791776\n",
      "train loss:0.0014989618181522324\n",
      "train loss:0.009385791070295482\n",
      "train loss:0.014249302269498837\n",
      "train loss:0.00268622219317303\n",
      "train loss:0.002411335275352805\n",
      "train loss:0.0013992480241875845\n",
      "train loss:0.0022585513701760015\n",
      "train loss:0.01717753390680092\n",
      "train loss:0.0021278550809100665\n",
      "train loss:0.0006413591857838313\n",
      "train loss:0.011853950212442217\n",
      "train loss:0.0028681670538164674\n",
      "train loss:0.0006724482547006402\n",
      "train loss:0.004453891982971964\n",
      "train loss:0.0010391178366809305\n",
      "train loss:0.009443730171675734\n",
      "train loss:0.005971538452279025\n",
      "train loss:0.01440456413017571\n",
      "train loss:0.022195035890685406\n",
      "train loss:0.001879951583499487\n",
      "train loss:0.0053792447048313565\n",
      "train loss:0.043163631580442875\n",
      "train loss:0.0018673611974003006\n",
      "train loss:0.007514185825615344\n",
      "train loss:0.0027092256962287105\n",
      "train loss:0.00042220081332175913\n",
      "train loss:0.0017798578642179972\n",
      "train loss:0.01894832846259319\n",
      "train loss:0.00332770918573238\n",
      "train loss:0.0095822060367702\n",
      "train loss:0.0010317450906690425\n",
      "train loss:0.013083389861167655\n",
      "train loss:0.00029459636894549004\n",
      "train loss:0.0033587144817599584\n",
      "train loss:0.005500452802924332\n",
      "train loss:0.0028388509636405905\n",
      "train loss:9.653733326942962e-05\n",
      "train loss:0.010865389885606296\n",
      "train loss:0.005047387007907196\n",
      "train loss:0.006487675755694463\n",
      "train loss:0.0018702612185129773\n",
      "train loss:0.02332464050067605\n",
      "train loss:0.004712723249428274\n",
      "train loss:0.000346833208241659\n",
      "train loss:0.008266970738241902\n",
      "train loss:0.0013029432951568347\n",
      "train loss:0.010322358271897127\n",
      "train loss:0.0029001679951693787\n",
      "train loss:0.0030495702935572607\n",
      "train loss:0.0018003279593430703\n",
      "train loss:0.013424414648669345\n",
      "train loss:0.00383308874513832\n",
      "train loss:0.002116346144347851\n",
      "train loss:0.005440718376377492\n",
      "train loss:0.009164432585589634\n",
      "train loss:0.0004479558806140179\n",
      "train loss:0.011196477641375\n",
      "train loss:0.0006505073306908298\n",
      "train loss:0.004621418032251468\n",
      "train loss:0.006197982678601375\n",
      "train loss:0.000258377117272308\n",
      "train loss:0.0023226063926962094\n",
      "train loss:0.00037449127252833195\n",
      "train loss:0.0024994876160210782\n",
      "train loss:0.0023572710650002124\n",
      "train loss:0.0015511515217691253\n",
      "train loss:0.0024948488810107973\n",
      "train loss:0.003758406493418438\n",
      "train loss:0.010361472492622892\n",
      "train loss:0.001390899131508527\n",
      "train loss:0.006893123078717758\n",
      "train loss:0.0018037947112519489\n",
      "train loss:0.014561102829319715\n",
      "train loss:0.009083271492075514\n",
      "train loss:0.0010634165723496874\n",
      "train loss:0.0028365973738117257\n",
      "train loss:0.0019173502306502757\n",
      "train loss:9.559266190475722e-05\n",
      "train loss:0.008915936426069797\n",
      "train loss:0.0032327427068061052\n",
      "train loss:0.010279513399389689\n",
      "train loss:0.0007707642424103633\n",
      "train loss:0.0032648271593242327\n",
      "train loss:0.004127295018050069\n",
      "train loss:0.004256259225507966\n",
      "train loss:0.0006122356986037119\n",
      "train loss:0.0006750171746800299\n",
      "train loss:0.0029902923982589496\n",
      "train loss:0.02290518553155637\n",
      "train loss:0.0020727885938687327\n",
      "train loss:0.000889171207566097\n",
      "train loss:0.007346105405791028\n",
      "train loss:0.014547967523805645\n",
      "train loss:0.007010058698306763\n",
      "train loss:0.0018252090618525083\n",
      "train loss:0.002363738144820989\n",
      "train loss:0.003327970592079427\n",
      "train loss:0.00040447989917014043\n",
      "train loss:0.024784161769582428\n",
      "train loss:0.0005845627627144803\n",
      "train loss:0.042479492646304344\n",
      "train loss:0.0006048944367031533\n",
      "train loss:0.009175516551738313\n",
      "train loss:0.0006849968968294449\n",
      "train loss:0.0017075919849648365\n",
      "train loss:0.0020653884507800813\n",
      "train loss:0.0017344874130934232\n",
      "train loss:0.006873384769443281\n",
      "train loss:0.0023022360268399847\n",
      "train loss:0.0008950962658898334\n",
      "train loss:0.004115953619968174\n",
      "train loss:0.0031438999389342918\n",
      "train loss:0.0014036584964094414\n",
      "train loss:0.0016228405210428965\n",
      "train loss:0.003951035343317516\n",
      "train loss:0.0020361881911749135\n",
      "train loss:0.006552421695716903\n",
      "train loss:0.00017224370837925273\n",
      "train loss:0.01276973589048408\n",
      "train loss:0.0017140379420598675\n",
      "train loss:0.010974029865872332\n",
      "train loss:0.0005586634653819944\n",
      "train loss:0.0023094973887613267\n",
      "train loss:0.0011902539764085066\n",
      "train loss:0.0019219404836824266\n",
      "train loss:0.003292850553113161\n",
      "train loss:0.0023749284303551223\n",
      "train loss:0.0013286829914217278\n",
      "train loss:0.0014374361453836902\n",
      "train loss:0.0007416901794763051\n",
      "train loss:0.004188593999890063\n",
      "train loss:0.003502881666103663\n",
      "train loss:0.002503915834108597\n",
      "train loss:0.0011428556965920895\n",
      "train loss:0.015482771018790482\n",
      "train loss:0.006494689431969027\n",
      "train loss:0.0011197620348855862\n",
      "train loss:0.00945396445469364\n",
      "train loss:0.004700713675627893\n",
      "train loss:0.0005854201223138285\n",
      "train loss:0.006404294817998676\n",
      "train loss:0.0032963559698135497\n",
      "train loss:0.0004620609287779983\n",
      "train loss:0.004831409605189937\n",
      "train loss:0.0015785169257225777\n",
      "train loss:0.009945698487518246\n",
      "train loss:0.005283999988611217\n",
      "train loss:0.0021728725959813565\n",
      "train loss:0.004398349942274516\n",
      "train loss:0.001400874701742169\n",
      "train loss:0.002317828048260343\n",
      "train loss:0.012931592575855681\n",
      "train loss:0.0058470125608047875\n",
      "train loss:0.007085193271354907\n",
      "train loss:0.002238541760775732\n",
      "train loss:0.004019181501295157\n",
      "train loss:0.0017072158521768293\n",
      "train loss:0.001221638597900451\n",
      "train loss:0.002423600897645379\n",
      "train loss:0.0029225403393383486\n",
      "train loss:0.006732065000385455\n",
      "train loss:0.0027650101966391286\n",
      "train loss:0.002246215060906139\n",
      "train loss:0.0021976156630506495\n",
      "train loss:0.0017053185091741101\n",
      "train loss:0.0006553058638266404\n",
      "train loss:0.004795626787257442\n",
      "train loss:0.007296594963419527\n",
      "train loss:0.010861851875333952\n",
      "train loss:0.009636716923983662\n",
      "train loss:0.0004508982039768854\n",
      "train loss:0.006049132144163741\n",
      "train loss:0.0011161191877195487\n",
      "train loss:0.0002134067976423862\n",
      "train loss:0.002046603514345027\n",
      "train loss:0.0029336971025576497\n",
      "train loss:0.005142068802700971\n",
      "train loss:0.0009063630665208555\n",
      "train loss:0.003941839242588887\n",
      "train loss:0.016606363503291192\n",
      "train loss:0.002442855808581041\n",
      "train loss:0.05936290943937774\n",
      "train loss:0.0006069871150056337\n",
      "train loss:0.004009665436878073\n",
      "train loss:0.0013422764632943537\n",
      "train loss:0.0011132167964405125\n",
      "train loss:0.0018815600253956684\n",
      "train loss:0.0029777499041074434\n",
      "train loss:0.0004949269664061913\n",
      "train loss:0.008463568696770866\n",
      "train loss:0.001384378841221936\n",
      "train loss:0.0010382097842124554\n",
      "train loss:0.0054607773097553455\n",
      "train loss:0.002987192523823875\n",
      "train loss:0.01889311044412044\n",
      "train loss:0.001228179231932685\n",
      "train loss:0.0014100799241371705\n",
      "train loss:0.005401558708333781\n",
      "train loss:0.004030763098161122\n",
      "train loss:0.0007707366294350802\n",
      "train loss:0.009678459227558644\n",
      "train loss:0.0037101592916738368\n",
      "train loss:0.0033907285228556992\n",
      "train loss:0.003246258147647826\n",
      "train loss:0.0009855929903901867\n",
      "train loss:0.0014430488877729933\n",
      "train loss:0.0004220418050519302\n",
      "train loss:0.0024850762787440818\n",
      "train loss:0.0042409721723242816\n",
      "train loss:0.005204736086884238\n",
      "train loss:0.0005099056208711381\n",
      "train loss:0.001672483924570764\n",
      "train loss:0.0025524278052579635\n",
      "train loss:0.00022321807586441465\n",
      "train loss:0.005196353811459891\n",
      "train loss:0.00016627201489309153\n",
      "train loss:0.014999998915839328\n",
      "train loss:0.003600282255640929\n",
      "train loss:0.0022994308657090646\n",
      "train loss:0.004916557060629045\n",
      "train loss:0.0013605458651419868\n",
      "train loss:0.0014494654016061488\n",
      "train loss:0.001143067948152413\n",
      "train loss:0.015190038457461356\n",
      "train loss:0.0003930160670770326\n",
      "train loss:0.000438009109567572\n",
      "train loss:0.001558508938264348\n",
      "train loss:0.000664881224183913\n",
      "train loss:0.0032746653377298167\n",
      "train loss:0.00779619503348957\n",
      "train loss:0.01420332911338673\n",
      "train loss:0.002248493284206431\n",
      "train loss:0.0025671933751341\n",
      "train loss:0.004735200792909154\n",
      "train loss:0.006543755968166307\n",
      "train loss:0.03898920195909398\n",
      "train loss:0.03847997701110757\n",
      "train loss:0.0009915302767795995\n",
      "train loss:8.322550670486795e-05\n",
      "train loss:0.0016276666440701959\n",
      "train loss:0.0016429067940945707\n",
      "train loss:0.0030751343489898407\n",
      "train loss:0.004770028747170701\n",
      "train loss:0.00032856019597520415\n",
      "train loss:0.0009028744551263066\n",
      "train loss:0.006977646514590823\n",
      "train loss:0.011483944481089008\n",
      "train loss:0.005292519760174261\n",
      "train loss:0.007566580249568236\n",
      "train loss:0.004913917299589917\n",
      "train loss:0.0021885786776388878\n",
      "train loss:0.0016471779445938314\n",
      "train loss:0.004299514551191375\n",
      "train loss:0.0006525399513449033\n",
      "train loss:0.00313992842498094\n",
      "train loss:0.0005608706391026165\n",
      "train loss:0.004340081294914117\n",
      "train loss:0.03039575730617888\n",
      "train loss:0.011090981957089196\n",
      "train loss:0.002171535449864352\n",
      "train loss:0.0011971164947587047\n",
      "train loss:0.002225159550925769\n",
      "train loss:0.009128255447945807\n",
      "train loss:0.0012400150375090422\n",
      "train loss:0.005792722889746008\n",
      "train loss:0.0021402700592042757\n",
      "train loss:0.0015412791344025841\n",
      "train loss:0.004533239000033365\n",
      "train loss:0.00020497587595038266\n",
      "train loss:0.003242555322784763\n",
      "train loss:0.010362405188292838\n",
      "train loss:0.009098032393269341\n",
      "train loss:0.0020583642570976323\n",
      "train loss:0.0035087742261427977\n",
      "train loss:0.0004165402082460296\n",
      "train loss:0.00085960438106221\n",
      "train loss:0.006727848144828118\n",
      "train loss:0.006373649141395703\n",
      "train loss:0.006349883306701192\n",
      "train loss:0.005381899021671282\n",
      "train loss:0.0004114967621499711\n",
      "train loss:0.01250829741420471\n",
      "train loss:0.0007946661517107076\n",
      "train loss:0.0030748559513000145\n",
      "train loss:0.004769949860767873\n",
      "train loss:0.0020334492894732987\n",
      "train loss:0.001238170138639302\n",
      "train loss:0.0036164149177313233\n",
      "train loss:0.0015073339018467351\n",
      "train loss:0.0004208140829282365\n",
      "train loss:0.0049191455375434835\n",
      "train loss:0.0006732294742136976\n",
      "train loss:0.002810588743930557\n",
      "train loss:0.0019491127204427183\n",
      "train loss:0.001046808366198017\n",
      "train loss:0.003076101501894111\n",
      "train loss:0.001675752104110151\n",
      "train loss:0.012195276076262595\n",
      "train loss:0.0012768313664994493\n",
      "train loss:0.009797208063064071\n",
      "train loss:0.026971396502334297\n",
      "train loss:0.0025324678893214794\n",
      "train loss:0.002931833908220198\n",
      "train loss:0.004090344201746035\n",
      "=== epoch:15, train acc:0.997, test acc:0.985 ===\n",
      "train loss:0.00043107130847495076\n",
      "train loss:0.004369642663632163\n",
      "train loss:0.0010577060742651954\n",
      "train loss:0.0005159137091660169\n",
      "train loss:0.027577101480265587\n",
      "train loss:0.017038060693511842\n",
      "train loss:0.0030481103253199816\n",
      "train loss:0.003654894656266211\n",
      "train loss:0.01821023488070604\n",
      "train loss:0.0021392024491474677\n",
      "train loss:0.001371288095327153\n",
      "train loss:0.0012464806001578157\n",
      "train loss:0.006121211104881701\n",
      "train loss:0.00039594010109205157\n",
      "train loss:0.00014135958318685753\n",
      "train loss:0.002064909195190683\n",
      "train loss:0.0022699488402909474\n",
      "train loss:0.01071685493651811\n",
      "train loss:8.893718064556745e-05\n",
      "train loss:0.001845602412559759\n",
      "train loss:0.002443476475818661\n",
      "train loss:0.01046538938566647\n",
      "train loss:0.001018509165981485\n",
      "train loss:0.004256762640671386\n",
      "train loss:0.001208727613511732\n",
      "train loss:0.006769563824336539\n",
      "train loss:0.019213184965435358\n",
      "train loss:0.0037685415692235136\n",
      "train loss:0.001327709844843695\n",
      "train loss:0.0034047991924256713\n",
      "train loss:0.0006087781951065377\n",
      "train loss:0.0013138961370307977\n",
      "train loss:0.0005287507544282564\n",
      "train loss:0.002907948859931447\n",
      "train loss:0.0028699025188795176\n",
      "train loss:0.005289119078750258\n",
      "train loss:0.0006117341084362572\n",
      "train loss:0.007244029791094988\n",
      "train loss:0.0008154167902873589\n",
      "train loss:0.001108322987892677\n",
      "train loss:0.0014083298606927903\n",
      "train loss:0.00280792192721825\n",
      "train loss:0.0018935451438004786\n",
      "train loss:0.0011785057887550882\n",
      "train loss:0.003961126012307774\n",
      "train loss:0.0029411250748126354\n",
      "train loss:0.0006066539286976812\n",
      "train loss:0.0011421255426516059\n",
      "train loss:0.005175934187708814\n",
      "train loss:0.0002114861665271288\n",
      "train loss:0.0007251845886047868\n",
      "train loss:0.0030587783708994763\n",
      "train loss:0.0019171341701841424\n",
      "train loss:0.0011937121126725963\n",
      "train loss:0.003452113559798989\n",
      "train loss:0.004385944136500257\n",
      "train loss:0.005744234413719695\n",
      "train loss:0.003088265224706621\n",
      "train loss:0.0021178214731945237\n",
      "train loss:0.0017192078061134975\n",
      "train loss:4.828339168214238e-05\n",
      "train loss:0.000328695720297622\n",
      "train loss:0.0015337966764014965\n",
      "train loss:0.007004194975879384\n",
      "train loss:0.0011100878856596897\n",
      "train loss:0.0018703284744704526\n",
      "train loss:0.005382134367383465\n",
      "train loss:0.01105391331683262\n",
      "train loss:0.0012823708299583051\n",
      "train loss:0.002551475352478344\n",
      "train loss:0.01015513243712081\n",
      "train loss:0.0004503216705851558\n",
      "train loss:0.004415157827221063\n",
      "train loss:0.001339258700548299\n",
      "train loss:0.0008360001472820182\n",
      "train loss:0.0008764533089427788\n",
      "train loss:0.0018805612365041346\n",
      "train loss:0.0023953276322682187\n",
      "train loss:0.000948512607590139\n",
      "train loss:0.006385863077083806\n",
      "train loss:0.0011182848449334505\n",
      "train loss:0.0027706269811870526\n",
      "train loss:0.002531724647372651\n",
      "train loss:0.0014438898543141438\n",
      "train loss:0.0005719604191149297\n",
      "train loss:0.003522113803393197\n",
      "train loss:0.001037728943576855\n",
      "train loss:0.0009387418981074875\n",
      "train loss:0.0005063455144931041\n",
      "train loss:0.0039298477582350696\n",
      "train loss:0.0007016311805368458\n",
      "train loss:0.007998186411809665\n",
      "train loss:0.0013329642554517581\n",
      "train loss:0.0009061481726099812\n",
      "train loss:0.0002348080693478287\n",
      "train loss:0.005113405534486237\n",
      "train loss:0.0020200094327955578\n",
      "train loss:0.00204974735297482\n",
      "train loss:0.0004598059808058048\n",
      "train loss:0.0009159958882335438\n",
      "train loss:0.0007580214476936753\n",
      "train loss:0.0007897796908750954\n",
      "train loss:0.024950191497703434\n",
      "train loss:0.007941804983532452\n",
      "train loss:0.0010184256919730126\n",
      "train loss:0.0036828931844728745\n",
      "train loss:0.00046045235112428717\n",
      "train loss:0.0003225544984773627\n",
      "train loss:0.042266952146357006\n",
      "train loss:0.016417642592162985\n",
      "train loss:0.0002573882914851612\n",
      "train loss:0.0014596750364545919\n",
      "train loss:0.0023253108684690767\n",
      "train loss:0.0027898540109262725\n",
      "train loss:0.00034266220221766084\n",
      "train loss:0.006218481147713316\n",
      "train loss:0.001980341570716425\n",
      "train loss:0.0001093727323283572\n",
      "train loss:0.00010068613811037538\n",
      "train loss:0.00047611344411032644\n",
      "train loss:0.00975611056005223\n",
      "train loss:0.0017100807265956592\n",
      "train loss:0.00015797081660747436\n",
      "train loss:0.0025725499256077492\n",
      "train loss:0.00033271156253550653\n",
      "train loss:0.00020529255030409804\n",
      "train loss:0.0036695233426407314\n",
      "train loss:0.003506031403929641\n",
      "train loss:0.00032878291660732467\n",
      "train loss:5.052573330018087e-05\n",
      "train loss:0.0007147208898912053\n",
      "train loss:0.0022366561009518923\n",
      "train loss:0.004627171157925484\n",
      "train loss:0.0007976451644035911\n",
      "train loss:0.00042330499942351817\n",
      "train loss:0.004745278394642139\n",
      "train loss:0.0011805402792887485\n",
      "train loss:0.01022850188929499\n",
      "train loss:0.00016247147349016943\n",
      "train loss:0.003119974309328606\n",
      "train loss:0.004832648806136458\n",
      "train loss:0.0003763449890758798\n",
      "train loss:0.0015461128562754218\n",
      "train loss:0.0013018125714823231\n",
      "train loss:0.016690674117309342\n",
      "train loss:0.0007377279275731313\n",
      "train loss:0.0031771886220811644\n",
      "train loss:0.0002668608882714105\n",
      "train loss:0.0010804795699209378\n",
      "train loss:0.00048058467797981714\n",
      "train loss:0.0006031485796836497\n",
      "train loss:0.02137953445715838\n",
      "train loss:0.0025350140535556048\n",
      "train loss:0.010857154270910512\n",
      "train loss:0.011267612728141347\n",
      "train loss:0.0021524688105781864\n",
      "train loss:0.005045889117049626\n",
      "train loss:0.0032392462755165548\n",
      "train loss:0.0024640022577628036\n",
      "train loss:0.00110568136528176\n",
      "train loss:0.001004486915307216\n",
      "train loss:0.00182378389626055\n",
      "train loss:0.004947041562919261\n",
      "train loss:0.02361959013588584\n",
      "train loss:0.00018559449384665216\n",
      "train loss:0.005980152031619615\n",
      "train loss:0.005555365763768793\n",
      "train loss:0.012829771367257626\n",
      "train loss:0.00048236555763290763\n",
      "train loss:0.005022278053399834\n",
      "train loss:0.0009454300593447967\n",
      "train loss:0.0008851414417148532\n",
      "train loss:0.0008229957306405695\n",
      "train loss:0.0006625879019173958\n",
      "train loss:0.0013913384982100101\n",
      "train loss:0.002624981972798326\n",
      "train loss:0.002578543353212728\n",
      "train loss:0.0017815821874844165\n",
      "train loss:0.002071350327021263\n",
      "train loss:0.0025790587197446918\n",
      "train loss:0.0008458798627229362\n",
      "train loss:0.009584590892278078\n",
      "train loss:0.0026006723233372043\n",
      "train loss:0.002016141994040707\n",
      "train loss:0.0009417555836389863\n",
      "train loss:0.0023837024858897353\n",
      "train loss:0.011176326905487184\n",
      "train loss:0.014939735195910464\n",
      "train loss:0.0008125113759183293\n",
      "train loss:0.0018748231067902516\n",
      "train loss:0.002900772288211133\n",
      "train loss:0.00913998012794049\n",
      "train loss:0.00016379610280492813\n",
      "train loss:0.0009456027720220284\n",
      "train loss:0.005236428516509815\n",
      "train loss:0.008907928560586524\n",
      "train loss:0.009670229081016422\n",
      "train loss:0.002825725426769577\n",
      "train loss:0.002638347316294539\n",
      "train loss:0.0008562316412133035\n",
      "train loss:0.005873802872702564\n",
      "train loss:0.0013365362586863284\n",
      "train loss:0.0016752035048242294\n",
      "train loss:0.0034496958270177386\n",
      "train loss:0.0066782821047109385\n",
      "train loss:0.0005649295853132453\n",
      "train loss:0.007219918654155278\n",
      "train loss:0.0014759563982130388\n",
      "train loss:0.002857531635277596\n",
      "train loss:0.00877811450718711\n",
      "train loss:0.0010474872963350955\n",
      "train loss:0.000200342505549089\n",
      "train loss:0.0007978345872343917\n",
      "train loss:0.00683512680607581\n",
      "train loss:0.0012284041569565907\n",
      "train loss:0.0021164741584931606\n",
      "train loss:0.002309901862675938\n",
      "train loss:0.003956914299195191\n",
      "train loss:0.00042124234583930804\n",
      "train loss:0.0038534834954320747\n",
      "train loss:0.0017817033827361257\n",
      "train loss:0.004364222004437853\n",
      "train loss:0.006445986871408063\n",
      "train loss:0.0006647272743277914\n",
      "train loss:0.034465636936381855\n",
      "train loss:0.003377903360743145\n",
      "train loss:0.0032615718132606615\n",
      "train loss:0.044882248941868125\n",
      "train loss:0.014073502233568552\n",
      "train loss:0.0010324539102132652\n",
      "train loss:0.01636035585261737\n",
      "train loss:0.0007696221523061041\n",
      "train loss:0.0029187868638360466\n",
      "train loss:0.0005380915318760603\n",
      "train loss:0.0028540270845090997\n",
      "train loss:0.0014738239155430502\n",
      "train loss:0.0009058690104907205\n",
      "train loss:0.011699885905719364\n",
      "train loss:0.004554542904674577\n",
      "train loss:0.0672536915730019\n",
      "train loss:0.013865667654957836\n",
      "train loss:0.0009528261477461\n",
      "train loss:0.0019950106424156643\n",
      "train loss:0.1626484829300281\n",
      "train loss:0.0012569225628443073\n",
      "train loss:0.0013139476024498385\n",
      "train loss:0.004290389467260113\n",
      "train loss:0.011829576102325094\n",
      "train loss:0.0037197457962144607\n",
      "train loss:0.0036677345029148506\n",
      "train loss:0.008906941351073686\n",
      "train loss:0.0088896551458628\n",
      "train loss:0.0013711967071087638\n",
      "train loss:0.002192219191298276\n",
      "train loss:0.0019349614296713064\n",
      "train loss:0.0035971138923537017\n",
      "train loss:0.02605728666531299\n",
      "train loss:0.0011865029580913821\n",
      "train loss:0.02046206384451186\n",
      "train loss:0.0012140885618195582\n",
      "train loss:0.000976395082878072\n",
      "train loss:0.006838426836635893\n",
      "train loss:0.0030229369494002194\n",
      "train loss:4.8511473649444864e-05\n",
      "train loss:0.10232999877431677\n",
      "train loss:0.0037584073375632825\n",
      "train loss:0.01059993475345434\n",
      "train loss:0.002626656139650955\n",
      "train loss:0.003487827118931665\n",
      "train loss:0.003144948466255539\n",
      "train loss:0.0010090360769839026\n",
      "train loss:0.0048832200781107\n",
      "train loss:0.00281741318487517\n",
      "train loss:0.0007263062499131552\n",
      "train loss:0.006381684190871804\n",
      "train loss:0.002532467090791936\n",
      "train loss:0.00672490475345789\n",
      "train loss:0.00479381366449447\n",
      "train loss:0.0007481783847704204\n",
      "train loss:0.0016646707109073194\n",
      "train loss:0.0033401958923640414\n",
      "train loss:0.0032028939119586298\n",
      "train loss:0.0003834062077604285\n",
      "train loss:0.0013610239004315239\n",
      "train loss:0.002831972796200581\n",
      "train loss:0.0014846363510911886\n",
      "train loss:0.0011974289193421841\n",
      "train loss:0.0035049209145280634\n",
      "train loss:0.003919667595246535\n",
      "train loss:0.0009019283784255411\n",
      "train loss:0.001385855129234642\n",
      "train loss:0.008068199810499246\n",
      "train loss:0.033906613927525066\n",
      "train loss:0.0007112409293131442\n",
      "train loss:0.0006189874840282533\n",
      "train loss:0.00045895276069520604\n",
      "train loss:0.013279370952674134\n",
      "train loss:0.0032626986174110867\n",
      "train loss:0.006000774970219145\n",
      "train loss:0.0033592009167937404\n",
      "train loss:0.0007255358428092416\n",
      "train loss:0.0028113007099618724\n",
      "train loss:0.001287894564021299\n",
      "train loss:0.0006856166152371587\n",
      "train loss:0.0013076922726293684\n",
      "train loss:0.00027724400321496226\n",
      "train loss:0.0008267410935192903\n",
      "train loss:0.012811956674226061\n",
      "train loss:0.0013641425499890017\n",
      "train loss:0.003739659009776512\n",
      "train loss:0.0014460887324287926\n",
      "train loss:0.00373562492367331\n",
      "train loss:0.007873904381666188\n",
      "train loss:0.00470638250013362\n",
      "train loss:0.11610003222206869\n",
      "train loss:0.0020384676578224926\n",
      "train loss:0.0009358793196736592\n",
      "train loss:0.006439029391968307\n",
      "train loss:0.00408377687677982\n",
      "train loss:0.0031786758743414857\n",
      "train loss:0.004805111600978058\n",
      "train loss:0.0003853415351785945\n",
      "train loss:0.005779950247372842\n",
      "train loss:0.0012793725073460378\n",
      "train loss:0.0011308722544435715\n",
      "train loss:0.0030964294974093146\n",
      "train loss:0.0027608847981868885\n",
      "train loss:0.000704689002361632\n",
      "train loss:0.018855855330833587\n",
      "train loss:0.004625504257600777\n",
      "train loss:0.0019293467420447505\n",
      "train loss:0.0023942350696270513\n",
      "train loss:0.0022166804366161165\n",
      "train loss:0.024325521937114455\n",
      "train loss:0.0003624949191356822\n",
      "train loss:0.0029456955089421323\n",
      "train loss:0.002427728214107137\n",
      "train loss:0.0010222022131529957\n",
      "train loss:0.0023131616214667277\n",
      "train loss:0.0006785742158619259\n",
      "train loss:0.0015834345126189775\n",
      "train loss:0.004344807230203599\n",
      "train loss:0.0006196189520020438\n",
      "train loss:0.00028166902101407917\n",
      "train loss:0.0019800953522724897\n",
      "train loss:0.0002419459390402982\n",
      "train loss:0.001013157589074669\n",
      "train loss:0.0017776158054496427\n",
      "train loss:0.020286537891769173\n",
      "train loss:0.0014877350373380992\n",
      "train loss:0.000601177121199989\n",
      "train loss:0.0016225672922085923\n",
      "train loss:0.0048821995728651\n",
      "train loss:0.002938997063711904\n",
      "train loss:0.019964773930898745\n",
      "train loss:0.0010248159868755523\n",
      "train loss:0.0030969857917553163\n",
      "train loss:0.0022539347307740654\n",
      "train loss:0.0013674114467579499\n",
      "train loss:0.011591472890606448\n",
      "train loss:0.00137098755984035\n",
      "train loss:0.0031302654813264615\n",
      "train loss:0.00296817251092568\n",
      "train loss:0.006893828715676835\n",
      "train loss:0.0013118094405186267\n",
      "train loss:0.0029048860559510313\n",
      "train loss:0.006664273600611107\n",
      "train loss:0.0030382169032838763\n",
      "train loss:0.003158380908502414\n",
      "train loss:0.006736445721908425\n",
      "train loss:0.0002488251476848764\n",
      "train loss:0.0012726989222406476\n",
      "train loss:0.0016543356738562406\n",
      "train loss:0.0048727641977036155\n",
      "train loss:0.0003235067588218443\n",
      "train loss:0.0009256880827192878\n",
      "train loss:0.0006921520360701879\n",
      "train loss:0.0003521772045685062\n",
      "train loss:0.00442525488447928\n",
      "train loss:0.0009107061286444302\n",
      "train loss:0.0004198460525608371\n",
      "train loss:0.0016377231739824096\n",
      "train loss:0.002431125978024431\n",
      "train loss:0.004155754405984105\n",
      "train loss:0.004182690120674379\n",
      "train loss:0.0029777451952703567\n",
      "train loss:0.0008465205951981094\n",
      "train loss:0.0022107209289694363\n",
      "train loss:0.0007018459710676564\n",
      "train loss:0.0037137506230258324\n",
      "train loss:0.0026158543733344735\n",
      "train loss:0.003979774933086842\n",
      "train loss:0.015621697574483057\n",
      "train loss:0.0020669007103787094\n",
      "train loss:0.0004566752351680104\n",
      "train loss:0.0036831503325059516\n",
      "train loss:0.0013222016415898657\n",
      "train loss:0.0007654742115597992\n",
      "train loss:0.003658351552740994\n",
      "train loss:0.0064015152583082775\n",
      "train loss:0.009281545393216043\n",
      "train loss:0.004460255754327303\n",
      "train loss:0.0037900752680317196\n",
      "train loss:0.00021585806900466231\n",
      "train loss:0.0017769575017922393\n",
      "train loss:0.003790483445078281\n",
      "train loss:0.0009305558328177878\n",
      "train loss:0.0009146022393700943\n",
      "train loss:0.022406677861636713\n",
      "train loss:0.00020369278413319522\n",
      "train loss:0.0012844500930706782\n",
      "train loss:0.0004665641096810479\n",
      "train loss:0.0011864898049223289\n",
      "train loss:0.0010734725181037886\n",
      "train loss:0.005844160363864177\n",
      "train loss:0.0029202596987401848\n",
      "train loss:0.0025847454118061624\n",
      "train loss:0.005977813814569062\n",
      "train loss:0.0851323364008903\n",
      "train loss:0.007268474487607549\n",
      "train loss:0.02441498653686249\n",
      "train loss:0.00010358061351590421\n",
      "train loss:0.0005750341969584154\n",
      "train loss:0.003380596618344474\n",
      "train loss:0.023367662621462462\n",
      "train loss:0.0050209176629902215\n",
      "train loss:0.001431111264023082\n",
      "train loss:0.00046689435221605415\n",
      "train loss:0.003201703339184282\n",
      "train loss:0.0010353840894667555\n",
      "train loss:0.006117622680705817\n",
      "train loss:0.006924872626641158\n",
      "train loss:0.0004674472541619051\n",
      "train loss:0.008531342399565671\n",
      "train loss:0.0022570650518537235\n",
      "train loss:0.004291354884258902\n",
      "train loss:0.00259996971356328\n",
      "train loss:0.00297767951372202\n",
      "train loss:0.004339256117589319\n",
      "train loss:0.004868353499601648\n",
      "train loss:0.002437566946615704\n",
      "train loss:0.003192521546202369\n",
      "train loss:0.0007458759941864489\n",
      "train loss:0.0022018829231733768\n",
      "train loss:0.005404635440866429\n",
      "train loss:0.0015424651319881943\n",
      "train loss:0.00233356155209843\n",
      "train loss:0.002034624658683326\n",
      "train loss:0.0017140990991447737\n",
      "train loss:0.00036787921022099707\n",
      "train loss:0.0012583877251871067\n",
      "train loss:0.0056725660522072\n",
      "train loss:0.0024280041252487776\n",
      "train loss:0.0010325218628941422\n",
      "train loss:0.0001483252920131187\n",
      "train loss:0.0027476192087120977\n",
      "train loss:0.09813865616326754\n",
      "train loss:0.0033120229549580727\n",
      "train loss:0.00014145557332656823\n",
      "train loss:0.005298232559263619\n",
      "train loss:0.001987099043337237\n",
      "train loss:0.0011904899656481845\n",
      "train loss:0.0034103596501519535\n",
      "train loss:0.0011866343958838966\n",
      "train loss:0.009701026522349398\n",
      "train loss:0.0007968942386051489\n",
      "train loss:0.0002579363254524279\n",
      "train loss:0.0027690402861037366\n",
      "train loss:0.00034575582247531696\n",
      "train loss:0.003025993883691902\n",
      "train loss:0.02059026662544603\n",
      "train loss:0.0029570512870322014\n",
      "train loss:0.0013314468459891331\n",
      "train loss:0.002672943825297375\n",
      "train loss:0.0028682717360212443\n",
      "train loss:0.0009804337247568365\n",
      "train loss:0.002441804275986912\n",
      "train loss:0.0031624596011258286\n",
      "train loss:0.00046092626093905564\n",
      "train loss:0.0024397069218687326\n",
      "train loss:0.002290932271109711\n",
      "train loss:0.0012557778500002113\n",
      "train loss:0.005723680230710785\n",
      "train loss:0.010083292725289737\n",
      "train loss:0.004736599631061406\n",
      "train loss:0.0003680130029681377\n",
      "train loss:0.0006781959219427938\n",
      "train loss:0.0036593824813321314\n",
      "train loss:0.0021865681243340897\n",
      "train loss:0.006523899635147717\n",
      "train loss:0.0024549162204951917\n",
      "train loss:0.003856788967248894\n",
      "train loss:0.0020031881544642508\n",
      "train loss:0.0014392364711688296\n",
      "train loss:0.0006669836231783323\n",
      "train loss:0.00395427683777386\n",
      "train loss:0.0009847296894252133\n",
      "train loss:0.01572472652310126\n",
      "train loss:0.0013247841689789852\n",
      "train loss:0.01838307892400699\n",
      "train loss:0.0006170534570621079\n",
      "train loss:0.003069302620953445\n",
      "train loss:0.007473417359996629\n",
      "train loss:0.0005244117258186084\n",
      "train loss:0.00041519639399895705\n",
      "train loss:0.0034857820839701795\n",
      "train loss:0.0019823759228641234\n",
      "train loss:0.0021743743373871254\n",
      "train loss:0.0010472161963918443\n",
      "train loss:0.0008877055171583471\n",
      "train loss:0.0004889368157727693\n",
      "train loss:0.03233700477475806\n",
      "train loss:0.0003789077832520109\n",
      "train loss:0.000663737155352261\n",
      "train loss:0.0012332058413325575\n",
      "train loss:0.00262817600060271\n",
      "train loss:0.0008270693879311427\n",
      "train loss:0.0009529917730742645\n",
      "train loss:0.002716781539402247\n",
      "train loss:0.0009459501663912713\n",
      "train loss:0.0019289254104302353\n",
      "train loss:0.00766157699743308\n",
      "train loss:0.0002562217797115729\n",
      "train loss:0.0015721378253667356\n",
      "train loss:0.00317859335457789\n",
      "train loss:0.0010956283953095485\n",
      "train loss:0.0006241595310305738\n",
      "train loss:0.004549833796036306\n",
      "train loss:0.0035182570211566254\n",
      "train loss:0.0031477440358010545\n",
      "train loss:0.008751340802756224\n",
      "train loss:0.0039246375849286445\n",
      "train loss:0.0001120666558959543\n",
      "train loss:0.0031663914174365772\n",
      "train loss:0.0005593028709957293\n",
      "train loss:0.010105795542831884\n",
      "train loss:0.006655030670335559\n",
      "train loss:0.0075915520720046025\n",
      "train loss:0.0012607666898456727\n",
      "train loss:0.004152998484285363\n",
      "train loss:0.0005037498911534983\n",
      "train loss:0.0019453381508485637\n",
      "train loss:0.0011570173081517081\n",
      "train loss:0.005242739737807337\n",
      "train loss:0.0005869860480417595\n",
      "train loss:0.003220731674623039\n",
      "train loss:0.0006405278026563754\n",
      "train loss:0.0070416455977774925\n",
      "train loss:0.0032559873272709518\n",
      "train loss:0.0005656903520146399\n",
      "train loss:0.00022234019630884772\n",
      "train loss:0.0018237933722233491\n",
      "train loss:0.001396143694320155\n",
      "train loss:0.00022653151099710629\n",
      "train loss:0.0001629501233269874\n",
      "train loss:0.0003687535002855077\n",
      "train loss:0.00045407332299874525\n",
      "train loss:0.0023085049369875334\n",
      "train loss:0.0022407801413453416\n",
      "train loss:0.0003834557415857247\n",
      "train loss:0.002053683814174417\n",
      "train loss:0.0009755237207289105\n",
      "train loss:0.0006029254605724116\n",
      "train loss:0.00210863287183219\n",
      "train loss:0.0028708359495685206\n",
      "train loss:0.002712476799071428\n",
      "train loss:0.0012051183173202855\n",
      "train loss:0.0031533824414088905\n",
      "train loss:9.803850018060521e-05\n",
      "train loss:0.00410813417338279\n",
      "train loss:0.004467204071468205\n",
      "train loss:0.0028090946306578925\n",
      "train loss:0.0008183154653087539\n",
      "train loss:0.00036907757767848425\n",
      "train loss:0.0016546845828251163\n",
      "train loss:0.0012924261987785055\n",
      "train loss:0.0007504403359206949\n",
      "train loss:0.0031572179799772087\n",
      "train loss:0.0008077525735297998\n",
      "train loss:0.003907342302446487\n",
      "train loss:0.0033191259351470796\n",
      "train loss:0.0007045108777768298\n",
      "train loss:0.002812450200209183\n",
      "train loss:0.013996683095583495\n",
      "train loss:0.0027972080888478174\n",
      "train loss:0.0015712309402490141\n",
      "train loss:0.002086345863750355\n",
      "train loss:0.004279212245792272\n",
      "train loss:0.00022682437418131563\n",
      "train loss:0.007682235183514391\n",
      "train loss:0.005496425980933198\n",
      "train loss:0.00015846834170967682\n",
      "train loss:0.0007622723655395709\n",
      "train loss:0.000727453271178076\n",
      "train loss:0.00034141508401816403\n",
      "train loss:0.00012958192811950552\n",
      "train loss:0.0011134506767426324\n",
      "train loss:0.00019837910795157194\n",
      "train loss:8.847335486660448e-05\n",
      "train loss:0.001239225512874747\n",
      "=== epoch:16, train acc:0.995, test acc:0.99 ===\n",
      "train loss:0.005944295653861537\n",
      "train loss:0.0031556721427136165\n",
      "train loss:0.0005016333851087272\n",
      "train loss:0.01796871056409624\n",
      "train loss:0.002996454862532711\n",
      "train loss:0.0005476650708554538\n",
      "train loss:0.001362147667275617\n",
      "train loss:0.0005044215780394972\n",
      "train loss:0.004174320324384485\n",
      "train loss:0.012461939870576522\n",
      "train loss:0.0033030454054422476\n",
      "train loss:0.012062836806599182\n",
      "train loss:0.002715468572975374\n",
      "train loss:0.0016917453352777872\n",
      "train loss:0.0009355647619788401\n",
      "train loss:0.007382691622231362\n",
      "train loss:0.0023071750692693657\n",
      "train loss:0.0005416393033705822\n",
      "train loss:0.004050436568057123\n",
      "train loss:0.047248870052974075\n",
      "train loss:0.014458164111423661\n",
      "train loss:0.010261506947947828\n",
      "train loss:0.013984689842909504\n",
      "train loss:0.0014495793934089893\n",
      "train loss:0.003013301178307642\n",
      "train loss:0.003021896036830805\n",
      "train loss:0.032449470092638155\n",
      "train loss:0.0010182603342246265\n",
      "train loss:0.004215177121187942\n",
      "train loss:0.010093923767538308\n",
      "train loss:0.00331686953362047\n",
      "train loss:0.009426464399562977\n",
      "train loss:0.0006795836792432296\n",
      "train loss:0.011294555741376316\n",
      "train loss:0.004742890861051956\n",
      "train loss:0.00026478779549927154\n",
      "train loss:0.0005963567468658714\n",
      "train loss:0.0014461794018629038\n",
      "train loss:0.00201581008506117\n",
      "train loss:0.00037940349069617304\n",
      "train loss:0.0007745762478089867\n",
      "train loss:0.011534605239201978\n",
      "train loss:0.0035740783333568877\n",
      "train loss:0.0012752273946454449\n",
      "train loss:0.000447790562262458\n",
      "train loss:0.003525498127977476\n",
      "train loss:0.002776689394368943\n",
      "train loss:0.0033633951647830613\n",
      "train loss:0.0003004004533573089\n",
      "train loss:0.002972398705629103\n",
      "train loss:0.00416742706071875\n",
      "train loss:0.008574854986997182\n",
      "train loss:0.0029676525043045967\n",
      "train loss:0.00014388562557382122\n",
      "train loss:0.0013127068699473642\n",
      "train loss:0.0014958449205837486\n",
      "train loss:0.0006983657162164783\n",
      "train loss:0.00025904064727352253\n",
      "train loss:0.006601593605207111\n",
      "train loss:0.006884387046673839\n",
      "train loss:0.0011605927785242453\n",
      "train loss:0.0033267428040950953\n",
      "train loss:0.0015618904232150939\n",
      "train loss:0.004645924831131892\n",
      "train loss:0.00044538782461544797\n",
      "train loss:0.009474411334332335\n",
      "train loss:0.0019398672341057275\n",
      "train loss:0.0017068124961577874\n",
      "train loss:0.004031530897735392\n",
      "train loss:0.0007804603879288577\n",
      "train loss:0.002228115638762387\n",
      "train loss:0.015625609726585315\n",
      "train loss:0.0018968697495745273\n",
      "train loss:0.003955450167462968\n",
      "train loss:0.014080535868792599\n",
      "train loss:0.010562828117766489\n",
      "train loss:0.0011871956874955419\n",
      "train loss:0.014231848267305227\n",
      "train loss:0.003777359063877073\n",
      "train loss:0.00014630485079972582\n",
      "train loss:0.003120717967537442\n",
      "train loss:4.5953035338086656e-05\n",
      "train loss:0.0025206199828422416\n",
      "train loss:0.019061567854242643\n",
      "train loss:0.004356936397283197\n",
      "train loss:0.0010292273170215043\n",
      "train loss:0.002053046454848906\n",
      "train loss:0.0024110179146359877\n",
      "train loss:0.0019340317769556393\n",
      "train loss:0.0007911789040212189\n",
      "train loss:0.0010818077124916553\n",
      "train loss:0.00013263738022364186\n",
      "train loss:0.0007099954688155336\n",
      "train loss:0.0005040051463757475\n",
      "train loss:0.0020411592131453463\n",
      "train loss:0.00262213609674637\n",
      "train loss:0.0036524948235813\n",
      "train loss:0.0021371837966345426\n",
      "train loss:0.013407868342886552\n",
      "train loss:0.0026209768677056687\n",
      "train loss:0.0025060508459981063\n",
      "train loss:0.003767637108798062\n",
      "train loss:0.0007134148003925112\n",
      "train loss:0.017421704604567587\n",
      "train loss:0.0044351623522126595\n",
      "train loss:0.0013005535279993457\n",
      "train loss:0.0040619420482807264\n",
      "train loss:0.003154451157643032\n",
      "train loss:0.005409807386229632\n",
      "train loss:0.0010454706263632735\n",
      "train loss:0.002006727102374611\n",
      "train loss:0.0035365514874133603\n",
      "train loss:0.0020439521168782917\n",
      "train loss:0.002927506375912295\n",
      "train loss:0.0046724919493942\n",
      "train loss:0.014972360328485711\n",
      "train loss:0.00413239891517658\n",
      "train loss:0.0008980407464748491\n",
      "train loss:0.004361960013856885\n",
      "train loss:0.002197706454561138\n",
      "train loss:0.0014398126223871249\n",
      "train loss:0.001581939324547196\n",
      "train loss:0.00012792009032752596\n",
      "train loss:0.02979929636127021\n",
      "train loss:0.006899112469642492\n",
      "train loss:0.004230845590544629\n",
      "train loss:0.00015743074466557813\n",
      "train loss:0.0022409287809759424\n",
      "train loss:0.0006334858072673036\n",
      "train loss:0.0003211642742383204\n",
      "train loss:0.004236101950007459\n",
      "train loss:0.016145387952437945\n",
      "train loss:0.0027414915884794062\n",
      "train loss:0.0016807891526335926\n",
      "train loss:0.0018750448437717282\n",
      "train loss:0.003515406940235563\n",
      "train loss:0.0015864868057796089\n",
      "train loss:0.0019216228115081604\n",
      "train loss:0.004502313318337169\n",
      "train loss:0.0002068074027410915\n",
      "train loss:0.0006753541034862038\n",
      "train loss:0.002738776784139847\n",
      "train loss:0.0008738178633082472\n",
      "train loss:0.000692841437228425\n",
      "train loss:0.000998158545365099\n",
      "train loss:0.0014004169305845712\n",
      "train loss:0.00379739677970678\n",
      "train loss:0.0005113722612447783\n",
      "train loss:0.0011678904782292767\n",
      "train loss:0.0004596558400148494\n",
      "train loss:0.0007715380107415613\n",
      "train loss:0.0008288611883764475\n",
      "train loss:0.0024381669808656043\n",
      "train loss:0.05732702856253574\n",
      "train loss:0.0031189913605118246\n",
      "train loss:0.0005471544177885128\n",
      "train loss:0.0028250878290771\n",
      "train loss:0.0012322676364700184\n",
      "train loss:0.0005099442434723687\n",
      "train loss:0.00041207596948028715\n",
      "train loss:0.002880647549945463\n",
      "train loss:0.0028039426611030593\n",
      "train loss:0.0018496113830368387\n",
      "train loss:0.0003114391979798279\n",
      "train loss:0.0009038131789400957\n",
      "train loss:0.00019105541242221403\n",
      "train loss:0.0017795031478771157\n",
      "train loss:0.0021684008662146066\n",
      "train loss:0.0025347349289205528\n",
      "train loss:0.02700772765874853\n",
      "train loss:0.008633051562266063\n",
      "train loss:0.0006925716383107838\n",
      "train loss:0.012491026806197516\n",
      "train loss:0.006045095493583081\n",
      "train loss:0.00030269683718409134\n",
      "train loss:0.0021309648077425794\n",
      "train loss:0.0027993208132496366\n",
      "train loss:0.0024482658718286425\n",
      "train loss:0.0008238740084500909\n",
      "train loss:0.0019577393934152236\n",
      "train loss:0.0019267638019538227\n",
      "train loss:0.0005349161978982149\n",
      "train loss:0.0010925608406526955\n",
      "train loss:0.0007176600713055723\n",
      "train loss:0.03502643185064005\n",
      "train loss:0.0008449268659475024\n",
      "train loss:0.00703002686720553\n",
      "train loss:0.0007240119175725156\n",
      "train loss:0.0027412560852942617\n",
      "train loss:8.677920228022384e-05\n",
      "train loss:0.000445792644473756\n",
      "train loss:0.001127260288941663\n",
      "train loss:0.0009592651348022306\n",
      "train loss:0.002394047759997178\n",
      "train loss:0.0033495436285218565\n",
      "train loss:0.002708825799162301\n",
      "train loss:0.0036434176777181016\n",
      "train loss:0.0011035690371720005\n",
      "train loss:0.00021662319440291463\n",
      "train loss:0.0006595248611430751\n",
      "train loss:0.004150834341997158\n",
      "train loss:0.0007545239821848528\n",
      "train loss:0.0010086718716745898\n",
      "train loss:0.0005386592798541479\n",
      "train loss:0.00672869114633839\n",
      "train loss:0.0011069076288983191\n",
      "train loss:0.007893632186680825\n",
      "train loss:0.0169835679510799\n",
      "train loss:0.0003555835835374412\n",
      "train loss:0.0003200190336089713\n",
      "train loss:0.005546121567828597\n",
      "train loss:0.0007028392074335179\n",
      "train loss:0.004299994371567437\n",
      "train loss:0.0036541142664982913\n",
      "train loss:0.001027601126726293\n",
      "train loss:0.0001430795364394012\n",
      "train loss:0.0029383318538424795\n",
      "train loss:0.0013474144861213588\n",
      "train loss:0.00017496939529238612\n",
      "train loss:0.0018702188524266727\n",
      "train loss:0.006766948624629839\n",
      "train loss:0.0003052149170454762\n",
      "train loss:0.0004709538255454101\n",
      "train loss:0.004323538886640205\n",
      "train loss:0.0020781622307890347\n",
      "train loss:0.0019081376924514643\n",
      "train loss:0.00210726509947616\n",
      "train loss:0.0024033813526078193\n",
      "train loss:0.02106514459595543\n",
      "train loss:0.003445356704688128\n",
      "train loss:0.0004779106616326399\n",
      "train loss:0.00021965116540560936\n",
      "train loss:0.0011844323685499575\n",
      "train loss:0.032594486347391795\n",
      "train loss:0.002285800028700808\n",
      "train loss:0.001691738529466289\n",
      "train loss:0.00156850020096906\n",
      "train loss:0.0020582832489014534\n",
      "train loss:0.0015708834384967948\n",
      "train loss:0.0009170421853701232\n",
      "train loss:0.0001498555756069905\n",
      "train loss:0.0005335426060328279\n",
      "train loss:0.001179778138121777\n",
      "train loss:0.0003792441359262844\n",
      "train loss:0.0016651638685797087\n",
      "train loss:0.0008723479590355206\n",
      "train loss:0.0046630866810305665\n",
      "train loss:0.02074742031378218\n",
      "train loss:0.016961929496485846\n",
      "train loss:0.0004994152990072297\n",
      "train loss:0.009305291557215243\n",
      "train loss:0.0004917760822854866\n",
      "train loss:0.0011891590155909562\n",
      "train loss:0.00019886328743359193\n",
      "train loss:0.000867648250099196\n",
      "train loss:0.0003405538440792214\n",
      "train loss:0.0006007985377048679\n",
      "train loss:0.0009931033547757571\n",
      "train loss:6.543341560755748e-05\n",
      "train loss:0.001611666219151042\n",
      "train loss:0.007645539894137275\n",
      "train loss:0.0014944241171562871\n",
      "train loss:0.0006862586029910669\n",
      "train loss:0.00046943414302435827\n",
      "train loss:0.0016684423877499075\n",
      "train loss:0.0013397379231676052\n",
      "train loss:0.0011046491474315735\n",
      "train loss:0.0003584557302855439\n",
      "train loss:0.028950984739355562\n",
      "train loss:0.004146616502680515\n",
      "train loss:0.0024842448256502604\n",
      "train loss:0.004177441484565007\n",
      "train loss:0.0013716933358482793\n",
      "train loss:0.0019481849071297042\n",
      "train loss:0.0011035585137126542\n",
      "train loss:0.0007912696412227404\n",
      "train loss:0.002454771386458622\n",
      "train loss:0.0003405850794274437\n",
      "train loss:0.0036198981916073297\n",
      "train loss:0.0020503862040173277\n",
      "train loss:0.005359511890568602\n",
      "train loss:0.0045869824874712655\n",
      "train loss:0.0013110205848689516\n",
      "train loss:0.004204870438166442\n",
      "train loss:0.061852258671455926\n",
      "train loss:0.0024720483099295726\n",
      "train loss:0.0002822163924156172\n",
      "train loss:0.002802344848966765\n",
      "train loss:0.0007552631744243551\n",
      "train loss:0.007490421014084847\n",
      "train loss:0.002460214149926774\n",
      "train loss:0.017612598809959516\n",
      "train loss:0.0006884435297381308\n",
      "train loss:0.0005412688016515568\n",
      "train loss:0.0029171207186075372\n",
      "train loss:0.00033150331028098165\n",
      "train loss:0.00016526278520665827\n",
      "train loss:0.0008094976414760951\n",
      "train loss:0.0024438746778654434\n",
      "train loss:0.0028248525956572155\n",
      "train loss:0.007108313827306252\n",
      "train loss:0.0025290936035327637\n",
      "train loss:0.0008851091973063639\n",
      "train loss:0.00289707559320909\n",
      "train loss:0.004175023281004814\n",
      "train loss:0.005798400762855553\n",
      "train loss:0.009166002116603195\n",
      "train loss:0.010614571423099017\n",
      "train loss:0.003852537942376093\n",
      "train loss:0.00010780195806242008\n",
      "train loss:0.0014451982697554233\n",
      "train loss:0.0017525177342500753\n",
      "train loss:0.00161401221900231\n",
      "train loss:0.0006188736759595082\n",
      "train loss:0.002027472291184918\n",
      "train loss:0.0011191143238959032\n",
      "train loss:0.00583673043661206\n",
      "train loss:0.0008439660721381135\n",
      "train loss:0.002111554830785683\n",
      "train loss:0.0012887085005440014\n",
      "train loss:0.0007331814951965111\n",
      "train loss:0.0004822168132713112\n",
      "train loss:0.0051253308914682326\n",
      "train loss:0.0034124951210363756\n",
      "train loss:0.004593380193960155\n",
      "train loss:0.00020811551792086208\n",
      "train loss:0.004560508214518631\n",
      "train loss:0.0017550789367287952\n",
      "train loss:0.00048645006720990104\n",
      "train loss:0.001086595082592322\n",
      "train loss:0.0030450899884193606\n",
      "train loss:0.00046574534509589814\n",
      "train loss:0.0017071080776859455\n",
      "train loss:0.0035061391378518637\n",
      "train loss:0.0008999875321413514\n",
      "train loss:0.0012670822970363425\n",
      "train loss:0.014816738583858619\n",
      "train loss:0.005196623748773574\n",
      "train loss:0.001179189586122619\n",
      "train loss:0.00036298455633859364\n",
      "train loss:0.0026666306185116105\n",
      "train loss:0.005268711844909187\n",
      "train loss:0.007483557970032867\n",
      "train loss:0.0012163826953114972\n",
      "train loss:0.0007389233350867877\n",
      "train loss:0.0002096511544789612\n",
      "train loss:0.0016253630549787263\n",
      "train loss:0.001673347840420208\n",
      "train loss:0.0017458783675427525\n",
      "train loss:0.0017842731366056641\n",
      "train loss:0.0009449425217665139\n",
      "train loss:0.002680409700521641\n",
      "train loss:0.02953603127905316\n",
      "train loss:0.004418109870895836\n",
      "train loss:0.002554908538706988\n",
      "train loss:0.00026389935161597175\n",
      "train loss:0.0021232634000796996\n",
      "train loss:0.006445731550265209\n",
      "train loss:0.005299733527422786\n",
      "train loss:0.002642893315348546\n",
      "train loss:0.002395543563876714\n",
      "train loss:0.004050596979107305\n",
      "train loss:0.00360020539171491\n",
      "train loss:0.0005193049209111217\n",
      "train loss:0.0007135940754890643\n",
      "train loss:0.00072980955584931\n",
      "train loss:0.001603302509017681\n",
      "train loss:0.00168173096229207\n",
      "train loss:0.00307145213921105\n",
      "train loss:0.0005215199339131997\n",
      "train loss:0.005377483002898306\n",
      "train loss:0.002192029987052042\n",
      "train loss:0.0026236186876788686\n",
      "train loss:0.0023213627287447004\n",
      "train loss:0.0010284483019226724\n",
      "train loss:0.0006210707727908764\n",
      "train loss:0.0010544304722549468\n",
      "train loss:0.0050573470675864315\n",
      "train loss:0.0018828323000463177\n",
      "train loss:0.0022238715795535446\n",
      "train loss:0.008489726295711241\n",
      "train loss:0.0014476747961664747\n",
      "train loss:0.0008055172368446168\n",
      "train loss:0.00031729682339356286\n",
      "train loss:0.0005773913899552917\n",
      "train loss:0.0019900446153422752\n",
      "train loss:0.006833902497142268\n",
      "train loss:0.001319185783645323\n",
      "train loss:0.0008948561964421468\n",
      "train loss:0.0015420196076772188\n",
      "train loss:0.003464968594602999\n",
      "train loss:0.0017292601557659494\n",
      "train loss:0.00035988708576086757\n",
      "train loss:0.002805899624512139\n",
      "train loss:0.0019381144219846407\n",
      "train loss:0.04130242604632157\n",
      "train loss:0.000506253357887978\n",
      "train loss:0.0014800415982071235\n",
      "train loss:0.0014865694130061016\n",
      "train loss:0.012649576688722283\n",
      "train loss:0.0018981704788935785\n",
      "train loss:0.0004359465528691432\n",
      "train loss:0.00033849254488776373\n",
      "train loss:0.0005432219885785746\n",
      "train loss:0.0015807033759927484\n",
      "train loss:0.001398469170783951\n",
      "train loss:9.248531091097246e-05\n",
      "train loss:0.0028569990365020503\n",
      "train loss:0.0002554709682703833\n",
      "train loss:9.966686854442244e-05\n",
      "train loss:0.002787969879796027\n",
      "train loss:0.004617492951411065\n",
      "train loss:0.0012399264051652793\n",
      "train loss:0.002034641343604951\n",
      "train loss:0.001168832728941819\n",
      "train loss:1.638902427840118e-05\n",
      "train loss:0.0018523044767454469\n",
      "train loss:0.0009948577351281587\n",
      "train loss:0.006532909256460436\n",
      "train loss:0.0035674661813425647\n",
      "train loss:0.0005976797379922753\n",
      "train loss:0.0006394370203264434\n",
      "train loss:0.00041660274112986654\n",
      "train loss:0.025919047650408356\n",
      "train loss:0.0011138538235682003\n",
      "train loss:0.00010471505579869003\n",
      "train loss:0.0007650172829350055\n",
      "train loss:0.011026709338120992\n",
      "train loss:0.0014864128894987432\n",
      "train loss:0.0036849786945786205\n",
      "train loss:7.444995712069003e-05\n",
      "train loss:0.0006416442050289008\n",
      "train loss:0.0014846642180255409\n",
      "train loss:0.0014499923125251809\n",
      "train loss:0.0025597050330783185\n",
      "train loss:0.0036217822498729392\n",
      "train loss:0.0004425393525769449\n",
      "train loss:0.0011344595949600487\n",
      "train loss:0.0028735031483195688\n",
      "train loss:0.0004323608333301504\n",
      "train loss:0.0013520607942759536\n",
      "train loss:0.0020756745431238753\n",
      "train loss:0.00039024758686143096\n",
      "train loss:0.0001564448596841303\n",
      "train loss:0.0031126407645793914\n",
      "train loss:0.002544268140924975\n",
      "train loss:0.0002088480546775426\n",
      "train loss:0.0002585883044851534\n",
      "train loss:0.0008928548644868733\n",
      "train loss:0.0006493335165724439\n",
      "train loss:0.0009297612921607295\n",
      "train loss:0.0003023333968563007\n",
      "train loss:0.000627796378809123\n",
      "train loss:6.31416231453591e-05\n",
      "train loss:9.098285852002038e-05\n",
      "train loss:5.8763030183738436e-05\n",
      "train loss:0.00017247464712502464\n",
      "train loss:0.0014755531898038223\n",
      "train loss:0.0008712518479707011\n",
      "train loss:0.0007457017400050902\n",
      "train loss:0.008872343897909607\n",
      "train loss:0.0033711423024585505\n",
      "train loss:0.00011678429062067111\n",
      "train loss:0.002352720624114759\n",
      "train loss:0.0005363481837444481\n",
      "train loss:0.00017817919378611236\n",
      "train loss:0.005366294082671955\n",
      "train loss:0.0002813574601859152\n",
      "train loss:0.004198235852643551\n",
      "train loss:0.004937343435959565\n",
      "train loss:0.0023359113505566167\n",
      "train loss:0.002975968230412116\n",
      "train loss:0.0011614871239685744\n",
      "train loss:0.004827358472444743\n",
      "train loss:0.0011669388529530588\n",
      "train loss:0.0007359010815031902\n",
      "train loss:0.00027444532640866703\n",
      "train loss:0.0019691068675058874\n",
      "train loss:0.0017824149246618128\n",
      "train loss:0.0046804237506022865\n",
      "train loss:0.0003986600304241698\n",
      "train loss:0.0003462436926910808\n",
      "train loss:0.0027594230203487535\n",
      "train loss:0.004278400902317622\n",
      "train loss:0.0003636615691489197\n",
      "train loss:0.00014221761105607245\n",
      "train loss:0.00037197568328537704\n",
      "train loss:0.0004483710770880921\n",
      "train loss:0.001096643085416314\n",
      "train loss:0.00062259819555744\n",
      "train loss:0.0018243103727713782\n",
      "train loss:0.004613016131998672\n",
      "train loss:0.00034318786816594745\n",
      "train loss:0.0005702340205325221\n",
      "train loss:0.0042453647272950165\n",
      "train loss:0.0027082437826909025\n",
      "train loss:0.0006574150518913356\n",
      "train loss:0.001243463060815628\n",
      "train loss:0.027704485840616435\n",
      "train loss:0.0023827732259411433\n",
      "train loss:0.0013839599878884992\n",
      "train loss:0.0006644060129966561\n",
      "train loss:0.00036122702570173836\n",
      "train loss:0.006616812931783213\n",
      "train loss:0.003674707702305798\n",
      "train loss:0.0007647279607344105\n",
      "train loss:0.0014693321493036288\n",
      "train loss:0.0008567791392739386\n",
      "train loss:0.0011488088860839509\n",
      "train loss:2.975688240481905e-05\n",
      "train loss:4.880065913608943e-05\n",
      "train loss:9.628713821350632e-05\n",
      "train loss:0.001315064808267907\n",
      "train loss:0.000729769444431274\n",
      "train loss:0.00011072624639710575\n",
      "train loss:0.0006561550723672399\n",
      "train loss:0.0012157962024834281\n",
      "train loss:0.05627447530264039\n",
      "train loss:0.0007892660017759924\n",
      "train loss:0.0005478603873228776\n",
      "train loss:0.003120775869294099\n",
      "train loss:0.004221556055440327\n",
      "train loss:0.005500934742230512\n",
      "train loss:0.004587096936630005\n",
      "train loss:0.0015948874813265256\n",
      "train loss:0.0004383609815842968\n",
      "train loss:0.002802362038918377\n",
      "train loss:0.0016962068649914392\n",
      "train loss:0.0005209016569466443\n",
      "train loss:0.004200752278862097\n",
      "train loss:0.0044962343749625705\n",
      "train loss:0.00139684738491013\n",
      "train loss:0.0003532906385979716\n",
      "train loss:0.00013888888162235586\n",
      "train loss:0.00033466912028478886\n",
      "train loss:0.0020905761568947756\n",
      "train loss:0.0005540890135921427\n",
      "train loss:0.0038394500945298842\n",
      "train loss:0.0007647685278051519\n",
      "train loss:0.0007147224947843145\n",
      "train loss:0.0005145944522152724\n",
      "train loss:0.004390571766204044\n",
      "train loss:0.0008194310685806916\n",
      "train loss:0.002109907141159889\n",
      "train loss:0.02647893353320867\n",
      "train loss:0.0030216572257394463\n",
      "train loss:0.002206792570542194\n",
      "train loss:0.0014535455063024976\n",
      "train loss:0.0002328989321818232\n",
      "train loss:0.0025153531607539476\n",
      "train loss:0.0004000169228467988\n",
      "train loss:0.00012313147485424353\n",
      "train loss:0.0004575238023309702\n",
      "train loss:0.0020061762069671822\n",
      "train loss:0.00041366788498847985\n",
      "train loss:0.001152336712517188\n",
      "train loss:0.0028362538949081233\n",
      "train loss:0.005013443556387783\n",
      "train loss:0.011938669272850371\n",
      "train loss:0.0012978069402669412\n",
      "train loss:0.00738604554973885\n",
      "train loss:0.0026004153792634627\n",
      "train loss:1.3052434363665537e-05\n",
      "train loss:0.0005088402709270833\n",
      "train loss:0.0026864919326688545\n",
      "train loss:0.00035085813659262334\n",
      "train loss:0.01229744231952095\n",
      "train loss:0.003212981631732692\n",
      "train loss:0.0001478997621347128\n",
      "train loss:0.0018534146537714305\n",
      "train loss:0.000511138651418882\n",
      "train loss:0.001222184121980129\n",
      "train loss:0.0011017141446226903\n",
      "train loss:0.002453375791585931\n",
      "train loss:0.0012086461099993508\n",
      "train loss:0.004806440147762047\n",
      "train loss:0.0024263174641319836\n",
      "train loss:0.0031141553143740758\n",
      "train loss:0.0011089553678625753\n",
      "train loss:0.00024208058708839609\n",
      "train loss:0.00044787435538405797\n",
      "train loss:0.0009243157375589103\n",
      "train loss:0.00031187000134352724\n",
      "train loss:0.003228614750145512\n",
      "train loss:0.002506862299801852\n",
      "train loss:0.0021790611655695636\n",
      "train loss:0.00093764772116824\n",
      "train loss:0.0035723570413916934\n",
      "train loss:0.000747866202573\n",
      "train loss:0.00013975463536484235\n",
      "train loss:0.0007283655498959262\n",
      "train loss:0.00255716967778531\n",
      "train loss:0.001384049197324055\n",
      "train loss:0.004115630084996726\n",
      "train loss:0.0002938734997035532\n",
      "train loss:0.00037495713513706515\n",
      "train loss:0.00015439478094740763\n",
      "train loss:0.000844519555433266\n",
      "train loss:0.0020056122100239825\n",
      "train loss:0.0008640801760789456\n",
      "=== epoch:17, train acc:0.998, test acc:0.985 ===\n",
      "train loss:0.0018526511521924928\n",
      "train loss:0.0001015416097964588\n",
      "train loss:0.0002069594996452998\n",
      "train loss:0.0010054857855858455\n",
      "train loss:0.0013420174686043932\n",
      "train loss:0.0006362997400391621\n",
      "train loss:0.0013233121987726638\n",
      "train loss:0.0016849748423199307\n",
      "train loss:0.002583038029541776\n",
      "train loss:0.002792292416622609\n",
      "train loss:0.0004097674259192984\n",
      "train loss:0.00794062736841205\n",
      "train loss:0.002096400531384113\n",
      "train loss:0.002025592462081871\n",
      "train loss:0.0018050951612028777\n",
      "train loss:0.00037295043361980417\n",
      "train loss:0.00048594931517972754\n",
      "train loss:0.0008515400131866561\n",
      "train loss:0.0014025606022590124\n",
      "train loss:0.0014289815495591956\n",
      "train loss:0.0008076566918777078\n",
      "train loss:0.0030771873531528255\n",
      "train loss:0.001669660327115745\n",
      "train loss:0.000336360623486141\n",
      "train loss:0.0010425957286939694\n",
      "train loss:0.0016681318051154736\n",
      "train loss:0.0008553257451192478\n",
      "train loss:0.0007344245170062645\n",
      "train loss:0.00010714857180791764\n",
      "train loss:0.0006698733727145195\n",
      "train loss:0.00215284629794518\n",
      "train loss:0.0021369581891931954\n",
      "train loss:0.006143864695931352\n",
      "train loss:0.00035353276668767677\n",
      "train loss:0.0007825704598442706\n",
      "train loss:0.00029244128943568885\n",
      "train loss:4.1193310316630536e-05\n",
      "train loss:0.0014597241413429021\n",
      "train loss:0.004340905978726269\n",
      "train loss:0.003840830041976493\n",
      "train loss:0.00014589197559895515\n",
      "train loss:1.0733035989006626e-05\n",
      "train loss:0.0011198146475858003\n",
      "train loss:0.004339708033166379\n",
      "train loss:0.0001082080990150465\n",
      "train loss:0.00032681610077591296\n",
      "train loss:0.001317299788507\n",
      "train loss:0.0023186781572318072\n",
      "train loss:5.736019071289756e-05\n",
      "train loss:0.0018776661732968008\n",
      "train loss:0.0005860731554434964\n",
      "train loss:0.00022045015850856016\n",
      "train loss:0.00011599590134908226\n",
      "train loss:0.00040439877962958023\n",
      "train loss:0.0013982001191813118\n",
      "train loss:0.0035413199809319253\n",
      "train loss:9.869243004105966e-05\n",
      "train loss:0.0002984699244947072\n",
      "train loss:2.613771934150811e-05\n",
      "train loss:0.005348849090438668\n",
      "train loss:0.0004938793104989691\n",
      "train loss:0.0002118744968131957\n",
      "train loss:0.0030913150201201073\n",
      "train loss:0.00025706983223823326\n",
      "train loss:0.0006198981018556458\n",
      "train loss:0.0038794906331181423\n",
      "train loss:0.0008819936180965714\n",
      "train loss:0.008852952392217528\n",
      "train loss:0.000515342434919443\n",
      "train loss:0.002762428933441247\n",
      "train loss:0.0009567692434742723\n",
      "train loss:0.001621010342836876\n",
      "train loss:0.0005232276251922588\n",
      "train loss:0.0023119632924045452\n",
      "train loss:0.00028656793397868074\n",
      "train loss:0.0016952031084252015\n",
      "train loss:0.005549019599302932\n",
      "train loss:0.004137324218457184\n",
      "train loss:0.00025932128719096493\n",
      "train loss:0.0002495237522371319\n",
      "train loss:0.0013754844847381547\n",
      "train loss:0.00046163514371854494\n",
      "train loss:0.0010346909743731625\n",
      "train loss:9.623507024544408e-05\n",
      "train loss:0.0004493077245230575\n",
      "train loss:0.0007336879624070944\n",
      "train loss:0.0028795931076943692\n",
      "train loss:0.0019254044431179108\n",
      "train loss:0.018986431608964415\n",
      "train loss:0.0003358003390450722\n",
      "train loss:0.0012793778100680683\n",
      "train loss:0.0013145633579586169\n",
      "train loss:0.005552009909033055\n",
      "train loss:0.0023347144674743816\n",
      "train loss:0.001366105806839678\n",
      "train loss:0.002834121583700159\n",
      "train loss:0.004077561568659213\n",
      "train loss:0.00046419995730690957\n",
      "train loss:0.0022640002547758397\n",
      "train loss:0.00038607442180196773\n",
      "train loss:0.0009233254455612616\n",
      "train loss:0.00040174279453760467\n",
      "train loss:0.0021417746360449692\n",
      "train loss:0.005724987456080023\n",
      "train loss:0.0002732503644921578\n",
      "train loss:0.002596538986805054\n",
      "train loss:0.004976205892039845\n",
      "train loss:0.002946924572627424\n",
      "train loss:0.003075597546449552\n",
      "train loss:0.0005646387016823649\n",
      "train loss:0.00022303164213545817\n",
      "train loss:0.006361455856355328\n",
      "train loss:0.0004092838384705266\n",
      "train loss:0.004802139314608777\n",
      "train loss:0.012383024372213274\n",
      "train loss:0.003729447017433776\n",
      "train loss:0.0011693143508133828\n",
      "train loss:0.0008550302056051884\n",
      "train loss:0.000133540891128448\n",
      "train loss:0.0053446233039352985\n",
      "train loss:4.117017158259527e-05\n",
      "train loss:0.0001671893051651008\n",
      "train loss:0.00021159654995731822\n",
      "train loss:0.00018864848095269886\n",
      "train loss:0.0006053716599875506\n",
      "train loss:0.0037577218281188607\n",
      "train loss:0.0031383748402468224\n",
      "train loss:0.0002021764746267994\n",
      "train loss:9.6304793978655e-05\n",
      "train loss:0.009003094946466955\n",
      "train loss:0.0021798574120540544\n",
      "train loss:0.0007434018993208115\n",
      "train loss:0.0015588552247686063\n",
      "train loss:3.1054510819355156e-05\n",
      "train loss:0.013284452909631393\n",
      "train loss:0.0002502520654070473\n",
      "train loss:0.001708662816476312\n",
      "train loss:0.0013718717516572035\n",
      "train loss:0.00030321671249788196\n",
      "train loss:0.0013819798810245012\n",
      "train loss:0.006615291715719515\n",
      "train loss:0.001301068079023655\n",
      "train loss:0.0011107029355075294\n",
      "train loss:0.0004017615241577438\n",
      "train loss:0.001409294880649311\n",
      "train loss:0.00013077951570739305\n",
      "train loss:0.000662696012266114\n",
      "train loss:0.004362259509800034\n",
      "train loss:0.005064989666371835\n",
      "train loss:0.0008206441594826859\n",
      "train loss:0.00041412106979877043\n",
      "train loss:0.0012214234163173015\n",
      "train loss:0.0034642190219988183\n",
      "train loss:0.0005006381732015095\n",
      "train loss:0.004846785571557216\n",
      "train loss:0.0015451768311204696\n",
      "train loss:0.0002555253852002417\n",
      "train loss:0.0021372544500309703\n",
      "train loss:0.0030959050127899916\n",
      "train loss:0.0009324919778075944\n",
      "train loss:0.001108701662576608\n",
      "train loss:0.0009742250691608076\n",
      "train loss:0.0032807437934087046\n",
      "train loss:0.001531955539968963\n",
      "train loss:0.00016408594156111668\n",
      "train loss:0.0025248580267257727\n",
      "train loss:0.00026827828739973633\n",
      "train loss:0.002465339948021756\n",
      "train loss:0.0002450112601970282\n",
      "train loss:0.00033176868120121536\n",
      "train loss:0.00014400429759021081\n",
      "train loss:0.00454285366912364\n",
      "train loss:0.0012213702972433704\n",
      "train loss:0.001619409464149818\n",
      "train loss:0.0019792615318424338\n",
      "train loss:0.0017976963004401368\n",
      "train loss:0.003206458025550393\n",
      "train loss:0.0010714256840304876\n",
      "train loss:0.00041072808212090466\n",
      "train loss:6.905590010902746e-05\n",
      "train loss:0.0042579110050884085\n",
      "train loss:0.0025898604519114347\n",
      "train loss:0.0004650528469746062\n",
      "train loss:0.002502804600481566\n",
      "train loss:0.007588913992438768\n",
      "train loss:0.003180137346411675\n",
      "train loss:0.0008549430622116448\n",
      "train loss:0.0003868301852259326\n",
      "train loss:0.002113567559826936\n",
      "train loss:0.0009126580945345396\n",
      "train loss:0.0008836642166876092\n",
      "train loss:0.00015407324816343537\n",
      "train loss:0.00824822460247404\n",
      "train loss:0.004844054713601092\n",
      "train loss:0.004431401891319788\n",
      "train loss:0.03655783887496768\n",
      "train loss:0.00030496458434065373\n",
      "train loss:0.005053178297956794\n",
      "train loss:0.004689840507010786\n",
      "train loss:0.002054054620834162\n",
      "train loss:0.001167448935355616\n",
      "train loss:0.00036663523230740706\n",
      "train loss:0.002002004124991581\n",
      "train loss:0.0026602090198565866\n",
      "train loss:0.005927493661174702\n",
      "train loss:0.0021003030084101263\n",
      "train loss:0.009902021978342123\n",
      "train loss:0.0011955355657535841\n",
      "train loss:0.002850597755854806\n",
      "train loss:0.0008709189325967634\n",
      "train loss:0.0012429643136156216\n",
      "train loss:0.0013139119143525768\n",
      "train loss:0.003266640359377373\n",
      "train loss:0.00026024047064525694\n",
      "train loss:0.00013949279878703252\n",
      "train loss:0.0004557969043671617\n",
      "train loss:0.00022658438395696472\n",
      "train loss:0.0008269070951260926\n",
      "train loss:0.0016949274417805977\n",
      "train loss:0.0013041261048078985\n",
      "train loss:0.005420796166241058\n",
      "train loss:0.00215780666247591\n",
      "train loss:4.431999806322813e-05\n",
      "train loss:0.00034918704311424366\n",
      "train loss:0.00021811748029499048\n",
      "train loss:0.00021250440890865671\n",
      "train loss:0.011495993598950314\n",
      "train loss:0.007547293050363364\n",
      "train loss:0.003574931557243221\n",
      "train loss:0.004031374144847554\n",
      "train loss:0.0010091326291706759\n",
      "train loss:0.0001996964409675086\n",
      "train loss:0.00011282004896938526\n",
      "train loss:0.0005306588305302403\n",
      "train loss:0.005797173538485642\n",
      "train loss:0.006269413391145733\n",
      "train loss:0.00033589221299235505\n",
      "train loss:0.006696146919494653\n",
      "train loss:0.005252711883743124\n",
      "train loss:0.001236368577839346\n",
      "train loss:0.002321703650552446\n",
      "train loss:0.0033944442729380393\n",
      "train loss:0.0027015369808465074\n",
      "train loss:0.005460910001995267\n",
      "train loss:0.0034593002078625473\n",
      "train loss:0.0013356787010491088\n",
      "train loss:0.00035564824915458056\n",
      "train loss:0.0021169914067061626\n",
      "train loss:0.0020353581735715054\n",
      "train loss:0.00070671860801943\n",
      "train loss:0.0003363498108483716\n",
      "train loss:0.00017730339303268232\n",
      "train loss:0.02146977048801103\n",
      "train loss:0.004555319369830544\n",
      "train loss:0.0022275877478544234\n",
      "train loss:0.001386339390878691\n",
      "train loss:9.463987192698179e-05\n",
      "train loss:0.000343749507759388\n",
      "train loss:0.003184907013512598\n",
      "train loss:0.0005325814377332316\n",
      "train loss:0.01123333255957055\n",
      "train loss:0.0003889107877726629\n",
      "train loss:0.00020186007560898494\n",
      "train loss:0.00031822391851910254\n",
      "train loss:0.004250153515216887\n",
      "train loss:0.00038574448208814635\n",
      "train loss:0.000941163109714032\n",
      "train loss:0.00021846989843284797\n",
      "train loss:9.85606671521753e-05\n",
      "train loss:0.0005932525388752924\n",
      "train loss:0.0011575087997809518\n",
      "train loss:0.00015805695424239916\n",
      "train loss:0.0006874073798523199\n",
      "train loss:0.011395570002692765\n",
      "train loss:0.0005588330453393603\n",
      "train loss:5.843394288037933e-05\n",
      "train loss:0.002343682758947874\n",
      "train loss:0.0006659045588923694\n",
      "train loss:5.7588067987070785e-05\n",
      "train loss:0.0054853584072488215\n",
      "train loss:0.0009594003821851017\n",
      "train loss:0.00010297616826937501\n",
      "train loss:0.0012468167933749556\n",
      "train loss:0.0005958109807986719\n",
      "train loss:0.0004327900736402373\n",
      "train loss:0.003104420177141062\n",
      "train loss:0.00018010133132349615\n",
      "train loss:0.005789970699248342\n",
      "train loss:0.00053673315244653\n",
      "train loss:0.0010755425327176737\n",
      "train loss:0.0023669346759586213\n",
      "train loss:0.01248942282852723\n",
      "train loss:0.0007751408105824635\n",
      "train loss:0.0006098729826282284\n",
      "train loss:0.0011197384830314715\n",
      "train loss:0.0012331848050032065\n",
      "train loss:0.002515729331387655\n",
      "train loss:0.017988357247337678\n",
      "train loss:0.004222546248356219\n",
      "train loss:0.0007682602744074266\n",
      "train loss:0.0003641094614917663\n",
      "train loss:0.0032478795107794118\n",
      "train loss:0.00048077989095284083\n",
      "train loss:0.0011658550727381256\n",
      "train loss:0.0001452031134511091\n",
      "train loss:0.000755070900987276\n",
      "train loss:0.0003980146564850896\n",
      "train loss:0.007751634103076455\n",
      "train loss:0.0017106292092856105\n",
      "train loss:0.0003407247039156547\n",
      "train loss:0.0011844720391850508\n",
      "train loss:0.0004805020952298367\n",
      "train loss:0.0006721041782044018\n",
      "train loss:0.0016058339657863884\n",
      "train loss:0.005591614759500555\n",
      "train loss:0.0003585766839895932\n",
      "train loss:0.0014239872826565447\n",
      "train loss:0.0012150042615854278\n",
      "train loss:0.0007082958999723965\n",
      "train loss:0.021042537102803967\n",
      "train loss:0.002003783721537576\n",
      "train loss:0.0003578816259862506\n",
      "train loss:0.0006706699856755589\n",
      "train loss:0.0035214565894935816\n",
      "train loss:0.00692822940680587\n",
      "train loss:0.00015035758887477403\n",
      "train loss:0.0001824010290890933\n",
      "train loss:4.500161917735488e-05\n",
      "train loss:0.0030780427515172864\n",
      "train loss:0.0013884528838002477\n",
      "train loss:0.009713634716013591\n",
      "train loss:0.0023697817313739385\n",
      "train loss:0.00251353312882416\n",
      "train loss:0.001312445638945129\n",
      "train loss:0.0010540561613964741\n",
      "train loss:2.8231817365552873e-05\n",
      "train loss:0.0017308809722841345\n",
      "train loss:0.0001500161400202049\n",
      "train loss:0.002860161310142119\n",
      "train loss:0.003107664497715655\n",
      "train loss:0.0005138879637264181\n",
      "train loss:0.0011681034235034807\n",
      "train loss:0.028012840365268882\n",
      "train loss:0.00033804985642040285\n",
      "train loss:0.0038771415191601443\n",
      "train loss:0.001333920050272902\n",
      "train loss:0.0008389796318634704\n",
      "train loss:0.0012624799957334314\n",
      "train loss:0.0005843917687012809\n",
      "train loss:5.3759782150298065e-05\n",
      "train loss:0.000630413513235355\n",
      "train loss:0.00034821011087297524\n",
      "train loss:0.00013738360130191438\n",
      "train loss:0.0006057660031202997\n",
      "train loss:0.001459369224058987\n",
      "train loss:3.710611603060556e-05\n",
      "train loss:0.0007324079257947111\n",
      "train loss:0.00428471467046592\n",
      "train loss:0.02018999395854002\n",
      "train loss:0.000890905350732656\n",
      "train loss:0.0014366824993135101\n",
      "train loss:0.0016059680477826346\n",
      "train loss:6.151795507585636e-05\n",
      "train loss:0.0012888436078213256\n",
      "train loss:0.0010295335135002523\n",
      "train loss:3.6034237957116076e-05\n",
      "train loss:0.00037963183201634147\n",
      "train loss:0.0034446380169388803\n",
      "train loss:0.003927745981173945\n",
      "train loss:0.05723598650421957\n",
      "train loss:0.0011144467429425192\n",
      "train loss:0.00022933694225042882\n",
      "train loss:0.0010858748296145048\n",
      "train loss:0.0018187628937571068\n",
      "train loss:0.0011634781705413026\n",
      "train loss:0.0021508602647845126\n",
      "train loss:8.761197334696904e-05\n",
      "train loss:0.001193415371666929\n",
      "train loss:0.0011073962625870054\n",
      "train loss:0.003549883965404149\n",
      "train loss:0.0011063278900399905\n",
      "train loss:0.002916421686290935\n",
      "train loss:0.013360859406635128\n",
      "train loss:0.0003085913408920359\n",
      "train loss:0.00014592467061336835\n",
      "train loss:0.0008301769778102573\n",
      "train loss:0.0003716484048921429\n",
      "train loss:0.0013190575340236623\n",
      "train loss:0.0033011356321118097\n",
      "train loss:7.685117103074625e-05\n",
      "train loss:0.0003474246487561837\n",
      "train loss:0.000717373126772611\n",
      "train loss:0.0012597220206140372\n",
      "train loss:0.0001101631204912325\n",
      "train loss:0.00012188911143019206\n",
      "train loss:0.0009181292898704851\n",
      "train loss:0.0007340106819196302\n",
      "train loss:0.0032400175698055044\n",
      "train loss:0.0025823295084695607\n",
      "train loss:0.00029035045488668586\n",
      "train loss:0.0026333754182446883\n",
      "train loss:0.006985895091643459\n",
      "train loss:0.0005120914420414744\n",
      "train loss:0.0012231896675735203\n",
      "train loss:0.0003669391406405574\n",
      "train loss:0.0007514918833195637\n",
      "train loss:0.0014032232666397156\n",
      "train loss:0.0005326052729547847\n",
      "train loss:0.004065435440467049\n",
      "train loss:0.0016991275557987357\n",
      "train loss:0.006137055845750854\n",
      "train loss:0.00021040004189021793\n",
      "train loss:0.0005762244608652445\n",
      "train loss:0.0004975361063859922\n",
      "train loss:0.0008748410757523989\n",
      "train loss:0.0016418815521848948\n",
      "train loss:0.018577108539583043\n",
      "train loss:0.00040006326991778244\n",
      "train loss:0.0004338304969301914\n",
      "train loss:0.0016194534429785296\n",
      "train loss:0.010017663242584556\n",
      "train loss:0.0014594112679470839\n",
      "train loss:0.003806446833071995\n",
      "train loss:0.0017850139615769184\n",
      "train loss:0.0022736232272364282\n",
      "train loss:0.003593543577568784\n",
      "train loss:0.0004367337684999801\n",
      "train loss:0.0016668477686487607\n",
      "train loss:0.0004917387713289431\n",
      "train loss:0.0024975506116903766\n",
      "train loss:0.0006553865852431728\n",
      "train loss:0.003239964726686746\n",
      "train loss:0.0013134368696628299\n",
      "train loss:0.00020568320469311976\n",
      "train loss:3.316017743507088e-05\n",
      "train loss:0.0009680522771180157\n",
      "train loss:0.007310495148428846\n",
      "train loss:0.0003441941474360005\n",
      "train loss:0.0010821948468836546\n",
      "train loss:0.00019751627857614307\n",
      "train loss:0.0006954530063703795\n",
      "train loss:0.0021155228090156478\n",
      "train loss:0.0005820774095564784\n",
      "train loss:0.0026294074298275293\n",
      "train loss:0.0042712905405024775\n",
      "train loss:0.0013547523178125134\n",
      "train loss:0.0024446554626330504\n",
      "train loss:0.0005684423843119446\n",
      "train loss:0.0014804992610332368\n",
      "train loss:0.002568786933920875\n",
      "train loss:0.0010629989431342273\n",
      "train loss:0.00015611110211151863\n",
      "train loss:0.05746506558731088\n",
      "train loss:0.005336188454520053\n",
      "train loss:0.005880250617079374\n",
      "train loss:0.00014273184535086084\n",
      "train loss:0.005054840429320389\n",
      "train loss:0.0007126563197580877\n",
      "train loss:0.0012619101425371858\n",
      "train loss:0.0007597906038401547\n",
      "train loss:0.00028014798486265634\n",
      "train loss:0.000619552386071539\n",
      "train loss:0.0002784559587314835\n",
      "train loss:0.0006065803138236819\n",
      "train loss:0.000856920946946669\n",
      "train loss:0.0006869320620024315\n",
      "train loss:0.00022723924869232017\n",
      "train loss:0.0008482721908661914\n",
      "train loss:0.001235606216992101\n",
      "train loss:0.0028911656316601892\n",
      "train loss:0.025130544944414735\n",
      "train loss:9.507302979272629e-05\n",
      "train loss:0.011251644852880558\n",
      "train loss:2.149850498028544e-05\n",
      "train loss:0.003538297695192365\n",
      "train loss:0.003052410590717961\n",
      "train loss:0.0002389687547129743\n",
      "train loss:0.0004114858122039073\n",
      "train loss:0.0016287339769976406\n",
      "train loss:5.53655440564894e-05\n",
      "train loss:0.00028315473032185093\n",
      "train loss:0.0003885234013229293\n",
      "train loss:0.0007455473250485618\n",
      "train loss:0.00011568915557321196\n",
      "train loss:0.0011289770594279352\n",
      "train loss:5.338455781286656e-05\n",
      "train loss:0.004610951888946678\n",
      "train loss:0.0004990983681574052\n",
      "train loss:0.0001082877180730183\n",
      "train loss:0.0007202419858237986\n",
      "train loss:7.233478894036622e-05\n",
      "train loss:0.0005246457983776121\n",
      "train loss:0.0020304643975639722\n",
      "train loss:0.005125248412417325\n",
      "train loss:0.0014121012512151112\n",
      "train loss:0.01854864342973333\n",
      "train loss:0.005662908878945996\n",
      "train loss:0.000891598175094813\n",
      "train loss:0.006075195210896235\n",
      "train loss:0.0012016528807880051\n",
      "train loss:0.0018348912171357165\n",
      "train loss:0.0012199370223829427\n",
      "train loss:0.0011629028613446624\n",
      "train loss:0.0017634831542160207\n",
      "train loss:0.00014864594166207127\n",
      "train loss:0.0012915329357165183\n",
      "train loss:0.0007940686926257491\n",
      "train loss:0.0015232189195078568\n",
      "train loss:0.0005417567129421529\n",
      "train loss:0.00768913008033602\n",
      "train loss:0.0005513870282054081\n",
      "train loss:0.00015112673833538655\n",
      "train loss:0.0015059589739854697\n",
      "train loss:4.1530789893648655e-05\n",
      "train loss:0.0009332442303418708\n",
      "train loss:0.0008250103069461289\n",
      "train loss:0.0003460870216474944\n",
      "train loss:6.544252157381178e-05\n",
      "train loss:0.0010740372016084959\n",
      "train loss:0.0013448167737667906\n",
      "train loss:7.76470424503516e-05\n",
      "train loss:0.0005117983061964288\n",
      "train loss:0.002494029621369054\n",
      "train loss:0.0019158062794970426\n",
      "train loss:0.0003437027524900701\n",
      "train loss:0.003039924578863353\n",
      "train loss:0.0011837624254580701\n",
      "train loss:0.00407259831392685\n",
      "train loss:0.007384572752610635\n",
      "train loss:9.621249897480796e-05\n",
      "train loss:0.0003602341140117174\n",
      "train loss:0.017248673445333366\n",
      "train loss:0.004082444350837347\n",
      "train loss:0.0010241745508085282\n",
      "train loss:6.936628101191491e-05\n",
      "train loss:0.00028150799489836135\n",
      "train loss:0.0009319569243591123\n",
      "train loss:0.0004161761136588681\n",
      "train loss:0.008248917994004875\n",
      "train loss:0.004786652464904804\n",
      "train loss:0.007429441917849952\n",
      "train loss:0.0006072186144853416\n",
      "train loss:0.00010956628388522323\n",
      "train loss:0.0005597446640250422\n",
      "train loss:0.002967052915456071\n",
      "train loss:0.003441330981538246\n",
      "train loss:0.0002442591891210719\n",
      "train loss:0.006430339114097375\n",
      "train loss:0.0014175295829846148\n",
      "train loss:0.0009303427313575684\n",
      "train loss:0.0018745084741038428\n",
      "train loss:0.0007709009740677809\n",
      "train loss:0.0022126761139034394\n",
      "train loss:0.001451225304889006\n",
      "train loss:0.005708825326571612\n",
      "train loss:0.009937310853140223\n",
      "train loss:0.004816104787028861\n",
      "train loss:0.006554935469499311\n",
      "train loss:0.0033947901184077037\n",
      "train loss:0.0004567593448632758\n",
      "train loss:0.005593636816720694\n",
      "train loss:0.00496986941912117\n",
      "train loss:0.0033723177534986737\n",
      "train loss:0.0024051123657528144\n",
      "train loss:0.010493203289269957\n",
      "train loss:0.004999397721500844\n",
      "train loss:0.0016980605225138125\n",
      "train loss:0.0007736834972695366\n",
      "train loss:0.00038274852103347556\n",
      "train loss:0.0008643939700684087\n",
      "train loss:0.0002163832596590971\n",
      "train loss:0.002556795474700849\n",
      "train loss:0.00026538149021756497\n",
      "train loss:0.0012638284640007775\n",
      "train loss:0.001367264700785354\n",
      "train loss:0.002302920114167784\n",
      "train loss:0.001108327818020386\n",
      "train loss:0.0020894042297445684\n",
      "train loss:0.0007388619881208634\n",
      "train loss:0.002542060548776246\n",
      "train loss:0.001475516720942907\n",
      "train loss:0.0016961565770849544\n",
      "train loss:0.0003175236866441017\n",
      "train loss:0.003814812972374021\n",
      "train loss:0.00010923605175837675\n",
      "train loss:0.0039053118143610064\n",
      "train loss:0.0006969713742051674\n",
      "train loss:0.003673893932495067\n",
      "train loss:0.0005240750531835193\n",
      "train loss:0.0009754280919969133\n",
      "train loss:0.00042366829492926356\n",
      "train loss:0.0011355612426514887\n",
      "train loss:6.248007760260403e-05\n",
      "train loss:0.0032440584922351183\n",
      "train loss:0.00018634922494469686\n",
      "train loss:0.0006367812658554417\n",
      "train loss:0.0003227609425811163\n",
      "train loss:0.00024604395244329707\n",
      "train loss:0.02587777719806993\n",
      "train loss:0.0012884653902065697\n",
      "=== epoch:18, train acc:0.997, test acc:0.986 ===\n",
      "train loss:0.0003985374142405612\n",
      "train loss:0.0015944600614201346\n",
      "train loss:0.0005700031312957834\n",
      "train loss:0.0011355877586418399\n",
      "train loss:0.0008834646214714512\n",
      "train loss:0.002032828486724942\n",
      "train loss:0.0004932863983724437\n",
      "train loss:0.0019071556597360063\n",
      "train loss:0.001852529701296477\n",
      "train loss:0.004406570203077241\n",
      "train loss:0.0003167091740141603\n",
      "train loss:0.003162577211603173\n",
      "train loss:0.016654999677374615\n",
      "train loss:0.006983787565127011\n",
      "train loss:0.0019172972260719978\n",
      "train loss:0.0012580680369326776\n",
      "train loss:0.0003251987463313115\n",
      "train loss:0.0007122836470815317\n",
      "train loss:0.00017801706965370887\n",
      "train loss:0.003320768178002329\n",
      "train loss:0.002506749826135825\n",
      "train loss:0.0008211425457767834\n",
      "train loss:3.799223490228188e-05\n",
      "train loss:0.008764961023718314\n",
      "train loss:0.0002709721098579354\n",
      "train loss:0.0002688452884668023\n",
      "train loss:0.0026143733843964696\n",
      "train loss:0.0010502447820202167\n",
      "train loss:0.0017099145379222919\n",
      "train loss:0.0019223113237298534\n",
      "train loss:0.0017946497464556364\n",
      "train loss:0.004567438924717098\n",
      "train loss:0.00046829128223522956\n",
      "train loss:0.001979212659662298\n",
      "train loss:0.0021664190256197864\n",
      "train loss:0.0037369437268675106\n",
      "train loss:0.0010452707817366505\n",
      "train loss:0.0007475365682628242\n",
      "train loss:0.0011181793927968274\n",
      "train loss:0.0009807701501559297\n",
      "train loss:0.0017981028674401503\n",
      "train loss:0.0002083146307201542\n",
      "train loss:0.001602728656890027\n",
      "train loss:0.0010072280019864845\n",
      "train loss:0.0011168766682278742\n",
      "train loss:0.0004173759086254108\n",
      "train loss:0.00205461302860675\n",
      "train loss:0.0016730596769340797\n",
      "train loss:0.06203041881750679\n",
      "train loss:0.0005312994203054302\n",
      "train loss:0.0022906823164492137\n",
      "train loss:0.002043845650412566\n",
      "train loss:0.0007407010174869453\n",
      "train loss:0.0004447608479701858\n",
      "train loss:0.004580311487864811\n",
      "train loss:0.0009301435153927426\n",
      "train loss:0.0015259076487300397\n",
      "train loss:0.0007290921222616832\n",
      "train loss:0.0007707674531503957\n",
      "train loss:0.009226269952102352\n",
      "train loss:0.002074644765365871\n",
      "train loss:0.00055256916285868\n",
      "train loss:0.001290600308833518\n",
      "train loss:0.013035182227356617\n",
      "train loss:0.004841494416445751\n",
      "train loss:0.0015880819989601786\n",
      "train loss:0.0007131178382171892\n",
      "train loss:0.0006701797125772611\n",
      "train loss:0.00260142893363074\n",
      "train loss:0.0016515110410852069\n",
      "train loss:0.0030238649928918332\n",
      "train loss:0.004115897528761122\n",
      "train loss:5.2606462065509136e-05\n",
      "train loss:0.001585273959396239\n",
      "train loss:0.001018953926029786\n",
      "train loss:0.002316022830963353\n",
      "train loss:0.007661663979317104\n",
      "train loss:0.0003532334769643325\n",
      "train loss:0.0001774575440197213\n",
      "train loss:0.0007079179531195068\n",
      "train loss:0.0009242959387980076\n",
      "train loss:0.003700993342849023\n",
      "train loss:0.0050721881214609065\n",
      "train loss:0.00032662328214354004\n",
      "train loss:0.0009657015972679234\n",
      "train loss:0.0024325355854604293\n",
      "train loss:0.0009784623987594424\n",
      "train loss:0.0005406776569434876\n",
      "train loss:0.0008007538147196228\n",
      "train loss:0.00491560337159711\n",
      "train loss:0.0016568487729719697\n",
      "train loss:0.00043595640671823254\n",
      "train loss:7.603997519878954e-05\n",
      "train loss:0.0061593956804645664\n",
      "train loss:0.0034748138237925153\n",
      "train loss:0.0054300817370288345\n",
      "train loss:0.0009124726229285549\n",
      "train loss:0.000554462211841602\n",
      "train loss:4.2914065622692416e-05\n",
      "train loss:0.0007662916873095226\n",
      "train loss:0.002985454668632497\n",
      "train loss:0.0017555670466688309\n",
      "train loss:0.0002819578826661948\n",
      "train loss:0.00013829709868613604\n",
      "train loss:0.0001296249049493431\n",
      "train loss:0.0032536811279080756\n",
      "train loss:0.0003260843941009829\n",
      "train loss:0.0005243465570709254\n",
      "train loss:0.002186274763647883\n",
      "train loss:0.0010988141131193549\n",
      "train loss:0.0016838628613595715\n",
      "train loss:0.0003295962867448686\n",
      "train loss:0.006686679607189601\n",
      "train loss:0.00036200970620684517\n",
      "train loss:0.0005913625231368998\n",
      "train loss:0.002251434842865813\n",
      "train loss:0.0005121941984762893\n",
      "train loss:8.88323424004661e-05\n",
      "train loss:0.0008655182993885774\n",
      "train loss:0.00016080863197780255\n",
      "train loss:0.001420793170085217\n",
      "train loss:0.0013228452044051695\n",
      "train loss:0.0018212542850402713\n",
      "train loss:0.0001266193984499806\n",
      "train loss:0.00024000644805661004\n",
      "train loss:0.0016587229328462916\n",
      "train loss:0.0005209311996672045\n",
      "train loss:0.0007345671549770951\n",
      "train loss:0.0025495910896162904\n",
      "train loss:0.005273218918739676\n",
      "train loss:0.006565398651996904\n",
      "train loss:9.934659888981953e-05\n",
      "train loss:0.0017312226077688773\n",
      "train loss:0.00012046703299007666\n",
      "train loss:0.0003108923832898816\n",
      "train loss:6.340496078005754e-05\n",
      "train loss:0.0032464184876254253\n",
      "train loss:6.290021284004608e-05\n",
      "train loss:0.0011667954238499276\n",
      "train loss:0.0011997289049151124\n",
      "train loss:0.0005081342522720669\n",
      "train loss:0.008185732609826545\n",
      "train loss:0.002739720233387616\n",
      "train loss:0.001196590161860914\n",
      "train loss:0.000432445465943977\n",
      "train loss:0.0029087756603022067\n",
      "train loss:0.0006128837956353747\n",
      "train loss:0.0002994278833176831\n",
      "train loss:0.0004874283652716496\n",
      "train loss:0.0015046980446441116\n",
      "train loss:0.0010780473848678418\n",
      "train loss:0.0067278088590472605\n",
      "train loss:0.0007164642251215851\n",
      "train loss:7.420808810091401e-05\n",
      "train loss:0.000444043043336516\n",
      "train loss:0.0010392657812539187\n",
      "train loss:0.0013527390595172487\n",
      "train loss:0.00014892249339349824\n",
      "train loss:0.00011232919506163832\n",
      "train loss:0.0008752521353417877\n",
      "train loss:0.0017033831026353524\n",
      "train loss:0.0002670922627274627\n",
      "train loss:0.001982656060886352\n",
      "train loss:0.0005896260109262301\n",
      "train loss:0.0036127565384506573\n",
      "train loss:0.005558824645828556\n",
      "train loss:0.0019541881021669753\n",
      "train loss:0.00010872029771572528\n",
      "train loss:0.00047072286122151687\n",
      "train loss:0.0003194993119379884\n",
      "train loss:0.0018281126994111696\n",
      "train loss:0.0005222994874765\n",
      "train loss:0.0009853556351346665\n",
      "train loss:0.001774177928874896\n",
      "train loss:0.00011317769230629189\n",
      "train loss:0.00037834471958628826\n",
      "train loss:0.002959962353345023\n",
      "train loss:0.000174643164131196\n",
      "train loss:0.0022129158695960984\n",
      "train loss:0.00769982535769307\n",
      "train loss:0.0032176510062728834\n",
      "train loss:0.000670932723355961\n",
      "train loss:4.697269938370748e-05\n",
      "train loss:0.0009509932900714799\n",
      "train loss:0.0005666980502255626\n",
      "train loss:0.00044386478256610277\n",
      "train loss:0.0011020241095570974\n",
      "train loss:0.0020044472317609077\n",
      "train loss:0.00042995939050746945\n",
      "train loss:0.0005444614004191281\n",
      "train loss:0.00010445734566233799\n",
      "train loss:6.695577334855946e-05\n",
      "train loss:0.0014274252986430666\n",
      "train loss:6.180978505095303e-05\n",
      "train loss:0.0001512304771732385\n",
      "train loss:0.0005240523357059996\n",
      "train loss:0.0007381762515299585\n",
      "train loss:0.0022580452207431757\n",
      "train loss:0.0003154021427954261\n",
      "train loss:0.00048381684000119006\n",
      "train loss:0.002440390938583086\n",
      "train loss:0.0005187207199923628\n",
      "train loss:0.0010750373577385597\n",
      "train loss:0.0025479659368894455\n",
      "train loss:0.0006284666630421208\n",
      "train loss:0.00016581469117667814\n",
      "train loss:3.338188814650679e-05\n",
      "train loss:0.0008826106037814008\n",
      "train loss:0.00014972334775936367\n",
      "train loss:0.003715417523887894\n",
      "train loss:0.0051193012821360885\n",
      "train loss:0.00014336730212275152\n",
      "train loss:0.004329582388617276\n",
      "train loss:0.0002396710533081133\n",
      "train loss:0.00023700673717263858\n",
      "train loss:0.0001245610417036804\n",
      "train loss:0.0002653433993706924\n",
      "train loss:0.00025819079873496634\n",
      "train loss:0.0009568554003270641\n",
      "train loss:0.0013435077929323488\n",
      "train loss:0.00023089113503096164\n",
      "train loss:0.0035457259505288475\n",
      "train loss:0.0005702792708726208\n",
      "train loss:0.0005958285133159574\n",
      "train loss:0.0010630754063860473\n",
      "train loss:0.0032228902077484183\n",
      "train loss:7.86354818762952e-05\n",
      "train loss:0.00026009177553475875\n",
      "train loss:0.0015675541443451431\n",
      "train loss:9.338067648998064e-05\n",
      "train loss:0.0031460060827891117\n",
      "train loss:0.003023698813905844\n",
      "train loss:0.0008293944834833487\n",
      "train loss:0.0005072502433616073\n",
      "train loss:0.0015133757283441475\n",
      "train loss:0.0002825915844227552\n",
      "train loss:0.0013954382095614562\n",
      "train loss:0.0010614576280116809\n",
      "train loss:0.0014783873524875874\n",
      "train loss:0.00016518673932349544\n",
      "train loss:0.0002384679422210445\n",
      "train loss:0.002301334575605248\n",
      "train loss:0.0005397431155771533\n",
      "train loss:0.0020785753176740624\n",
      "train loss:0.00025100523486059007\n",
      "train loss:0.00023398944315701102\n",
      "train loss:0.000480433988586951\n",
      "train loss:0.0010835220558340585\n",
      "train loss:0.00025381682929784125\n",
      "train loss:0.00021252798392524982\n",
      "train loss:0.00168034640167581\n",
      "train loss:0.00019424726210514075\n",
      "train loss:0.0015715779667246986\n",
      "train loss:0.0024483482476480426\n",
      "train loss:0.00030257329084333676\n",
      "train loss:0.0017833927187743087\n",
      "train loss:0.0009728534801381472\n",
      "train loss:0.00045038681088527743\n",
      "train loss:0.0003108025385226217\n",
      "train loss:0.001096056928759158\n",
      "train loss:0.00010599004010975581\n",
      "train loss:0.0006242458819537733\n",
      "train loss:0.000749439739018394\n",
      "train loss:0.0005476458024429909\n",
      "train loss:0.0025246397370369844\n",
      "train loss:0.0009161964052755209\n",
      "train loss:0.0008072696383393038\n",
      "train loss:0.001430059490374208\n",
      "train loss:5.006926687852536e-05\n",
      "train loss:0.03310988037143011\n",
      "train loss:0.0015546633013392738\n",
      "train loss:0.001599760228080536\n",
      "train loss:0.0007023557133149222\n",
      "train loss:0.0021821601127660194\n",
      "train loss:6.567466726658829e-05\n",
      "train loss:0.00130521457094026\n",
      "train loss:0.0004132792516772391\n",
      "train loss:0.0007094954538988655\n",
      "train loss:0.00042998196403857316\n",
      "train loss:8.233344364058331e-05\n",
      "train loss:0.0004973930980356153\n",
      "train loss:0.0029858994163223956\n",
      "train loss:0.0017365020419817185\n",
      "train loss:0.0030672870536397902\n",
      "train loss:0.00022740703484907477\n",
      "train loss:0.00018578151048305458\n",
      "train loss:0.0003428391809182734\n",
      "train loss:0.00019813585014492774\n",
      "train loss:9.270636902326255e-05\n",
      "train loss:0.000373259195705645\n",
      "train loss:0.0006653353652058239\n",
      "train loss:0.0023599390721568787\n",
      "train loss:2.3907399596828832e-05\n",
      "train loss:0.00014722331301783912\n",
      "train loss:0.00012537329507281568\n",
      "train loss:0.0009337425669353853\n",
      "train loss:0.00021807487373200551\n",
      "train loss:0.0014629069724254773\n",
      "train loss:0.0014768371035307195\n",
      "train loss:0.00012302869702050172\n",
      "train loss:0.0006623910195195583\n",
      "train loss:0.002234039044601138\n",
      "train loss:0.00046689639326539406\n",
      "train loss:0.0007668186982448181\n",
      "train loss:0.004013498518583767\n",
      "train loss:0.00043812426153523425\n",
      "train loss:0.0006515935778409354\n",
      "train loss:0.0011150763187204277\n",
      "train loss:0.00016246229308925327\n",
      "train loss:0.0001840732796355302\n",
      "train loss:0.0001843525664480915\n",
      "train loss:0.0001859434038413582\n",
      "train loss:0.0002494449891663094\n",
      "train loss:8.6095736600256e-05\n",
      "train loss:0.000261648436087038\n",
      "train loss:0.00032431734506870236\n",
      "train loss:7.561699882675609e-05\n",
      "train loss:0.0042932234024579624\n",
      "train loss:6.577217422673106e-05\n",
      "train loss:0.00015445511405116279\n",
      "train loss:0.0029573419718870023\n",
      "train loss:0.00012908347101575455\n",
      "train loss:0.0003750539519223295\n",
      "train loss:0.0005195783723635774\n",
      "train loss:0.0003413572860908747\n",
      "train loss:1.072444296544632e-05\n",
      "train loss:0.00014714503033507123\n",
      "train loss:0.0007883946410448478\n",
      "train loss:0.00039488789721225893\n",
      "train loss:0.0034543695404706874\n",
      "train loss:0.0015872415951603256\n",
      "train loss:0.0025012391262574417\n",
      "train loss:0.0003406161760424083\n",
      "train loss:0.005869357079199556\n",
      "train loss:0.002508909327107387\n",
      "train loss:5.5540063509457134e-05\n",
      "train loss:0.002772400682101263\n",
      "train loss:5.822615485552244e-05\n",
      "train loss:0.003166159283073172\n",
      "train loss:0.0010582786580668193\n",
      "train loss:0.002712711706098458\n",
      "train loss:0.0005640766384449921\n",
      "train loss:0.020841875388942454\n",
      "train loss:0.00035819734401048507\n",
      "train loss:6.907802157093607e-06\n",
      "train loss:0.0004687672173903284\n",
      "train loss:0.038754079754836905\n",
      "train loss:0.00019009783612651234\n",
      "train loss:8.53008308969069e-05\n",
      "train loss:0.001112798102933877\n",
      "train loss:0.00014799034890087108\n",
      "train loss:0.0023012673647770203\n",
      "train loss:0.001905513001619357\n",
      "train loss:4.716461991033279e-05\n",
      "train loss:0.0015312676018090557\n",
      "train loss:9.004005719686579e-05\n",
      "train loss:0.0022896079807923366\n",
      "train loss:0.0012049283154519476\n",
      "train loss:0.0013364339503084926\n",
      "train loss:0.00038904764421830067\n",
      "train loss:0.007575849074936585\n",
      "train loss:0.0018625715234668783\n",
      "train loss:0.0007039079411677154\n",
      "train loss:0.01849963121809035\n",
      "train loss:0.0011279498100606675\n",
      "train loss:0.0005815815354422445\n",
      "train loss:0.00045743522259451087\n",
      "train loss:0.005071319325430073\n",
      "train loss:0.001209966437407669\n",
      "train loss:0.001487574854678152\n",
      "train loss:0.00011637566489206969\n",
      "train loss:0.0032898187762459108\n",
      "train loss:0.011221562501408513\n",
      "train loss:0.003431543361009121\n",
      "train loss:0.0022452103792331055\n",
      "train loss:0.002510832968878058\n",
      "train loss:0.004166949948354659\n",
      "train loss:0.00040843527995168625\n",
      "train loss:0.0008058031672652888\n",
      "train loss:0.0007568398520145171\n",
      "train loss:7.353737615077704e-05\n",
      "train loss:0.00034708930351037316\n",
      "train loss:0.05259639089871085\n",
      "train loss:0.003411953384824759\n",
      "train loss:0.00045280937798532976\n",
      "train loss:0.003104882117218554\n",
      "train loss:0.0011513332731571226\n",
      "train loss:0.0013666456180925206\n",
      "train loss:0.0051763426218536576\n",
      "train loss:0.00015648804093430964\n",
      "train loss:0.00030446830645370964\n",
      "train loss:0.004299640139086607\n",
      "train loss:0.003392578367520525\n",
      "train loss:0.012869781020181565\n",
      "train loss:0.0013147113577190656\n",
      "train loss:0.0029469569911529853\n",
      "train loss:0.0005805800942816885\n",
      "train loss:0.0041516506455262\n",
      "train loss:0.00040545823505493925\n",
      "train loss:0.002420800403710196\n",
      "train loss:0.00033112105478759496\n",
      "train loss:0.00015342713698662705\n",
      "train loss:0.0003805787791244291\n",
      "train loss:0.0011272284648774224\n",
      "train loss:0.00019807167646573173\n",
      "train loss:0.0019767163818707823\n",
      "train loss:0.0003616412370025989\n",
      "train loss:0.004663442410220286\n",
      "train loss:0.0016339884336937594\n",
      "train loss:0.0061750972206821644\n",
      "train loss:0.0012019680377748617\n",
      "train loss:0.0001834345007196542\n",
      "train loss:0.004019575820959603\n",
      "train loss:0.0025246994931771706\n",
      "train loss:0.03629685774350669\n",
      "train loss:0.0011998969206647174\n",
      "train loss:0.0009193164129439219\n",
      "train loss:0.0008224326242508093\n",
      "train loss:0.002066054691015533\n",
      "train loss:0.0015515640089659476\n",
      "train loss:0.00023005897853640688\n",
      "train loss:0.0006064896932566981\n",
      "train loss:0.0011472331262732392\n",
      "train loss:0.0031611763980668216\n",
      "train loss:2.681561289080288e-05\n",
      "train loss:0.001389407323480145\n",
      "train loss:0.0008876832389970281\n",
      "train loss:0.0012912753557882961\n",
      "train loss:0.002827138923408418\n",
      "train loss:0.0002832094328301129\n",
      "train loss:0.0027510494040770563\n",
      "train loss:0.0018170998550225297\n",
      "train loss:0.002070534804857972\n",
      "train loss:0.0017037250778420031\n",
      "train loss:0.0005032957505715425\n",
      "train loss:0.0001432781182804721\n",
      "train loss:0.0002495905101300308\n",
      "train loss:0.00442063931122186\n",
      "train loss:0.00022959204570820833\n",
      "train loss:0.0016712500164062982\n",
      "train loss:0.0025449933524911905\n",
      "train loss:0.0017495820331714352\n",
      "train loss:0.0024558222703665988\n",
      "train loss:0.001120121008877527\n",
      "train loss:0.0001319402060454774\n",
      "train loss:0.002053460426949321\n",
      "train loss:0.00019505465078859313\n",
      "train loss:0.003246525504392441\n",
      "train loss:0.0004625207344412792\n",
      "train loss:0.001008776386053666\n",
      "train loss:0.0012088988649228995\n",
      "train loss:0.0007397268180320362\n",
      "train loss:0.0001696347493562596\n",
      "train loss:0.0002596983560595864\n",
      "train loss:0.00026098422520809384\n",
      "train loss:0.00036731088201235625\n",
      "train loss:0.00044820604606515206\n",
      "train loss:6.804725727271616e-05\n",
      "train loss:0.0006949209715997717\n",
      "train loss:0.00032710434449783693\n",
      "train loss:0.007372486058728411\n",
      "train loss:0.0002460023604242965\n",
      "train loss:0.0004723057789304786\n",
      "train loss:0.00039044743309613034\n",
      "train loss:0.0003402263178762692\n",
      "train loss:0.024142624186177145\n",
      "train loss:0.005108122015733834\n",
      "train loss:0.00034585139427407833\n",
      "train loss:0.0005915627214480551\n",
      "train loss:0.0007511008737326428\n",
      "train loss:0.0005727617701346661\n",
      "train loss:0.0017387828825402617\n",
      "train loss:0.007799734243696498\n",
      "train loss:0.0021639874928267343\n",
      "train loss:0.0005145330185310497\n",
      "train loss:0.0032882714218409816\n",
      "train loss:6.750009530427348e-05\n",
      "train loss:0.00034197041134287484\n",
      "train loss:0.001092693898775191\n",
      "train loss:0.0013996570707106046\n",
      "train loss:0.0010293362003557314\n",
      "train loss:0.003943574426463805\n",
      "train loss:0.0010142633328603303\n",
      "train loss:0.00013968799684458034\n",
      "train loss:0.0017350415336641514\n",
      "train loss:0.00262264423905702\n",
      "train loss:0.00022007766663275394\n",
      "train loss:0.0007550554347943004\n",
      "train loss:0.0004243591073884508\n",
      "train loss:6.294250272403796e-05\n",
      "train loss:0.00011399693103816332\n",
      "train loss:0.0010445869228289375\n",
      "train loss:0.00011019450953995124\n",
      "train loss:0.000284528226690286\n",
      "train loss:0.0004903367674360547\n",
      "train loss:0.0005344020912557781\n",
      "train loss:0.007403167406214296\n",
      "train loss:0.0012723843137071884\n",
      "train loss:0.00016141632723996944\n",
      "train loss:0.0002311667809903248\n",
      "train loss:0.00020801769926739476\n",
      "train loss:0.0004035839358445485\n",
      "train loss:0.00040827436074153444\n",
      "train loss:0.005315228727576104\n",
      "train loss:0.0037419146001163002\n",
      "train loss:0.0006357438626928376\n",
      "train loss:0.0040658710765990105\n",
      "train loss:0.0014471798795055502\n",
      "train loss:0.0005852974440243604\n",
      "train loss:0.0010738370497728456\n",
      "train loss:0.002427453373045106\n",
      "train loss:0.000261374995528785\n",
      "train loss:7.338846010277943e-05\n",
      "train loss:0.00040500031619332015\n",
      "train loss:0.0006874343281976919\n",
      "train loss:0.0007866355611555445\n",
      "train loss:0.0007634955852794632\n",
      "train loss:0.0012810356228180496\n",
      "train loss:0.00021516269176038029\n",
      "train loss:0.0005932613429098715\n",
      "train loss:0.004551953154720592\n",
      "train loss:0.0004243711953017676\n",
      "train loss:0.0005166461767423166\n",
      "train loss:0.0037442124090727186\n",
      "train loss:0.011023584342716877\n",
      "train loss:0.00047599100789915696\n",
      "train loss:0.002359289670317129\n",
      "train loss:0.002493207558225724\n",
      "train loss:0.003026145372486152\n",
      "train loss:0.0005115066079137531\n",
      "train loss:0.011529499295059616\n",
      "train loss:0.0016790814780614303\n",
      "train loss:0.00023666512278228754\n",
      "train loss:0.00040233906666442017\n",
      "train loss:0.002095780794301922\n",
      "train loss:0.00012356402428999842\n",
      "train loss:0.0022871392109825015\n",
      "train loss:0.0004893447689610981\n",
      "train loss:0.0008442798053995118\n",
      "train loss:0.0007649173650371249\n",
      "train loss:0.0006448884765231057\n",
      "train loss:0.0006091931602814286\n",
      "train loss:0.0013933679395413093\n",
      "train loss:0.00018617718226650635\n",
      "train loss:0.000888295539336832\n",
      "train loss:2.3291316107935e-05\n",
      "train loss:0.0004581042133541149\n",
      "train loss:0.00034746932866503615\n",
      "train loss:9.072621414774303e-05\n",
      "train loss:0.0031275366532566968\n",
      "train loss:0.0010321646063189234\n",
      "train loss:0.00013158043852380503\n",
      "train loss:0.00023758615781218695\n",
      "train loss:0.0017796126205084722\n",
      "train loss:0.0024033486570240454\n",
      "train loss:0.00018207254578378598\n",
      "train loss:0.0024108368227635435\n",
      "train loss:0.0006574464096517451\n",
      "train loss:0.0002889779817488194\n",
      "train loss:0.0003344000699717532\n",
      "train loss:0.00034227109583031406\n",
      "train loss:0.002230406159047806\n",
      "train loss:0.001711613922924807\n",
      "train loss:0.0007607522633650493\n",
      "train loss:0.00024048123881658678\n",
      "train loss:0.0015028145232779883\n",
      "train loss:5.26117584787015e-05\n",
      "train loss:0.0001989468111494054\n",
      "train loss:0.00014856541983676878\n",
      "train loss:0.0005529665364080263\n",
      "train loss:0.00010717161611742755\n",
      "train loss:0.00010786236867120787\n",
      "train loss:0.0012050373803993146\n",
      "train loss:0.006094575413731219\n",
      "train loss:0.00015940018883731105\n",
      "train loss:0.0004893672202113406\n",
      "train loss:0.001695834064316171\n",
      "train loss:0.00042919088475043885\n",
      "train loss:0.00059827373938683\n",
      "train loss:0.00031061241800178476\n",
      "train loss:0.0001885553800716144\n",
      "train loss:0.004285507077275963\n",
      "train loss:0.0006885970010667184\n",
      "train loss:0.0018604968961900175\n",
      "train loss:6.150389388063208e-05\n",
      "train loss:0.0009817151013368738\n",
      "train loss:0.00012658369011020872\n",
      "train loss:0.004121118569277284\n",
      "train loss:0.004493734920823216\n",
      "train loss:0.0011802685462404467\n",
      "train loss:0.00019126826051540593\n",
      "train loss:0.0005777451330956985\n",
      "train loss:0.0003540649643231756\n",
      "train loss:0.01140075343029686\n",
      "train loss:0.0005020196866264182\n",
      "train loss:0.0011178069292561755\n",
      "train loss:0.0004264480864463457\n",
      "train loss:0.0009482486593407429\n",
      "train loss:0.003057124161522992\n",
      "train loss:8.801421869709638e-05\n",
      "=== epoch:19, train acc:0.999, test acc:0.988 ===\n",
      "train loss:0.0006780302447830874\n",
      "train loss:0.001151430053615918\n",
      "train loss:0.0015490828624619213\n",
      "train loss:2.1399015913711405e-05\n",
      "train loss:3.0235288505922736e-05\n",
      "train loss:0.004075628786889154\n",
      "train loss:0.0012548627256502084\n",
      "train loss:0.00018845047118048612\n",
      "train loss:0.00020418810977139047\n",
      "train loss:0.001481805671202161\n",
      "train loss:0.0033568003640032716\n",
      "train loss:0.00029061606200395584\n",
      "train loss:0.0010807878641253472\n",
      "train loss:0.0007756381537858526\n",
      "train loss:0.00018553687837440347\n",
      "train loss:0.0009178138446263762\n",
      "train loss:8.774398609324741e-05\n",
      "train loss:0.00043455732302379523\n",
      "train loss:2.0084661592129618e-05\n",
      "train loss:0.001222865479584542\n",
      "train loss:0.0019951004969492596\n",
      "train loss:6.894202087102028e-05\n",
      "train loss:0.0005445498267951006\n",
      "train loss:6.099448241121031e-05\n",
      "train loss:0.0009181790221694557\n",
      "train loss:0.000315190660974531\n",
      "train loss:0.0010636919548195004\n",
      "train loss:0.0011234553420664869\n",
      "train loss:0.0020101355245071067\n",
      "train loss:0.00024226796401310437\n",
      "train loss:0.00022440971739998657\n",
      "train loss:0.000667982099708159\n",
      "train loss:0.0020536511943575167\n",
      "train loss:6.256064949579722e-05\n",
      "train loss:0.0008849849724490746\n",
      "train loss:0.0002116712337317591\n",
      "train loss:0.0008419545063471422\n",
      "train loss:0.0020026596007640136\n",
      "train loss:0.0005383353932171905\n",
      "train loss:0.0016399563533944328\n",
      "train loss:0.0022854783409673885\n",
      "train loss:0.00023699066516828392\n",
      "train loss:9.037606922823793e-05\n",
      "train loss:0.0001644019265780408\n",
      "train loss:0.0013570419080962772\n",
      "train loss:4.756125671808269e-05\n",
      "train loss:0.0017947028497940476\n",
      "train loss:0.0012925670493637766\n",
      "train loss:0.00011353583776609131\n",
      "train loss:0.00023518944778139106\n",
      "train loss:0.00022607559430427057\n",
      "train loss:0.00048454892209262026\n",
      "train loss:6.262717918006569e-05\n",
      "train loss:0.0007506485286145864\n",
      "train loss:0.00013084241176676434\n",
      "train loss:0.0006680341508750364\n",
      "train loss:0.0005154560741167202\n",
      "train loss:0.0003205577585443826\n",
      "train loss:0.000853483221864261\n",
      "train loss:0.0010335386914765148\n",
      "train loss:0.0004225240852577818\n",
      "train loss:0.00019293723772783998\n",
      "train loss:0.0003748017059782468\n",
      "train loss:0.00022136485715796642\n",
      "train loss:0.00217692441282987\n",
      "train loss:0.0003543964175048765\n",
      "train loss:0.0007055602264349408\n",
      "train loss:0.0004622688981355332\n",
      "train loss:0.0016855845911744479\n",
      "train loss:0.0003114617814440188\n",
      "train loss:0.00015431860433241156\n",
      "train loss:0.00030212124010461434\n",
      "train loss:0.0001833165130044293\n",
      "train loss:6.699968886732259e-05\n",
      "train loss:0.00044165496511284005\n",
      "train loss:6.0135877772549835e-05\n",
      "train loss:0.00021696727870196595\n",
      "train loss:0.0005363860713242421\n",
      "train loss:0.0005630647243036097\n",
      "train loss:0.00012182075526796856\n",
      "train loss:0.0014698614316615158\n",
      "train loss:0.000930845890291311\n",
      "train loss:0.0010750799758291873\n",
      "train loss:5.439067360752922e-05\n",
      "train loss:0.0008498441226694835\n",
      "train loss:0.00019101628969637747\n",
      "train loss:0.0024465018696736267\n",
      "train loss:0.00025984276172516063\n",
      "train loss:0.0007131405190408982\n",
      "train loss:0.00012392153313163143\n",
      "train loss:0.0016763885645524207\n",
      "train loss:0.0019325431460678918\n",
      "train loss:0.0020868576138409773\n",
      "train loss:0.001578395541924257\n",
      "train loss:0.001140015234218853\n",
      "train loss:0.0025411066072470404\n",
      "train loss:0.00017181056737039654\n",
      "train loss:0.002234165299591635\n",
      "train loss:0.002766778105817786\n",
      "train loss:0.0024175785770367015\n",
      "train loss:0.00039516395279961294\n",
      "train loss:0.0007586197234707941\n",
      "train loss:0.0030407085521284597\n",
      "train loss:0.0007000713503261011\n",
      "train loss:0.0004381059585297406\n",
      "train loss:0.0022074821067059307\n",
      "train loss:0.0017585615411973426\n",
      "train loss:0.00173748412884232\n",
      "train loss:0.001765455517878745\n",
      "train loss:0.0003359035439682946\n",
      "train loss:0.00029834320696075513\n",
      "train loss:0.006409956077423831\n",
      "train loss:0.0005511329718228773\n",
      "train loss:8.144350460973618e-05\n",
      "train loss:0.00040340918921708067\n",
      "train loss:0.0014685781697832035\n",
      "train loss:0.0002987122840468294\n",
      "train loss:0.00026611321657194284\n",
      "train loss:0.0001658539513600177\n",
      "train loss:0.0019503279308987751\n",
      "train loss:0.0007178995933116103\n",
      "train loss:0.00018007570279394037\n",
      "train loss:0.00015239559662257434\n",
      "train loss:0.00018382195005687594\n",
      "train loss:0.0001590905298423285\n",
      "train loss:0.0016420342925782235\n",
      "train loss:0.0023059840900214847\n",
      "train loss:0.001422263400280055\n",
      "train loss:0.0006416568188524251\n",
      "train loss:0.002333616133859696\n",
      "train loss:0.0005920419148012983\n",
      "train loss:0.011692935661235844\n",
      "train loss:1.8005304956322132e-05\n",
      "train loss:0.002429559566625894\n",
      "train loss:0.005467792173921611\n",
      "train loss:0.0005523887994963788\n",
      "train loss:0.0006543705973540548\n",
      "train loss:0.00046515075873591584\n",
      "train loss:0.002832579856394674\n",
      "train loss:0.004880677720625285\n",
      "train loss:0.001213770862491967\n",
      "train loss:0.00046559706963076957\n",
      "train loss:0.00034337037584636153\n",
      "train loss:3.691400174879475e-05\n",
      "train loss:0.00016319002834284236\n",
      "train loss:0.00020813709371434504\n",
      "train loss:0.0006358403968201789\n",
      "train loss:0.00046831311027104727\n",
      "train loss:0.0036834876699213762\n",
      "train loss:0.000984604552029357\n",
      "train loss:0.0015618883888070395\n",
      "train loss:0.0032411975994909702\n",
      "train loss:0.0023943187799414897\n",
      "train loss:0.00026954233394819603\n",
      "train loss:0.000452549279901385\n",
      "train loss:0.0009728043440469074\n",
      "train loss:0.004448848411356305\n",
      "train loss:0.0007160587514417432\n",
      "train loss:0.001600951275678399\n",
      "train loss:0.00017093201419107772\n",
      "train loss:0.002192362666675178\n",
      "train loss:0.0006787140157902208\n",
      "train loss:0.0018180030193262404\n",
      "train loss:0.002046923075522882\n",
      "train loss:0.0022808941728454848\n",
      "train loss:0.00829286449871174\n",
      "train loss:0.0017239677867212286\n",
      "train loss:0.00022728875801000857\n",
      "train loss:0.00023570687551440172\n",
      "train loss:0.002009367406238662\n",
      "train loss:0.000812092261296416\n",
      "train loss:0.0014385689520139166\n",
      "train loss:0.00038506937670591856\n",
      "train loss:0.0009307433380940598\n",
      "train loss:0.019718296057842276\n",
      "train loss:0.0028194965774844404\n",
      "train loss:0.00010878072289128475\n",
      "train loss:0.0007492379513712559\n",
      "train loss:0.0003638328489636918\n",
      "train loss:0.0008946391335933419\n",
      "train loss:0.00022452195867095278\n",
      "train loss:0.0024174958290595827\n",
      "train loss:0.00020262008060260142\n",
      "train loss:0.00025224900705622845\n",
      "train loss:0.00042631530317022013\n",
      "train loss:0.0003868150855730179\n",
      "train loss:0.00270928567107865\n",
      "train loss:0.0007337307563394273\n",
      "train loss:0.0022424259190399615\n",
      "train loss:0.0016455311700195382\n",
      "train loss:0.0022623585953658494\n",
      "train loss:0.00010730301401510381\n",
      "train loss:0.0003718352790144488\n",
      "train loss:0.001185960141937943\n",
      "train loss:0.00042212734494451973\n",
      "train loss:0.0013133635647399708\n",
      "train loss:0.0009739721517867225\n",
      "train loss:0.0065079899444369415\n",
      "train loss:0.000983270006493657\n",
      "train loss:0.0007257632913903339\n",
      "train loss:0.0009015498532659984\n",
      "train loss:0.00046799104254886174\n",
      "train loss:0.0031982803482608997\n",
      "train loss:3.3722944675105577e-05\n",
      "train loss:0.00037307957938673424\n",
      "train loss:0.0004789733670836389\n",
      "train loss:0.00032894167364085194\n",
      "train loss:0.0006705509517115893\n",
      "train loss:0.002043388957550372\n",
      "train loss:5.9298884529791406e-05\n",
      "train loss:0.00018111495942187435\n",
      "train loss:0.007614313638885092\n",
      "train loss:0.003267670146817051\n",
      "train loss:0.0015080158040978841\n",
      "train loss:0.0006611680697432719\n",
      "train loss:0.00040884883889019674\n",
      "train loss:0.009190331505550025\n",
      "train loss:0.0033832794651235776\n",
      "train loss:0.0017460867652384468\n",
      "train loss:0.0015111600307649833\n",
      "train loss:0.01016800725851807\n",
      "train loss:0.00022460540363360805\n",
      "train loss:0.010822665357105592\n",
      "train loss:0.0012822758969881164\n",
      "train loss:0.0009306087274668991\n",
      "train loss:0.0005698678338836915\n",
      "train loss:0.0010365863541247026\n",
      "train loss:0.0004981515727939444\n",
      "train loss:0.0015714755522411958\n",
      "train loss:5.585780739698807e-05\n",
      "train loss:0.005818902748167563\n",
      "train loss:0.001208076854650744\n",
      "train loss:0.00741955972774066\n",
      "train loss:0.0005026730398818161\n",
      "train loss:0.01239386835354319\n",
      "train loss:7.857674584120869e-05\n",
      "train loss:0.000681570552619308\n",
      "train loss:0.00037344641154286607\n",
      "train loss:0.012313737230818841\n",
      "train loss:0.005070790127896923\n",
      "train loss:0.0015307030065632194\n",
      "train loss:0.0011640155481642187\n",
      "train loss:0.005156738521402129\n",
      "train loss:0.01021552855540479\n",
      "train loss:0.00013561939498983117\n",
      "train loss:0.0029260601842883366\n",
      "train loss:0.00046560769237578576\n",
      "train loss:0.0012482081740023143\n",
      "train loss:0.000813641257214803\n",
      "train loss:0.004846230620608269\n",
      "train loss:0.0001706935274663027\n",
      "train loss:0.004462911013598856\n",
      "train loss:0.0009004602732650117\n",
      "train loss:0.00014583703036241701\n",
      "train loss:0.00011083593037898619\n",
      "train loss:7.214211222331619e-05\n",
      "train loss:0.005230796709248399\n",
      "train loss:0.00030025370709090677\n",
      "train loss:0.0011041801398351658\n",
      "train loss:0.0004829268660847809\n",
      "train loss:0.0005386648540489851\n",
      "train loss:6.620732763844292e-05\n",
      "train loss:0.0035886456724417183\n",
      "train loss:0.0006438030811989893\n",
      "train loss:0.0005368324267456321\n",
      "train loss:0.00013056463451256993\n",
      "train loss:0.0012283531751592768\n",
      "train loss:0.0014210045952758408\n",
      "train loss:4.594223660220155e-05\n",
      "train loss:0.0003437751425983603\n",
      "train loss:0.0035264726002360632\n",
      "train loss:0.005360947026666665\n",
      "train loss:7.303850010539574e-05\n",
      "train loss:0.00042392915825628554\n",
      "train loss:0.001069429118216053\n",
      "train loss:0.0007116357703367727\n",
      "train loss:8.807343306331098e-05\n",
      "train loss:0.001596656224835004\n",
      "train loss:0.0029447347800927175\n",
      "train loss:0.00023885171699079084\n",
      "train loss:7.381566223177426e-05\n",
      "train loss:0.0003592423409840462\n",
      "train loss:0.0013820012908756863\n",
      "train loss:0.0004176732187426919\n",
      "train loss:3.927185391139065e-05\n",
      "train loss:0.0004002643875039435\n",
      "train loss:0.0024413464906647697\n",
      "train loss:0.0006454644725790602\n",
      "train loss:6.186097674159113e-05\n",
      "train loss:0.0028000488362342085\n",
      "train loss:0.0009824852864989504\n",
      "train loss:0.0018681199539273014\n",
      "train loss:0.0008679806228188143\n",
      "train loss:0.0006423813831692923\n",
      "train loss:0.0011665243344173995\n",
      "train loss:0.001167407613404023\n",
      "train loss:0.002084477473017874\n",
      "train loss:0.004805269472234934\n",
      "train loss:0.00016624468891972486\n",
      "train loss:9.009307368334167e-05\n",
      "train loss:0.0004699300771989071\n",
      "train loss:0.0014343976108172835\n",
      "train loss:0.00014222564935533258\n",
      "train loss:0.0005025216731883885\n",
      "train loss:0.002002149148067749\n",
      "train loss:0.0014555468723047556\n",
      "train loss:0.00349961434822207\n",
      "train loss:0.0005218813747840712\n",
      "train loss:0.0028517042355682\n",
      "train loss:0.03270096496848445\n",
      "train loss:0.002507122166467905\n",
      "train loss:0.0011390361733612713\n",
      "train loss:0.010903956392466376\n",
      "train loss:0.0020730518420691603\n",
      "train loss:0.015756938905534628\n",
      "train loss:0.001067434440710588\n",
      "train loss:0.000439207190743541\n",
      "train loss:0.0005072173360469963\n",
      "train loss:0.00010406280115530736\n",
      "train loss:0.004994575314681221\n",
      "train loss:0.0011268343801792708\n",
      "train loss:0.0011295174090515667\n",
      "train loss:0.00529717618465144\n",
      "train loss:0.013063168246237298\n",
      "train loss:0.017871548110676327\n",
      "train loss:0.00080903188146733\n",
      "train loss:0.0008443516236400716\n",
      "train loss:0.002860579336260087\n",
      "train loss:0.0010618737395979847\n",
      "train loss:0.001335792019053402\n",
      "train loss:0.0002923399196249734\n",
      "train loss:0.0014316956740490336\n",
      "train loss:0.0004364690655870449\n",
      "train loss:0.0006522418422007114\n",
      "train loss:0.0005532226448695885\n",
      "train loss:0.0051295445788340525\n",
      "train loss:0.0015741239518864355\n",
      "train loss:0.0007579364123224104\n",
      "train loss:0.0008135343798292159\n",
      "train loss:0.0018553952881748778\n",
      "train loss:0.0017297322171998606\n",
      "train loss:0.016053859126154774\n",
      "train loss:0.00029863536706934644\n",
      "train loss:0.01086002463560787\n",
      "train loss:0.0001937571900894799\n",
      "train loss:2.775175734121987e-05\n",
      "train loss:0.0005609887273992574\n",
      "train loss:0.000519603893128254\n",
      "train loss:0.004301901129829115\n",
      "train loss:0.003888671602787043\n",
      "train loss:0.0014669886245125892\n",
      "train loss:0.0012238957426810514\n",
      "train loss:0.001846848907622074\n",
      "train loss:0.005444200636951276\n",
      "train loss:0.0007508119966343561\n",
      "train loss:0.0015030260378756418\n",
      "train loss:0.003481242686547674\n",
      "train loss:0.1614719112183902\n",
      "train loss:0.0002141524274414495\n",
      "train loss:0.0007677600293121071\n",
      "train loss:0.004210323380441388\n",
      "train loss:0.0016259055128170074\n",
      "train loss:0.0033759446370911143\n",
      "train loss:0.001442794811573821\n",
      "train loss:0.0029841542722326936\n",
      "train loss:0.005223755912695399\n",
      "train loss:0.0008353411464072878\n",
      "train loss:0.00040414411911380036\n",
      "train loss:0.001785389569041404\n",
      "train loss:0.0003112891917161686\n",
      "train loss:0.0015798569383307884\n",
      "train loss:0.0012898590210178873\n",
      "train loss:0.015389722690646977\n",
      "train loss:0.004258142528242742\n",
      "train loss:0.002031867805630885\n",
      "train loss:0.004638582990013501\n",
      "train loss:0.0011117217806904233\n",
      "train loss:0.00014187009985552337\n",
      "train loss:0.00467912610764422\n",
      "train loss:0.009273271483849568\n",
      "train loss:0.0031907886997164497\n",
      "train loss:0.0004075941351694663\n",
      "train loss:0.0005745735704911087\n",
      "train loss:0.0005378826564209117\n",
      "train loss:0.00042124197468920363\n",
      "train loss:0.0024346334355281342\n",
      "train loss:0.000533672476606411\n",
      "train loss:0.0013693137274412084\n",
      "train loss:0.03229464749236481\n",
      "train loss:0.0033242255439890965\n",
      "train loss:0.0061509257153300224\n",
      "train loss:0.0006125083225721141\n",
      "train loss:0.003817895928256953\n",
      "train loss:0.0005826174633368546\n",
      "train loss:0.0007728596121047987\n",
      "train loss:0.0013727134017157542\n",
      "train loss:0.0028952678706900196\n",
      "train loss:0.0023367793208662756\n",
      "train loss:6.294642390420734e-05\n",
      "train loss:0.005843046186556574\n",
      "train loss:0.005154799837987299\n",
      "train loss:0.0006811117985379139\n",
      "train loss:0.0014266197416788941\n",
      "train loss:0.0030773769148376264\n",
      "train loss:0.0007447571607560237\n",
      "train loss:0.0015913643113184964\n",
      "train loss:0.001435856539767008\n",
      "train loss:0.0010373201306817649\n",
      "train loss:0.00040372884539046147\n",
      "train loss:0.001503289910793854\n",
      "train loss:0.003611968550987366\n",
      "train loss:0.0001367501244378842\n",
      "train loss:0.007978103297148329\n",
      "train loss:0.0020026828216290708\n",
      "train loss:0.004773837506684441\n",
      "train loss:0.0033161100965666355\n",
      "train loss:0.003513549127393365\n",
      "train loss:0.005006210375666826\n",
      "train loss:0.0017048738839936823\n",
      "train loss:5.034346925469342e-05\n",
      "train loss:0.0013873894121847594\n",
      "train loss:0.0022683045003594115\n",
      "train loss:0.0004770982614303401\n",
      "train loss:0.007180899533149395\n",
      "train loss:0.0013400073235319352\n",
      "train loss:0.009847572886822014\n",
      "train loss:3.2887297579191817e-05\n",
      "train loss:0.00035175888925046176\n",
      "train loss:0.0007397191447012527\n",
      "train loss:0.00048080588866119737\n",
      "train loss:0.002369507715739263\n",
      "train loss:0.0002691738456604471\n",
      "train loss:0.0012540835204153554\n",
      "train loss:0.002362751019545612\n",
      "train loss:0.00015055148089718784\n",
      "train loss:7.73874282352585e-05\n",
      "train loss:0.00037426962550120757\n",
      "train loss:0.0014246044001958327\n",
      "train loss:0.003461845338237061\n",
      "train loss:0.0010711070071241877\n",
      "train loss:0.00264216554596999\n",
      "train loss:0.00047354298007372495\n",
      "train loss:0.00045874269898063763\n",
      "train loss:0.00021272613419606205\n",
      "train loss:0.0022123016301164628\n",
      "train loss:0.003151485675318573\n",
      "train loss:0.0003982755684959314\n",
      "train loss:0.0035326811208831804\n",
      "train loss:0.03885576344935959\n",
      "train loss:0.0005718492678478085\n",
      "train loss:0.0035458047032279464\n",
      "train loss:0.001078352025893794\n",
      "train loss:0.00019811031911586488\n",
      "train loss:0.0001270050533841244\n",
      "train loss:0.0003339736772191987\n",
      "train loss:0.0007397109830606058\n",
      "train loss:0.0014215404770886375\n",
      "train loss:0.0003669474594844315\n",
      "train loss:0.003856115917850225\n",
      "train loss:0.0020525843044652283\n",
      "train loss:0.002161280039233794\n",
      "train loss:0.0013191879080775666\n",
      "train loss:0.002479791038159162\n",
      "train loss:0.0009472560842907158\n",
      "train loss:0.00033048858813106776\n",
      "train loss:0.022339996544287134\n",
      "train loss:0.003722677190374828\n",
      "train loss:0.0013082126793906679\n",
      "train loss:0.0009367356673920087\n",
      "train loss:0.0008483825325154353\n",
      "train loss:0.0020882917561606575\n",
      "train loss:0.002237670133005684\n",
      "train loss:0.0013373300436974847\n",
      "train loss:0.001527984202635609\n",
      "train loss:0.006579091024478023\n",
      "train loss:0.002899358690193281\n",
      "train loss:0.011119057205269935\n",
      "train loss:0.002714597362316291\n",
      "train loss:0.003952767655303722\n",
      "train loss:0.0016598668923905938\n",
      "train loss:0.0016501507010691357\n",
      "train loss:0.000488561195563158\n",
      "train loss:0.00012070979671794246\n",
      "train loss:0.00019242822510318042\n",
      "train loss:0.002422423177995755\n",
      "train loss:0.0007923668833513379\n",
      "train loss:5.4651034333497785e-05\n",
      "train loss:0.009936078640524907\n",
      "train loss:0.007208948494437895\n",
      "train loss:0.001934127464735172\n",
      "train loss:0.00037859558887517276\n",
      "train loss:0.004645673010073823\n",
      "train loss:0.0017103633738411153\n",
      "train loss:0.0007701811149426748\n",
      "train loss:0.002397409778421809\n",
      "train loss:0.0007775188285175406\n",
      "train loss:0.00032091350238416913\n",
      "train loss:0.0025641743526723594\n",
      "train loss:0.00023033257266275077\n",
      "train loss:0.0031125126563471715\n",
      "train loss:0.0015844391322558946\n",
      "train loss:0.0008773784846515746\n",
      "train loss:0.0015872205851510466\n",
      "train loss:0.0023272172521555563\n",
      "train loss:0.001172453731860201\n",
      "train loss:0.016961633600766194\n",
      "train loss:0.0025506746226367626\n",
      "train loss:0.006306779830182775\n",
      "train loss:0.0005766553591912434\n",
      "train loss:0.006138077629896155\n",
      "train loss:0.001974231855733835\n",
      "train loss:0.00022212141682089446\n",
      "train loss:0.002740065247874346\n",
      "train loss:0.0005513453314595134\n",
      "train loss:0.0037956286952593454\n",
      "train loss:0.0068163426205700515\n",
      "train loss:0.0009295947569850373\n",
      "train loss:0.0029169089981185116\n",
      "train loss:9.79495895132608e-05\n",
      "train loss:0.003563367682783582\n",
      "train loss:0.035883971682087915\n",
      "train loss:0.0035859280435055806\n",
      "train loss:0.002103146653228373\n",
      "train loss:0.0006065369258121842\n",
      "train loss:0.014623429556287428\n",
      "train loss:0.0013839920501053724\n",
      "train loss:0.0011423204280714618\n",
      "train loss:0.002848423886244109\n",
      "train loss:0.00014236940026567097\n",
      "train loss:0.0030873551991722798\n",
      "train loss:0.0018661720245257579\n",
      "train loss:0.06484390016559638\n",
      "train loss:0.001814101571631369\n",
      "train loss:0.0023002083584592646\n",
      "train loss:0.00037487446897432186\n",
      "train loss:0.001960921560496857\n",
      "train loss:0.002992563300874988\n",
      "train loss:0.0005903211760311511\n",
      "train loss:0.0026759701739665982\n",
      "train loss:0.004114571315011773\n",
      "train loss:0.0036105876467400246\n",
      "train loss:0.002259043195327402\n",
      "train loss:0.004311479348065063\n",
      "train loss:2.958653240467954e-05\n",
      "train loss:0.00264541518891455\n",
      "train loss:0.01801112136730405\n",
      "train loss:6.145685061393975e-05\n",
      "train loss:0.00012724696683532715\n",
      "train loss:0.001325275223908883\n",
      "train loss:0.0026188996301235224\n",
      "train loss:0.000787099329634355\n",
      "train loss:0.0013692515868844247\n",
      "train loss:0.002292406059455066\n",
      "train loss:0.026480929171617516\n",
      "train loss:0.003192690160744639\n",
      "train loss:0.00016949133971265253\n",
      "train loss:0.0013311626155175463\n",
      "train loss:0.0003119405736353589\n",
      "train loss:4.062874490060986e-05\n",
      "train loss:0.00011558062683638906\n",
      "train loss:3.4674535454384655e-05\n",
      "train loss:0.0006767618817508051\n",
      "train loss:0.0012094404781859242\n",
      "train loss:0.0013308840858437222\n",
      "train loss:0.0005860805727880945\n",
      "train loss:0.0021594919816799586\n",
      "train loss:0.009269518412862245\n",
      "train loss:6.043242355753883e-05\n",
      "train loss:0.00022112089839541115\n",
      "train loss:0.0007630673905921891\n",
      "train loss:0.0015704558300807592\n",
      "train loss:0.00816653473078355\n",
      "train loss:0.0012550639525137897\n",
      "train loss:0.003147888415998297\n",
      "train loss:0.014419595922066314\n",
      "train loss:0.0050318959047337105\n",
      "train loss:0.0004402499927727201\n",
      "train loss:0.006946904771427559\n",
      "train loss:0.002620044145553964\n",
      "train loss:4.609008700327743e-05\n",
      "train loss:0.000990479792578001\n",
      "train loss:0.0004059269691060367\n",
      "train loss:0.0013802000785302748\n",
      "train loss:0.00776373899911158\n",
      "train loss:0.0002539416637325311\n",
      "train loss:0.000554454608985623\n",
      "train loss:0.000632007734868468\n",
      "train loss:0.006934359697214817\n",
      "train loss:0.002455869271758722\n",
      "train loss:0.00022297978919808115\n",
      "train loss:0.015409598747502784\n",
      "train loss:5.356473318331566e-05\n",
      "train loss:0.004064976636322678\n",
      "train loss:0.00031296193233479585\n",
      "train loss:0.002509366812218648\n",
      "train loss:0.014824994294077935\n",
      "train loss:0.0046741458562703534\n",
      "train loss:0.001397989419350618\n",
      "train loss:0.00066677907275915\n",
      "train loss:0.002362452974260513\n",
      "=== epoch:20, train acc:0.999, test acc:0.984 ===\n",
      "train loss:0.003365352834567045\n",
      "train loss:0.000768833417398628\n",
      "train loss:0.0008172839640577328\n",
      "train loss:0.0015454898643006392\n",
      "train loss:0.0031318968537131404\n",
      "train loss:0.0006634663733102492\n",
      "train loss:0.0014817353441168245\n",
      "train loss:0.0013507111306735769\n",
      "train loss:0.00020196754672858344\n",
      "train loss:0.00907825681115455\n",
      "train loss:0.005795015597056526\n",
      "train loss:0.001879351521376503\n",
      "train loss:0.00021904440712937218\n",
      "train loss:0.01301332679340278\n",
      "train loss:0.0002908582134009822\n",
      "train loss:0.0005761839542331251\n",
      "train loss:0.001134539957680623\n",
      "train loss:7.564571587664114e-05\n",
      "train loss:0.00012928013359157133\n",
      "train loss:0.0008139503865966206\n",
      "train loss:2.8345584650393637e-05\n",
      "train loss:0.01008517086737321\n",
      "train loss:0.00010375989789672666\n",
      "train loss:0.0001503976940087941\n",
      "train loss:0.006432960201721604\n",
      "train loss:0.00042056345810441155\n",
      "train loss:0.0003420169345677229\n",
      "train loss:0.0029612917960128247\n",
      "train loss:0.0007400056490266774\n",
      "train loss:0.0012510320190552988\n",
      "train loss:0.0004762003513423006\n",
      "train loss:0.000494818073193255\n",
      "train loss:0.0021147089597684057\n",
      "train loss:0.0010660125449924339\n",
      "train loss:0.000641022428714209\n",
      "train loss:0.011632866099884419\n",
      "train loss:0.00040835878551797255\n",
      "train loss:0.003273939041415644\n",
      "train loss:0.000287226287906692\n",
      "train loss:0.00015540849585441322\n",
      "train loss:0.00017361984035030558\n",
      "train loss:0.0039008963232593902\n",
      "train loss:0.00041568538347094565\n",
      "train loss:0.00014215874405397593\n",
      "train loss:0.00039981696674540546\n",
      "train loss:0.0007954085954614782\n",
      "train loss:0.00045214969678068263\n",
      "train loss:0.0007274321117845475\n",
      "train loss:2.9777239823321873e-05\n",
      "train loss:0.0003638219513788374\n",
      "train loss:0.011969770757306734\n",
      "train loss:0.00011364821229976424\n",
      "train loss:0.0007343815770171783\n",
      "train loss:7.778318250933647e-05\n",
      "train loss:0.00021437477374911313\n",
      "train loss:0.0005337487548892743\n",
      "train loss:0.0022909976589555914\n",
      "train loss:0.008817075382445406\n",
      "train loss:0.0021983245711350657\n",
      "train loss:0.0023693719000874693\n",
      "train loss:6.267517305922458e-05\n",
      "train loss:0.001826480991195964\n",
      "train loss:0.0015604181872846256\n",
      "train loss:0.008020357367764852\n",
      "train loss:0.0029241201585856595\n",
      "train loss:0.0009779024847049842\n",
      "train loss:0.00012491932957732152\n",
      "train loss:0.027087655023592182\n",
      "train loss:0.0007945416975360893\n",
      "train loss:0.0034089542894189073\n",
      "train loss:0.005039153771516105\n",
      "train loss:0.0013643385176293696\n",
      "train loss:0.0011162834446013013\n",
      "train loss:0.005485742006203034\n",
      "train loss:0.00273219470094706\n",
      "train loss:0.0012574576190883496\n",
      "train loss:0.000575431149342196\n",
      "train loss:0.00021282838500847\n",
      "train loss:0.0027351551058247257\n",
      "train loss:0.004252981045464259\n",
      "train loss:0.0002251940982465975\n",
      "train loss:0.00029192056437000856\n",
      "train loss:0.0018780437733442476\n",
      "train loss:5.385561515575285e-05\n",
      "train loss:0.00012499586299492192\n",
      "train loss:0.0008567096080491557\n",
      "train loss:0.0010211475684502793\n",
      "train loss:0.014288572193850591\n",
      "train loss:0.0043565021841163905\n",
      "train loss:0.0008046084506898051\n",
      "train loss:0.002127943657436374\n",
      "train loss:0.000769778626730216\n",
      "train loss:0.001301559641980819\n",
      "train loss:0.0022583004625431986\n",
      "train loss:0.00010511049132678095\n",
      "train loss:0.003389437075829491\n",
      "train loss:0.00025106759059052456\n",
      "train loss:0.0011474155629394892\n",
      "train loss:0.0013845189397839444\n",
      "train loss:0.0004716918012961921\n",
      "train loss:0.0026944873545195823\n",
      "train loss:0.0010792281571799303\n",
      "train loss:0.0016422022920118695\n",
      "train loss:0.00026546789085497174\n",
      "train loss:0.0004047049238607274\n",
      "train loss:0.0012277331571398957\n",
      "train loss:0.004054306391718791\n",
      "train loss:0.0007744271450412529\n",
      "train loss:0.0007462862286070053\n",
      "train loss:0.0007563822902599205\n",
      "train loss:0.004412532936989599\n",
      "train loss:0.03935708430628479\n",
      "train loss:0.0005328303035944631\n",
      "train loss:0.0006623943333718824\n",
      "train loss:0.003100607894443663\n",
      "train loss:0.003416093158615084\n",
      "train loss:0.00036653122592354237\n",
      "train loss:0.0030755765823035964\n",
      "train loss:0.002745144670583467\n",
      "train loss:0.0018698542927570278\n",
      "train loss:0.00147751833110555\n",
      "train loss:0.0017756989356625316\n",
      "train loss:0.006057691020723287\n",
      "train loss:0.0031228295623704555\n",
      "train loss:0.0005266357714861263\n",
      "train loss:8.50674617595291e-05\n",
      "train loss:0.0017837838999711566\n",
      "train loss:0.0038372258623198745\n",
      "train loss:0.0013230717472159617\n",
      "train loss:0.0011172423564625787\n",
      "train loss:0.0003495507216071944\n",
      "train loss:0.0017187721220975438\n",
      "train loss:0.003060197580755718\n",
      "train loss:0.0008447488735194735\n",
      "train loss:0.00033809837933643493\n",
      "train loss:0.0023873742859656178\n",
      "train loss:0.005358747453859244\n",
      "train loss:0.0016572787183655137\n",
      "train loss:0.0006272085616474202\n",
      "train loss:0.0006312467619361561\n",
      "train loss:0.0009022151565169202\n",
      "train loss:0.00013416305166611496\n",
      "train loss:0.00014922256528651835\n",
      "train loss:0.0006810942854072395\n",
      "train loss:0.004373239359075094\n",
      "train loss:0.00387063981635309\n",
      "train loss:0.004964466920781261\n",
      "train loss:0.0017091161023040015\n",
      "train loss:0.0005188411263148543\n",
      "train loss:0.0003342641049305385\n",
      "train loss:0.00148958436169176\n",
      "train loss:0.0011433287390697183\n",
      "train loss:0.0012476451718046449\n",
      "train loss:0.0016242550029449778\n",
      "train loss:0.00011049346315281898\n",
      "train loss:0.00885559542926199\n",
      "train loss:0.0005493764863130988\n",
      "train loss:0.000751123101920684\n",
      "train loss:0.0005387134235734951\n",
      "train loss:0.0014108906970130765\n",
      "train loss:0.0014512496800782853\n",
      "train loss:0.0008679785568643845\n",
      "train loss:0.00012123002565582465\n",
      "train loss:4.3355879969815305e-05\n",
      "train loss:0.00032804523441747334\n",
      "train loss:0.0035713618677514504\n",
      "train loss:0.00012008959266498628\n",
      "train loss:7.78519031267181e-05\n",
      "train loss:0.002218580083102725\n",
      "train loss:0.00012630515741943367\n",
      "train loss:0.0015814150433466337\n",
      "train loss:0.00048048372751374887\n",
      "train loss:5.038176020843025e-05\n",
      "train loss:0.00019520096684452594\n",
      "train loss:0.0030090523599058355\n",
      "train loss:0.010183008430162573\n",
      "train loss:0.00023826048725884356\n",
      "train loss:0.0008560508771411455\n",
      "train loss:0.0002367703562168618\n",
      "train loss:6.553079869236282e-05\n",
      "train loss:0.00025056161385715955\n",
      "train loss:0.0018414337757138033\n",
      "train loss:0.0030479249518923475\n",
      "train loss:0.0005911971644474656\n",
      "train loss:0.0006669505439616089\n",
      "train loss:0.0007260685651919373\n",
      "train loss:0.0005037042109222603\n",
      "train loss:0.0018395942192004878\n",
      "train loss:0.0011336498716189181\n",
      "train loss:0.0031789532016140228\n",
      "train loss:0.0026181551074748515\n",
      "train loss:0.0003932047252903591\n",
      "train loss:0.000938016876591405\n",
      "train loss:0.00023984404738277117\n",
      "train loss:8.402256847765825e-05\n",
      "train loss:0.0006929568399085174\n",
      "train loss:0.0038248661867202444\n",
      "train loss:0.0017676346442075553\n",
      "train loss:0.008774149168267641\n",
      "train loss:0.0019177122331287468\n",
      "train loss:0.0017744963404778156\n",
      "train loss:0.000385736206337435\n",
      "train loss:0.0017302175900276993\n",
      "train loss:0.0016272117683114088\n",
      "train loss:0.0042766975981042045\n",
      "train loss:0.001351311643054579\n",
      "train loss:0.00012769682912050305\n",
      "train loss:0.00018480098047467158\n",
      "train loss:0.002131521820896097\n",
      "train loss:0.0008081760239536592\n",
      "train loss:0.00337813860883056\n",
      "train loss:0.0016077399991655471\n",
      "train loss:0.0006196061605145242\n",
      "train loss:0.0005720441187853069\n",
      "train loss:0.0015934996583694342\n",
      "train loss:0.00011841618384540155\n",
      "train loss:0.0015973055377423025\n",
      "train loss:0.00023296914494418358\n",
      "train loss:7.86678137568963e-05\n",
      "train loss:0.0004540527428045137\n",
      "train loss:0.0002187556609459887\n",
      "train loss:0.0017977240131278735\n",
      "train loss:0.00295506423860023\n",
      "train loss:0.0003914966079685349\n",
      "train loss:0.001407205008604989\n",
      "train loss:4.292669078556388e-05\n",
      "train loss:0.0014044405022413547\n",
      "train loss:0.0021232435275065987\n",
      "train loss:0.0001925146733914682\n",
      "train loss:0.00043449786614589334\n",
      "train loss:0.0002224932172458582\n",
      "train loss:0.12010141124174165\n",
      "train loss:0.000950020767088246\n",
      "train loss:0.0009843494566326395\n",
      "train loss:0.00016539100667543286\n",
      "train loss:0.0038139070773152805\n",
      "train loss:0.0002772196807354256\n",
      "train loss:0.0001678318908430315\n",
      "train loss:0.004393797494646167\n",
      "train loss:0.0014804573170517504\n",
      "train loss:0.0023479600529629978\n",
      "train loss:0.013569340841507484\n",
      "train loss:0.001619812739014689\n",
      "train loss:0.0008967473040548306\n",
      "train loss:0.0018021347288367327\n",
      "train loss:0.0007404453202957514\n",
      "train loss:0.013109756146024488\n",
      "train loss:0.0007874244793130113\n",
      "train loss:0.00011870626355881637\n",
      "train loss:9.877711486978821e-05\n",
      "train loss:0.002214358835997535\n",
      "train loss:0.0120095015263334\n",
      "train loss:0.0004939087013250793\n",
      "train loss:0.0010750599410382635\n",
      "train loss:0.00015081586201320138\n",
      "train loss:0.003073304443092956\n",
      "train loss:0.001241682970142294\n",
      "train loss:0.0011076303519020484\n",
      "train loss:0.0020630723084593973\n",
      "train loss:0.00021624733184352153\n",
      "train loss:0.002045913931960908\n",
      "train loss:0.0010554240687449482\n",
      "train loss:0.0002319074503710743\n",
      "train loss:0.0005669630196934724\n",
      "train loss:0.00016110391726039465\n",
      "train loss:0.00010405423494591101\n",
      "train loss:0.0021486346854017262\n",
      "train loss:0.0006181205482451342\n",
      "train loss:0.00027979500503878786\n",
      "train loss:0.004631479934313066\n",
      "train loss:0.0005268142107699901\n",
      "train loss:0.002865151008296003\n",
      "train loss:0.0009753234342788779\n",
      "train loss:0.00023290449473806215\n",
      "train loss:0.00021286176894671005\n",
      "train loss:0.0014154364445851753\n",
      "train loss:0.007604670338092474\n",
      "train loss:0.0010012218631847698\n",
      "train loss:0.003729272826706794\n",
      "train loss:0.0008623620613265961\n",
      "train loss:0.0002936832119882977\n",
      "train loss:0.0005551706756233589\n",
      "train loss:0.0030788826266274433\n",
      "train loss:0.00035454551005500594\n",
      "train loss:0.00040110233406227624\n",
      "train loss:0.005386405022508243\n",
      "train loss:0.0020883474437278175\n",
      "train loss:0.0017535801817504496\n",
      "train loss:0.00024879950226573856\n",
      "train loss:0.0005694646181489695\n",
      "train loss:0.0004677068247438757\n",
      "train loss:3.3702586920269035e-05\n",
      "train loss:0.0002749821031970301\n",
      "train loss:0.00035935704346628177\n",
      "train loss:0.0008100830204957155\n",
      "train loss:0.0003684310158322699\n",
      "train loss:0.00040048568019563654\n",
      "train loss:9.687694064961519e-05\n",
      "train loss:0.0006381990169396897\n",
      "train loss:0.0002627923799304832\n",
      "train loss:0.0007077259136601845\n",
      "train loss:0.0001966474132460375\n",
      "train loss:0.00022333073850129236\n",
      "train loss:0.0005264840013729326\n",
      "train loss:0.0023721915030989905\n",
      "train loss:0.003432459456424598\n",
      "train loss:0.000338657029194909\n",
      "train loss:0.0024906869297932534\n",
      "train loss:0.0015070161693783004\n",
      "train loss:0.00013209416945301014\n",
      "train loss:0.0005044109375456948\n",
      "train loss:0.0005723125009322018\n",
      "train loss:0.0007998945536012821\n",
      "train loss:0.0014863459855049303\n",
      "train loss:0.00011127114869125448\n",
      "train loss:0.002263753691185477\n",
      "train loss:0.00029103367015723293\n",
      "train loss:0.00010829096601006946\n",
      "train loss:0.004003184796489182\n",
      "train loss:0.0033253930332061044\n",
      "train loss:0.00022299895845609438\n",
      "train loss:0.000328652350198799\n",
      "train loss:0.0005228909774512132\n",
      "train loss:0.0007253992940934719\n",
      "train loss:5.04528584594856e-05\n",
      "train loss:0.007220451676192055\n",
      "train loss:0.00034026219199135644\n",
      "train loss:0.0017927952535357766\n",
      "train loss:0.0038070238915885023\n",
      "train loss:0.0005453249504477537\n",
      "train loss:0.00015746100005531102\n",
      "train loss:0.0005770302722175401\n",
      "train loss:0.00022560000445337783\n",
      "train loss:0.0004102328307189107\n",
      "train loss:0.0003420576168825528\n",
      "train loss:0.00023395633534444474\n",
      "train loss:0.0017266446903980834\n",
      "train loss:0.003918987449581921\n",
      "train loss:0.002376722818026298\n",
      "train loss:0.0004821561978730604\n",
      "train loss:0.0025093059169936606\n",
      "train loss:0.0073293006029634\n",
      "train loss:0.00013171153968849855\n",
      "train loss:0.0003667342810001514\n",
      "train loss:0.007698749744585855\n",
      "train loss:0.0004929727900048287\n",
      "train loss:0.007601370490432647\n",
      "train loss:0.00031703833980496266\n",
      "train loss:0.0018858590264599338\n",
      "train loss:0.0028934129771824644\n",
      "train loss:0.002642984103468274\n",
      "train loss:0.0010042034169680575\n",
      "train loss:0.00020850612913729874\n",
      "train loss:0.00014061811037878978\n",
      "train loss:9.786558080333583e-05\n",
      "train loss:0.0005416359385076812\n",
      "train loss:0.004177350116893447\n",
      "train loss:0.00015847427376618515\n",
      "train loss:0.0012456988355378632\n",
      "train loss:0.001980706458332182\n",
      "train loss:0.00018346098963794487\n",
      "train loss:0.0008396635000972161\n",
      "train loss:0.00011010618139715556\n",
      "train loss:0.0008166601690185613\n",
      "train loss:0.0003074710077296425\n",
      "train loss:0.001641652537055222\n",
      "train loss:0.0062669030100986935\n",
      "train loss:0.0005319913450647595\n",
      "train loss:0.0005866957178740738\n",
      "train loss:0.0012964279037248024\n",
      "train loss:0.009523617401487458\n",
      "train loss:0.0011299885415194779\n",
      "train loss:0.0019616607602543075\n",
      "train loss:0.003265168993125075\n",
      "train loss:0.000884365348612498\n",
      "train loss:0.0030132623436724864\n",
      "train loss:0.0008559598977360844\n",
      "train loss:0.0007199414923115864\n",
      "train loss:0.0026896433355379596\n",
      "train loss:0.0001499055720258325\n",
      "train loss:0.00048420806747614703\n",
      "train loss:0.0001999740239314328\n",
      "train loss:0.00014454823706930254\n",
      "train loss:0.0008877567985540576\n",
      "train loss:0.00031290628669310817\n",
      "train loss:0.00012900308821937252\n",
      "train loss:0.00010142444209083709\n",
      "train loss:8.442656527848234e-05\n",
      "train loss:0.00024721273240010965\n",
      "train loss:0.002532723262771897\n",
      "train loss:0.0006412734989795505\n",
      "train loss:0.0014535472422813383\n",
      "train loss:0.00020986179012924994\n",
      "train loss:0.0009459102901423304\n",
      "train loss:0.0017320192602928084\n",
      "train loss:0.00026636674905303827\n",
      "train loss:0.006363463542241671\n",
      "train loss:0.0020322332093960574\n",
      "train loss:0.0010678635295484872\n",
      "train loss:0.0012366274364269364\n",
      "train loss:0.0009785918239444723\n",
      "train loss:0.004963053481109643\n",
      "train loss:0.00014711359217600773\n",
      "train loss:0.0006563676773668563\n",
      "train loss:0.001639371227021909\n",
      "train loss:0.00011620534773878439\n",
      "train loss:0.0004829312194125104\n",
      "train loss:0.0008509435541930831\n",
      "train loss:0.00627639563512019\n",
      "train loss:0.000908044326937705\n",
      "train loss:0.000641696210607603\n",
      "train loss:0.0020605154563266338\n",
      "train loss:0.0006025670261640296\n",
      "train loss:0.0006459196364076125\n",
      "train loss:0.0002260545654342822\n",
      "train loss:7.120970160658352e-05\n",
      "train loss:0.0008583134152371452\n",
      "train loss:0.00028320004409647647\n",
      "train loss:0.00019595086544538438\n",
      "train loss:0.0008461594061473172\n",
      "train loss:0.0035496030792286208\n",
      "train loss:0.0009002868487156053\n",
      "train loss:0.00032931177208706935\n",
      "train loss:0.0033605334255238923\n",
      "train loss:0.0013672018130911452\n",
      "train loss:0.0005126334855173621\n",
      "train loss:0.007673617953033895\n",
      "train loss:4.343472324125886e-05\n",
      "train loss:0.0004831750361428928\n",
      "train loss:0.0017135555063590457\n",
      "train loss:0.0016389980652806767\n",
      "train loss:0.002859000147734194\n",
      "train loss:0.0007497008083598636\n",
      "train loss:0.0025069529427567128\n",
      "train loss:5.689837522316518e-05\n",
      "train loss:0.0002934520157845192\n",
      "train loss:0.0004512866811496033\n",
      "train loss:0.0007487862568163836\n",
      "train loss:0.0007338849305013369\n",
      "train loss:0.0003268024125989748\n",
      "train loss:0.0007832408532205164\n",
      "train loss:0.0008623560851055543\n",
      "train loss:0.00015457410609726136\n",
      "train loss:0.0008274854285732443\n",
      "train loss:0.0004619490147837711\n",
      "train loss:0.0002534751797516544\n",
      "train loss:0.00021192887828584128\n",
      "train loss:0.00016111164179529\n",
      "train loss:0.0017957614916084036\n",
      "train loss:0.00027795954052764607\n",
      "train loss:0.002321247782554537\n",
      "train loss:0.0010929739050888794\n",
      "train loss:4.493778058290393e-05\n",
      "train loss:0.005144717747158898\n",
      "train loss:0.0006146602716647131\n",
      "train loss:0.002064042215720252\n",
      "train loss:7.076087545867632e-05\n",
      "train loss:0.0003666652314139034\n",
      "train loss:7.120111081241969e-05\n",
      "train loss:0.003754500883661376\n",
      "train loss:0.002482311471945095\n",
      "train loss:0.00019259785693972437\n",
      "train loss:1.675921833370137e-05\n",
      "train loss:0.0014917825035301701\n",
      "train loss:0.00011274122822473169\n",
      "train loss:0.001172690274175638\n",
      "train loss:0.005024463058514006\n",
      "train loss:0.0012186482008649303\n",
      "train loss:0.0018111745501927792\n",
      "train loss:0.005516258631656073\n",
      "train loss:0.002032000587566725\n",
      "train loss:0.0017752679368990875\n",
      "train loss:0.0015621510713187188\n",
      "train loss:0.002509394075131421\n",
      "train loss:0.0010484760021534118\n",
      "train loss:0.00036545869820951476\n",
      "train loss:0.003416978662903047\n",
      "train loss:0.00034565682444504653\n",
      "train loss:0.0003590274519173769\n",
      "train loss:8.704750852058967e-05\n",
      "train loss:0.0012450706027231232\n",
      "train loss:0.0002149352129973931\n",
      "train loss:0.0001872381558625788\n",
      "train loss:8.790885212607628e-05\n",
      "train loss:0.0014027844880302393\n",
      "train loss:0.0017632502158517473\n",
      "train loss:0.013334774097083509\n",
      "train loss:0.014145446967763648\n",
      "train loss:0.0002988121802805374\n",
      "train loss:0.005967264364966257\n",
      "train loss:0.0007434922794407987\n",
      "train loss:0.0028774169635646953\n",
      "train loss:0.0018539262069051528\n",
      "train loss:0.0005133817620161215\n",
      "train loss:8.562821149305542e-05\n",
      "train loss:0.0010717929980353462\n",
      "train loss:0.002651263390095362\n",
      "train loss:0.00040508606096174\n",
      "train loss:0.0015916722725540936\n",
      "train loss:0.0009463110137810019\n",
      "train loss:0.002774892655418831\n",
      "train loss:0.03289856213307154\n",
      "train loss:0.0019271559743482332\n",
      "train loss:0.001354060082830605\n",
      "train loss:0.0003588918759642537\n",
      "train loss:0.00033894267316152837\n",
      "train loss:0.00242349514262938\n",
      "train loss:0.0006472925164652124\n",
      "train loss:0.0003488276313218482\n",
      "train loss:0.0007775455597351769\n",
      "train loss:0.0009925016285289275\n",
      "train loss:0.00988981880303282\n",
      "train loss:0.00015675455677829206\n",
      "train loss:0.0018421439275450866\n",
      "train loss:0.004293235653487406\n",
      "train loss:6.775669551049075e-05\n",
      "train loss:0.001051033170469852\n",
      "train loss:0.00011685356136327862\n",
      "train loss:4.0913778729489624e-05\n",
      "train loss:0.0005023463000256541\n",
      "train loss:0.00019305746142474981\n",
      "train loss:0.0003359184412167159\n",
      "train loss:0.0026850768843656283\n",
      "train loss:0.0003876158903968066\n",
      "train loss:0.0007979335463278947\n",
      "train loss:0.0019028345480492466\n",
      "train loss:0.00024562141681296556\n",
      "train loss:0.0004206843574303644\n",
      "train loss:0.00016178870198780776\n",
      "train loss:0.0005165795945322478\n",
      "train loss:0.002076923923288553\n",
      "train loss:0.0008058972635132395\n",
      "train loss:0.0003363094922971807\n",
      "train loss:4.49735979834347e-05\n",
      "train loss:0.0016757872263961744\n",
      "train loss:0.00021704336511165303\n",
      "train loss:0.0007423384794373089\n",
      "train loss:0.00488291475267029\n",
      "train loss:0.0003677553878044398\n",
      "train loss:0.0013309977982920908\n",
      "train loss:0.0005284237886843206\n",
      "train loss:1.9346589867456032e-05\n",
      "train loss:0.0003688124301341602\n",
      "train loss:0.00026256003078214263\n",
      "train loss:0.0005281946731142428\n",
      "train loss:0.001455934138691335\n",
      "train loss:0.0015915818857760425\n",
      "train loss:0.0009908439224388158\n",
      "train loss:0.0008628253214761687\n",
      "train loss:0.0012672222361640665\n",
      "train loss:0.0009365625645441459\n",
      "train loss:0.0007719218555658664\n",
      "train loss:0.0003658327476925653\n",
      "train loss:0.0011973860415838567\n",
      "train loss:0.000423469113548574\n",
      "train loss:0.003302714403340021\n",
      "train loss:0.0021272882797038644\n",
      "train loss:0.0006744810328130484\n",
      "train loss:0.0006482259830021135\n",
      "train loss:0.0004260439650836659\n",
      "train loss:0.0006568942425758748\n",
      "train loss:0.0011298779417344145\n",
      "train loss:0.0031027997574977744\n",
      "train loss:0.00044016555628073855\n",
      "train loss:0.0178157166769469\n",
      "train loss:0.0016093777994321795\n",
      "train loss:0.00045354963211944005\n",
      "train loss:0.0003546245861381773\n",
      "train loss:0.001396448428127538\n",
      "train loss:0.00027914192712541756\n",
      "train loss:0.0008870164004898392\n",
      "train loss:0.00013267304598163734\n",
      "train loss:0.0001946859794811447\n",
      "train loss:0.00027978571654618573\n",
      "train loss:0.00025890448913155775\n",
      "train loss:0.0004591031011599008\n",
      "train loss:0.0002274131915207241\n",
      "train loss:0.0001415542380260272\n",
      "train loss:1.074849765618408e-05\n",
      "train loss:0.0002910767857382648\n",
      "train loss:0.0010810897243989442\n",
      "train loss:6.332238062965797e-05\n",
      "train loss:0.00045016577829658184\n",
      "train loss:7.251442474765983e-05\n",
      "train loss:0.0003790171406904371\n",
      "train loss:0.0006988264921289245\n",
      "train loss:0.00028740429323660747\n",
      "train loss:0.000346263515600189\n",
      "train loss:9.707031124431466e-06\n",
      "train loss:0.000375467003133462\n",
      "train loss:0.0015429871583698606\n",
      "train loss:8.674052991296842e-05\n",
      "train loss:0.005955506755755512\n",
      "train loss:0.0025114350533123626\n",
      "train loss:0.0009996565547715631\n",
      "train loss:9.353639637625843e-05\n",
      "train loss:0.0007447988863939362\n",
      "train loss:0.00010895297822324058\n",
      "train loss:0.0008884512359275533\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9869\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('./dl_ex'))  #載入父目錄檔案的設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dl_ex.dataset.mnist import load_mnist\n",
    "from dl_ex.common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "\n",
    "# 如果處理需要時間，則減少數據\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# 保存參數\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 繪製圖形\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7.6 CNN的視覺化\n",
    "使用於CNN的卷積層，究竟可以「看到甚麼」? 以下將透過視覺化卷積層的方式，探討CNN到底執行了甚麼<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.6.1 視覺化第1層權重\n",
    "前面針對MNIST資料集，進行了單純的CNN學習。現在，第1層卷積層的權重形狀為(30,1,5,5)，亦即大小是5x5，色板為1的濾鏡有30個。\n",
    "濾鏡大小為5x5，色板數量為1，代表濾鏡可以當作是1色板的灰階影像。接下來，把卷積層(第1層)的濾鏡顯示成影像。\n",
    "這裡要比較的是學習前與學習後的權重，結果如下圖所示。<br>\n",
    "![7.6.1](./img/7.6.1.PNG)<br><br>\n",
    "\n",
    "由於學習前的濾鏡進行了隨機初始化，所以黑白深淺沒有規則性。然而，結束學習的濾鏡改變成了有規則的影像。由此可知，藉由學習，更新成具有規則性的濾鏡，包括伴隨由白到黑漸層變化的濾鏡、具有塊狀區域(這裡稱作「塊(blob)」)的濾鏡等。<br>\n",
    "圖中，學習前與學習後第1層卷積層的權重:權重的元素是實數，在顯示的影像中，以最小值為黑色(0)，最大值為白色(255)來進行正規化。<br><br>\n",
    "\n",
    "右側這種有規則性的濾鏡，「看到了甚麼」?看到了邊界(顏色變化的邊緣)與塊狀(局部的塊狀區域)等。假設濾鏡的左半部分是白色，右半部分是黑色，那就會變成顯示垂直邊界的濾鏡，如下圖所示。<br>\n",
    "![7.6.1.1](./img/7.6.1.1.PNG)<br>\n",
    "反應水平邊界與垂直邊界的濾鏡:輸出影像1在垂直邊界出現白色像素，而輸出影像2在水平邊界出現許多白色像素。而這種卷積層的濾鏡可以擷取出邊界與塊狀等原始資料，把這種原始資料傳遞給下一層，就是由前面說明的CNN來執行。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAG/9JREFUeJzt3GlwleX9//HvSU6Sk+VkgwChYVHW0DJsoogiLoUCLaAtILXSKTNMLdNOVSxtp3VkqmJH26KjTlu6jJQWRUTAWlEKraxS9lYlAZQlrEISSSB7Qu7fA66cpp0p1+f+jfr/N7/369H94HN9ue6c+5xPwsy5IkEQGAAAMEv6f70BAAD+f0EpAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAEw0TTktLCzIyMry5lJQUeWZycrKUS09Pl2fGYjFv5vTp01ZVVRVxs4N4PO5dk5+fL++hublZyn0cP6uSkpKKIAgKotFokJaW5s3X1dXJe+jevbuUU/dqpv8Mjhw5UhEEQYGZWWpqaqA8E9Go/ojX19dLuTAzc3NzvZkPP/zQampqImZmKSkpgfL8pqamfqR7MAv3mtXW1kq506dPVwRBUJCbmxsUFhZ688rz2qa8vFzKffDBB/LM1tZWNZp4Fjt37hz07t3bu6ClpUXeR1VVlZQLcyJZQ0ODN3PhwgWrr6+PmJllZmYGymdedna2vAf153vmzBl5ZmNjo5RraGhIvGZXEqoUMzIy7NZbb/XmunbtKs/MycmRcp/+9KflmQMHDvRmvvrVryau4/G4TZs2zbtm5syZ8h7UN2yYn5X64TZ48OAys8sfMMrPbdeuXfIe5s6dK+XUvZqZdevWTcpNnz69rO06PT3dRo0a5V3TqVMneR+lpaVSLswvR1OnTvVmfvrTnyauY7GYDR8+3LumV69e8h4mT54s5fLy8uSZ27dvl3IPPfRQmZlZYWGhPffcc958v3795D0sXrxYyj3++OPyzAsXLqjRxLPYu3dv2717t3fBuXPn5H28+uqrUq6pqUmeeeDAAW9m+fLliev8/HybN2+ed824cePkPVy8eFHKhXnN3n//fSm3f//+Mn+K/z4FACCBUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAACcUF/eT05Olr5sX1FRIc986aWXpNycOXPkmXfffbc30/5knrS0NOvfv793zU033STv4e9//7uUe/311+WZp06dkrNml78w/cMf/tCbU7+Qb6adimFmtm7dOnnmsGHD5Gyb7t2728MPP+zNVVdXyzOffPJJKdelSxd5pvLcLlmyJHE9YMAA27Rpk3fNli1b5D2oX/BWvzBuZrZt2zY5a2aWmZlpI0eO9ObCnPqinqQS5pQc9QCD8+fPJ67fe+89mzRpkneNejiEmdmxY8ek3IQJE+SZAwYMkLNml38Wd9xxhzd3+PBheebKlSul3MaNG+WZn/vc56Tc/v37pRx/KQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADihjnmLxWLSUUHvv/++PFM9Eu4f//iHPDMIAjlrdvnIJuX4oR07dsgzS0pKpNy5c+fkmeoRa21yc3Nt6tSp3lyYY8s2b94s5dQjlczMioqK5GybjIwMGzp0qDe3ePFieeYbb7wh5SZPnizPbH+c4H+SlPTP301bW1utvr7eu+bIkSPyHn75y19KOfVoQrPLnwVhBEEgvS+jUf0jST32sKqqSp6ZmZkpZ9s0NzfbmTNnvDn1ODKzf30mruTGG2+UZyqfNampqf9y3bt3b+8a9eg2M7OXX35ZyoU5nvErX/mKlFuxYoWU4y9FAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAAJzQJ9oUFxd7c4MGDZJnqqeI7Nu3T5758MMPezOnT59OXNfV1dnOnTu9a7Zt2ybvoVu3blKuU6dO8sxrrrlGyv3ud78zM7OzZ8/aU0895c2HOZGiZ8+eUi47O1uemZubK2fb1NTUSK/HiRMn5JmdO3eWcikpKfLMpUuXejOVlZWJ66NHj0ondLz55pvyHtQTXfLy8uSZymlCZv/cZyQSsUgk4s2rp9SYme3Zs0fKhTmlRn1u2/9M+/bta6tXr/auOXz4sLyPsrIyKaeeCGamPTMXL15MXNfU1EgnWG3YsEHeQ3l5uZS79dZb5Zn/mxOxroS/FAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAAJxQx7xFo1HpKKyWlhZ55m233Sbljh8/Ls986623vJna2trEdXZ2to0dO9a7JilJ/x0iPT1dyvXo0UOemZ+fL2fNLh/z9sQTT3hzI0eOlGd+73vfk3Jvv/22PLO0tFTOtgmCwFpbW0Ovu5I+ffpIuWhUf9vk5OR4M8nJyYnrqqoq6ciwMM/ikCFDpFy/fv3kmV27dpVy7Y8Wa3+f/0mYZ0GZZxbuyMHu3btLufafR7W1tdIxkQsWLJD3oR6nqBy72SYej3sz7X+m5eXl9utf/9q7pv0xhT5f/vKXpdz1118vz6yrq5OzCv5SBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMCJBEGghyORcjMr+/i284nqFQRBgVmHuy8zd28d9b7MOtxr1lHvy4xn8b9NR70vs3b3diWhShEAgI6M/z4FAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAnGiYcGZmZpCbm+vNJSXpXVtfXy/lGhoa5JmxWMybqampsYaGhojLB5mZmd41ly5dkveQlpYm5YqKiuSZkUhEyu3Zs6ciCIKCSCQSyMNFffv2lXK1tbXyzJqaGil38eLFiiAICszMsrKygvz8fO+aaFR/xFNSUtR9yDOV57a2ttYaGxsjZmYZGRlBTk6Od01GRoa8h9bWVikX5n2rvr5nz56tCIKgIC0tTXqPNTU1yXtQ3w9hngFVVVVV4llMTU0NlM+c5ORkeX5eXp6UC/M+U57F+vp6a2pqSnwuZmVleddcuHBB3oNKeQ+0UX9W7733XuI1u5JQT0tubq7NnTvXmwvzhn377bel3KFDh+SZ/fr182Zee+21xHVmZqZNnDjRu6a6uvoj3YOZ2Y9//GN5plq0kUikTB4a0qJFi6Tcrl275Jnbt2+Xchs2bEjcV35+vj3wwAPeNV27dpX3oWY3b94szzxw4IA3s27dusR1Tk6OzZ4927tmxIgR8h7q6uqkXJj37c6dO6XcE088UWZ2+T02btw4b/748ePyHtT3g/LLU1irV69OPIuxWMxGjRrlXaMUTJsZM2ZIOfV1MDMrLS31Zt56663EdVZWln3+85/3rlm/fr28B/UXr0mTJskzp0+fLuXGjx8vfS7y36cAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOCE+vJ+YWGhPfjgg97cvHnz5Jl79+6Vcp/5zGfkmenp6d5M+9MwsrKy7IYbbvCuWbVqlbyHJ598Usr1799fnjlt2jQ5a2Y2aNAge/HFF725MF8EX7x4sZQL8wX3++67T8pt2LAhcR2NRqUv2584cULex+rVq6Xcpk2b5JlXX321N9P+FJf6+np79913vWsaGxvlPQwaNEjKzZo16yOf+cQTT5iZWUtLi5WXl3vzyhfM26gnqYT50rxyMs2/y8jIsGHDhnlzL730kjzz6NGjUi7Mz+trX/uaN7N79+7EdWNjo7QP9VQyM7OBAwdKuS996UvyzF69eslZBX8pAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOKGOeTt48KDddNNN3tyWLVvkmRMnTpRygwcPlmcq/377o7Vyc3Pt9ttv964Jc2TY1q1bpdyKFSvkmUVFRXLW7PJxd8rxeM8//7w8U82mpKTIM8Mc4demurra/vjHP3pzSqZNbW2tlOvdu7c8Uzlaa+HChYnraDRq+fn53jVhjgxT3rNmZtOnT5dnFhYWylmzy0etjR071purqamRZyrH4ZmZBUEgz1SOovt3RUVF9vjjj3tzx44dk2dWVVVJuaFDh8ozhw8f7s28/vrrieumpibpM0/dq5lZp06dpNzo0aPlmfF4XM4q+EsRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAACfUiTYNDQ128OBBb27WrFnyzK9//etSbvny5fLMs2fPejMtLS2J6yAIrLm52bumX79+8h6mTJki5ZSfZ5u//OUvctbM7OjRo3b33Xd7c4cPH5Zn3nXXXVIuzMkgq1atkrNt6urq7J133vHm+vbtK8+89dZbpZx6CpOZ2c033+zNLF68OHGdlpZmV199tXfNG2+8Ie9h2bJlUu7MmTPyzAkTJshZM7Pu3bvbggULvLnbbrtNnnny5EkpV1dXJ89sf6LLlaxcuTJxXVJSYkOGDPGuOXTokLyPOXPmSLnGxkZ5ZvuTk/6TDz74IHEdiUSkk6muuuoqeQ/9+/eXcpFIRJ4Z5qQxBX8pAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOKGOecvJybFJkyZ5czNnzpRnPvvss1KutLRUnjl48GBv5tixY4nrmpoa27Ztm3dNEATyHiZPnizl8vLy5Jn19fVy1uzyfW3fvt2bGzt2rDzzhhtukHLtf74+a9eulbNt8vPzbfr06d5cmCMH1eOqwhwr1f4It/+k/ZF4sVjMBg4c6F2j3Hub3bt3S7kwx/2tX79ezppdPmpt79693lxmZqY887Of/ayUS09Pl2fG43Ep1/6Yt2g0al26dPGuue++++R9dOrUSco9+uij8szjx4/LWTOz1tZWq6mp8ebUvZqZ9ejRQ8qFeY+FOT5PwV+KAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADiRMKe0RCKRcjMr+/i284nqFQRBgVmHuy8zd28d9b7MOtxr1lHvy4xn8b9NR70vs3b3diWhShEAgI6M/z4FAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADAoRQBAHAoRQAAnGiYcCwWC+LxuDeXmpoqz1SzycnJ8szMzExv5uTJk1ZZWRkxM0tLSwsyMjK8a/Lz8+U9xGIxKVdXVyfPPH/+vJSrrq6uCIKgICcnJ+jatas3H+b1am1tlXJnz56VZ9bU1Ei5pqamiiAICszMkpOTA+WZCIJA3kdOTo6UC/OaKf9+c3OztbS0RMzMMjMzA+U5C/PzzcvLk3LKe6CN+nMtKyurCIKgICkpKYhGQ33cfGR7CKOlpUWNJp7FpKQk6VlUPxPM9GcxjHPnznkzLS0t1traGjEzS0lJCZQ9q+9fM7Nu3bpJuRCvg1VXV0u55ubmxGt2JaGe0ng8brfffrs316NHD3lmr169pJz6xjYzu+6667yZ8ePHJ64zMjLslltu8a6ZOXOmvIeBAwdKub1798oz16xZI+VeeeWVMjOzrl272jPPPOPNX3XVVfIeLl68KOWeeuopeeaWLVukXFlZWVnbdXJysvQGa25ulvcxceJEKRfmNWtsbPRmjh07lrjOz8+3e++917tm0aJF8h5mzJgh5YYNGybPVD+05syZU2ZmFo1GrXPnzt58mF9+1T2EKc8Qv2z8y7OYm5vrXaB+JpiZTZo0ScpFIhF55tNPP+3NVFRUJK5jsZhdc8013jUbN26U9zB79mwpV1lZKc987bXXpNypU6fK/Cn++xQAgARKEQAAh1IEAMChFAEAcChFAAAcShEAAIdSBADACf1tWuV7RGfOnJHn7dmzR8op3yNsM2XKFG8mJSUlcd2zZ0979tlnvWs6deok7+HRRx+Vci+//LI8M8z3P80uf89I+W7U/fffL89cvXq1lAuz1zlz5ki5BQsWJK7j8bjdfPPN3jXK9wTbqF+YbmhokGd+85vf9GZ+8pOfJK4LCgps7ty53jUvvPCCvIekJO133xMnTsgzd+zYIWfb9qAc/NH+O5s+6ncawxwaUFDg/W63mZmVl5cnrlNTU6Xv+oY5JGPUqFFS7sknn5RnjhkzxptZv3594rqoqMgee+wx75qpU6fKe8jOzpZyvXv3lmeq78elS5dKOf5SBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcEId86YeGxbGvn37pFyYo8jWrFnjzRw8eDBx3djYaEeOHPGu+cY3viHv4dVXX5Vy/fv3l2dOmDBByv35z382M7PKykpbtmyZN19SUiLv4Y477pBy3//+9+WZ1157rZRrf8xbEATW1NTkXfP222/L+zh//ryUU48CM9OOPGxubk5cJyUlWWZmpndNmKPLCgsLpVxxcbE8s/0xZ4pu3brZ/PnzvbmNGzfKMysqKqRcXl6ePLNPnz5SbuHChYnr4uJi27lzp3fNL37xC3kfmzZtknIZGRnyTOU9+c477ySuU1JS7FOf+tRHuoe//vWvUq5v377yzDBHOSr4SxEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9SJNkEQWH19vTfXr18/eebs2bOl3OjRo+WZBw4c8GaSkv75+8C5c+fs6aef9q5RT6kxM7v++uul3He+8x155siRI6XcvHnzzMzs0qVLVllZ6c1/61vfkvegnmj05ptvyjPnzJkjZ9vU1NTYtm3bvLnU1FR5ZlZWlpQrLS2VZ27dulXOmpkdP37c5s6d680pJ6i0+dnPfibl1q5dK89saGiQs2ZmnTt3ll7nQYMGyTPVk0yys7PlmUVFRVKu/Yk2Z86csUceecS75qGHHpL38e1vf1vKHTp0SJ6pvCcvXryYuK6oqLDf/va33jVpaWnyHlpaWqTc3/72N3nmyZMn5ayCvxQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAh1IEAMChFAEAcChFAACcUMe8NTc3W3l5uTenHr9kZjZ+/HgpN2LECHnm888/783s2bMncd3S0mLnz5/3rglzJNv8+fOlXJcuXeSZP//5z+Ws2eXjl/r06ePNVVdXyzNnzZol5cLcl3J04L+LRCKWnJzszanHCJr96zNxJWGOHHzxxRe9mWuuuSZx3dLSIh3Nt2TJEnkPTU1NUu7gwYPyzAceeEDK/epXvzKzy8eHKceMFRYWynvIzc2VcmGOxFuxYoWcbVNRUSG9Ho899pg8s6CgQMopn8dt7rnnHm/m97//feK6qanJjh8/7l1z5513yntQj10sKSmRZ+7du1fKRSIRKcdfigAAOJQiAAAOpQgAgEMpAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4kSAI9HAkUm5mZR/fdj5RvYIgKDDrcPdl5u6to96XWYd7zTrqfZnxLP636aj3Zdbu3q4kVCkCANCR8d+nAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgEMpAgDgRMOEU1NTg1gs5s01NjbKM5V5ZmaRSESemZaW5s1cuHDB6uvrI24PQVZWlndNZmamvIeCggI5qzp//ryUO3LkSEUQBAXRaDRITU315uvr6+U9KPPMwt1/TU2NlKuurq4IgqDA7SPIyMjwrmloaJD3od6b8u+2icfj3sy5c+esuro6YmaWkpISKM9vUpL++2xOTo6UC3Nf0aj20VFSUpJ4Fj/q+wqCQMrl5ubKM9Vn4OjRo4lnER1LqFKMxWI2atQob+7QoUPyzOLiYimnvgnNzPr06ePNvPDCC4nrrKwsmzJlinfNddddJ+/hnnvukXLqG9vMbOXKlVJuxowZZWaX3+B9+/b15ktLS+U9FBYWSrm5c+fKM7du3Srl/vSnP5W1XWdkZNiNN97oXfPee+/J++jZs6eUGzp0qDxz7Nix3sz999+fuE5LS7PBgwd714QpsIkTJ0q5kSNHyjPVohk6dGiZ2eX7GjhwoDev/BLRpqWlRcp94QtfkGf26tVLyt11111l/hT+G/HfpwAAOJQiAAAOpQgAgEMpAgDgUIoAADiUIgAADqUIAIAT6nuK8XjcxowZ480lJyfLMy9evCjl1C94m5n0Pa/2XxJubGy0999/37vmueeek/dw7733SrmpU6fKM3/0ox/JWTOzvLw8mzlzpjf34IMPyjPVAwTCfHm/ublZzrZJTU2Vvld47NgxeeaGDRukXJhnUfmOXPtnMT09XXp+t2/fLu9h/vz5Uu6+++6TZ86YMUPOmpn16NHDnnnmGW9u+fLl8sxVq1ZJuaqqKnnmtddeK2fRMfGXIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACAQykCAOBQigAAOJQiAAAOpQgAgBPqmLdYLGYDBgzw5sIcq7Rv3z4p16dPH3nmF7/4RW9mzZo1ieu8vDzp2Krvfve78h7effddKVdaWirPDPNzNTMrLCy0H/zgB97c0qVL5ZnxeFzK1dXVyTOzs7PlbJu8vDy78847vbnhw4fLMx9//HEpt3//fnnmunXrvJkLFy4krvPy8mzatGneNUEQyHsoKSmRcrt27ZJnpqeny1kzs6ysLBs9erQ395vf/EaeeerUKSlXXFwszywqKpKz6Jj4SxEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9SJNikpKVZYWOjNhTkVYtGiRVJOOUmnjXJCS2VlZeK6qanJTpw44V2Tmpoq76GlpUXK1dfXyzP/8Ic/yFmzy/e4ZMkSb+7gwYPyzPz8fCm3cOFCeWZFRYWcbROPx23s2LHe3IgRI+SZO3bskHKrVq2SZyqn37R/BrKzs238+PHeNWlpafIe1JOQlPdAmz179shZM7OGhgbpOduyZYs8MxaLSbkwpystW7ZMzqJj4i9FAAAcShEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9Qxb1lZWTZmzBhvbt++ffJM9aimS5cuyTO3bdvmzdTU1CSuo9GodHzZ2rVr5T2ox7cVFxfLMz/88EM5a2Z2+vRpe+SRR7y5IUOGyDMnTJgg5RobG+WZW7dulXK7d+9OXNfW1tr27du9a+LxuLyPcePGSbny8nJ5ZkNDgzcTBEHiuqqqyl555RXvGvXoNjOTjsMzM9u8ebM888KFC3LW7PLPoaSkxJvr3r27PDM9PV3KhTm+7pZbbpGz6Jj4SxEAAIdSBADAoRQBAHAoRQAAHEoRAACHUgQAwKEUAQBwKEUAABxKEQAAJ9L+NA1vOBIpN7Oyj287n6heQRAUmHW4+zJz99ZR78usw71mHfW+zP4PPIvoWEKVIgAAHRn/fQoAgEMpAgDgUIoAADiUIgAADqUIAIBDKQIA4FCKAAA4lCIAAA6lCACA8z+ItsWYzI6+lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('./dl_ex'))  #載入父目錄檔案的設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dl_ex.ch07.simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx)) #取頂\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 隨機初始化後的權重\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 經過學習後的權重\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7.6.2 利用階層結構擷取資料\n",
    "上面的結果是以第1層卷積層為對象，在第1層卷積層中，擷取出邊界及塊狀等初階資料。在可以重疊多層結構的CNN中，隨著層數加深，擷取的資料(也就是反應強烈的神經元)會變得更抽象化。<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7.7 代表性的CNN\n",
    "CNN提出了各種結構的網路，以下要介紹兩個最重要的網路。第一種是在1998年首度提出的CNN鼻祖LeNet。另一個是深度學習受到矚目後，於2012年提出的AlexNet。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.7.1 LeNet\n",
    "1998年所提出的，用來進行手寫數字辨識的網路，如下圖所示，連續執行卷積層與池化層(正確來說只有「分隔元素」的次取樣層)，最後透過全連接層輸出結果。<br>\n",
    "![7.7.1](./img/7.7.1.PNG)<br>\n",
    "比較LeNet與現在的CNN，可以發現有些差異，如第一個活化函數，LeNet使用的是sigmoid函數，而現在的CNN以ReLU為主。另外，LeNet中，還會以次取樣(subsampling)來縮小中間資料的大小，但是現在的CNN是以最大池化為主。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 7.7.2 AlexNet\n",
    "LeNet問世後，過了將近20年，才提出AlexNet。AlexNet在深度學習中框架中扮演著先驅者的腳色，基本上的結構與LeNet差不多。<br>\n",
    "![7.7.2](./img/7.7.2.PNG)<br>\n",
    "與LeNet的相異點列於以下。\n",
    "* 使用ReLU活化函數\n",
    "* 使用LRN(Local Response Normalization)局部性正規化層\n",
    "* 使用dropout(參考6.4.3)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7.8 重點整理\n",
    "本章學習了CNN，構成CNN的基本模組「卷積層」與「池化層」只要了解後，後續就只剩下如何運用的問題。在處理影像的範疇，幾乎都可以使用CNN。\n",
    "* CNN在全連接層的網路中，新加入了卷積層與池化層\n",
    "* 使用im2col(將影像轉換成陣列的函數)，就可以輕鬆快速地執行卷積層與池化層\n",
    "* 將CNN視覺化，可以瞭解加深層數後，擷取出的高階資料模樣\n",
    "* CNN的代表性網路包括LeNet、AlexNet\n",
    "* 大數據與GPU對深度學習發展有重要的貢獻\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
