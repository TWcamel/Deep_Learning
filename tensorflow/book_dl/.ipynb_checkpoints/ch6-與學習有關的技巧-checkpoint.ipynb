{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 與學習有關的技巧\n",
    "本章要說明在神經網路的學中，成為關鍵的重要部分。本章挑選的主題包括，找出最佳參數的最佳化方法、權重參數的預設值、超參數的設定方法等，每個都是在神經網路的學中，十分重要的主題。另外，還會扼要說明執行Weight decay( 權重衰減 )與 Dropout 等正規化方法，當作過度學解決對策。最後，簡單說明近年來眾多研究中常用到的 Batch Normalization 手法，可以快速進行神經網路( 深度學習 )的學習。所以，讓我們開始本章的內容吧!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 6.1 更新參數\n",
    "神經網路的學習目的，就是找出可以盡量縮小損失函數的參數，亦即找出最佳參數。<br><br>\n",
    "\n",
    "為了找出最佳化參數，計算了參數的梯度( 微分 )。重複執行利用參數的梯度，往梯度方向進行更新參數，最後會逐漸趨近最佳參數，這種方法稱作SGD，是非常單純的作法，但，還有比SGD更聰明的方法存在。以下要說明SGD的缺點，並介紹其他最佳化手法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 6.1.1 SGD\n",
    "$\n",
    "W ← W - \\eta\\frac{\\partial{L}}{\\partial{W}} \\tag{6.1}\n",
    "$\n",
    "這裡的更新權重參數為$W$，損失函數的梯度為$\\frac{\\partial{L}}{\\partial{W}}$。$\\eta$代表學習率，實際上使用的是事先決定的0.01或0.001等數值。以下實現SGD。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 說明\n",
    "初始化時的引數lr代表learning rate( 學習率 )，並且把學率當作實例變數。另外，還定義了update(self, params, grads)方法。在SGD中，會重複呼叫這個方法。我們來使用SGD看看。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SGD的缺點\n",
    "SGD很單純，也很容易執行，但遇到部分問題，可能會變得很沒有效率。以下將藉由思考函數最小值問題，指出SGD的缺點。<br>\n",
    "$\n",
    "f(x,y) = \\frac{1}{20}x^2 + y^2 \\tag{6.2}\n",
    "$<br>\n",
    "由算式可知，這個梯度的特色是，往$y$軸方向變大，往$x$軸方向就會變小。<br><br>\n",
    "\n",
    "換句話說，SGD的缺點是，函數的形狀如果沒有**等向性**，非延伸形狀的函數，就會以沒有效率的路徑進行探索。為了改善SGD的缺點，接下來將介紹Momentum、AdaGrad、Adam等取代SGD的3種手法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 6.1.2 Momentum\n",
    "Momentum的意思是「動量」與物理有關係。此手法可以用下列算式表示。<br>\n",
    "$\n",
    "v ← \\alpha v - \\eta\\frac{\\partial{L}}{\\partial{W}} \\tag{6.3}\n",
    "$<br>\n",
    "$\n",
    "W ← W + v \\tag{6.4}\n",
    "$<br>\n",
    "和前面的SGD一樣，$W$是更新的權重參數，$\\frac{\\partial{L}}{\\partial{W}}$是與$W$有關的損失函數梯度，$\\eta$代表學習率。這裡出現了新的變數$v$，站在物理的角度，$v$代表著「速度」。算式( 6.3 )代表，物體往梯度方向受力，並將這個力量加上物體速度$v$( 6.4 )的物理定律。<br>\n",
    "\n",
    "另外，算式( 6.3 )還出現了$\\alpha v$，這是當物體沒有受力時，逐漸減速的功能( $\\alpha$ 設定為0.9等數值 )。站在物理學的角度，相當於地面磨擦力或是空氣阻力。以下是Momentum的執行過程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.1.3 AdaGrad\n",
    "在神經網路的學習中，學習率的值分常重要( 算式中顯示為$\\eta$ )。學習率太小，會花費太多時間浪費在學習上，反之則會無法正確學習。 <br><br>\n",
    "\n",
    "有個學習率有關的有效技巧，稱為**學習率衰減( learning rate decay )** 逐漸降低學習率的概念，相當於統一降低參數「整個」學習率。進一步發展後，就成為AdaGrad，此手法為針對「每個」參數，準備「客製」值。<br><br>\n",
    "\n",
    "AdaGrad是適應各個參數的元素，一邊調整學習率，一邊學習的手法( AdaGrad 的 Ada 為 Adaptive )。接下來，用算式來顯示AdaGrad的更新方法。<br>\n",
    "$\n",
    "h ← h + \\frac{\\partial{L}}{\\partial{W}} \\bigodot \\frac{\\partial{L}}{\\partial{W}} \\tag{6.5} \n",
    "$ <br>\n",
    "$\n",
    "W ← W -  \\eta \\frac{1}{\\sqrt{h}} \\frac{\\partial{L}}{\\partial{W}} \\tag{6.6}\n",
    "$ <br>\n",
    "\n",
    "和前面的SGD一樣，$W$是更新的權重參數，$\\frac{\\partial{L}}{\\partial{W}}$是與$W$有關的損失函數梯度，$\\eta$代表學習率。這裡出現了新的變數$h$，如算式 ( 6.5 )所示，$h$維持為前面提到的梯度值平方和( 算式 ( 6.5 )的$\\bigodot$代表矩陣各元素相乘再加總 )。更新參數時，乘上$\\frac{1}{\\sqrt{h}}$，調整學習規模。這是代表在參數的元素中，經常變動( 放大 )的元素，由於$h$在分母位置，因此學習率會因故而變小。由於經常變動的參數，學習率會逐漸變小，所以我們可以針對參數的各個元素，執行學習率衰減。接下來，我們來執行他。<br><br>\n",
    "\n",
    "{Note} AdaGrad是把過去所有的梯度當作平方和，全都記錄下來。實際上，無限學習下去，更新量會變成0，完全不在更新。改善這個問題的手法，就是RMSPorp。其手法並非把過去所有的梯度一率加總，而是逐漸忘記過去的梯度，以大幅反應新梯度資料的方式加總。專有名詞稱作「指數移動平均」，以指數函數減少過去梯度的規模。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "這裡必須注意到，最後一行加上了小數值1e-7這一點。這是為了避免當self.h[ key ]出現0時，發生除以0的問題。在大部分的深度學習框架中，這種小數值也可以設定成參數，這裡使用固定值1e-7。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.1.4 Adam\n",
    "Momentum是依照球在碗內來回滾動的物理定律為基準來移動，而AdaGrad是依照各個參數的元素，調整適應的更新步驟。假如將Momentum與AdaGrad相結合，就是Adam了!<br><br>\n",
    "\n",
    "直覺來說，結合以上兩種手法的優點，可以有效探索參數空間。另外，還能進行超參數的「偏權值調整( 偏離校正 )」，這也是Adam的特色之一。接下來我們來執行他。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 6.1.5 利用MNIST資料集比較更新手法\n",
    "以下將以手寫辨識數字為對象，比較前面說明過的SGD、Momentum、AdaGrad、Adam等4種手法。確認每種手法的學習進展有何不同。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "SGD:2.514474623768444\n",
      "Momentum:2.4516549352540915\n",
      "AdaGrad:2.1349995697883264\n",
      "Adam:2.1780539319091217\n",
      "===========iteration:100===========\n",
      "SGD:1.6859501559166479\n",
      "Momentum:0.3372707331357125\n",
      "AdaGrad:0.14306830757444475\n",
      "Adam:0.2093739054507598\n",
      "===========iteration:200===========\n",
      "SGD:1.1151636032834205\n",
      "Momentum:0.4715188773469323\n",
      "AdaGrad:0.20358828072647178\n",
      "Adam:0.32716717947845353\n",
      "===========iteration:300===========\n",
      "SGD:0.6351014519762351\n",
      "Momentum:0.2872815998681052\n",
      "AdaGrad:0.11360218314243592\n",
      "Adam:0.22953152942787947\n",
      "===========iteration:400===========\n",
      "SGD:0.48078727445189384\n",
      "Momentum:0.1743641296125323\n",
      "AdaGrad:0.12601837036153068\n",
      "Adam:0.16702915089512946\n",
      "===========iteration:500===========\n",
      "SGD:0.48497086006342693\n",
      "Momentum:0.23888648222753242\n",
      "AdaGrad:0.09289388719682903\n",
      "Adam:0.1611121362478513\n",
      "===========iteration:600===========\n",
      "SGD:0.31940195545119276\n",
      "Momentum:0.16487866980416677\n",
      "AdaGrad:0.06894860059386664\n",
      "Adam:0.14987688224707174\n",
      "===========iteration:700===========\n",
      "SGD:0.3940331690183011\n",
      "Momentum:0.11634831820401481\n",
      "AdaGrad:0.05068279932467385\n",
      "Adam:0.11810327730186329\n",
      "===========iteration:800===========\n",
      "SGD:0.27795467300928045\n",
      "Momentum:0.08772718981750256\n",
      "AdaGrad:0.030767468780170806\n",
      "Adam:0.044830319060271476\n",
      "===========iteration:900===========\n",
      "SGD:0.2691471075996901\n",
      "Momentum:0.2195512539493598\n",
      "AdaGrad:0.08202244261926606\n",
      "Adam:0.08450993874805654\n",
      "===========iteration:1000===========\n",
      "SGD:0.30410668112835226\n",
      "Momentum:0.13355479783449645\n",
      "AdaGrad:0.06983489138043922\n",
      "Adam:0.06911196450034655\n",
      "===========iteration:1100===========\n",
      "SGD:0.27798432218287417\n",
      "Momentum:0.1529866308717747\n",
      "AdaGrad:0.045676887920477714\n",
      "Adam:0.08448073846504658\n",
      "===========iteration:1200===========\n",
      "SGD:0.23874119367269528\n",
      "Momentum:0.02968002408714387\n",
      "AdaGrad:0.023064855192620887\n",
      "Adam:0.033130004016650914\n",
      "===========iteration:1300===========\n",
      "SGD:0.21004538826106672\n",
      "Momentum:0.09424372597739539\n",
      "AdaGrad:0.07331600848434226\n",
      "Adam:0.12961015222950986\n",
      "===========iteration:1400===========\n",
      "SGD:0.3463619780540365\n",
      "Momentum:0.11000106479015485\n",
      "AdaGrad:0.04019081565654273\n",
      "Adam:0.0695852785319855\n",
      "===========iteration:1500===========\n",
      "SGD:0.34207933748094893\n",
      "Momentum:0.1795972554381534\n",
      "AdaGrad:0.09158835634125838\n",
      "Adam:0.13624808219265191\n",
      "===========iteration:1600===========\n",
      "SGD:0.26248833400262045\n",
      "Momentum:0.0642978585862373\n",
      "AdaGrad:0.024106181020037303\n",
      "Adam:0.06599059878216693\n",
      "===========iteration:1700===========\n",
      "SGD:0.28812138550907196\n",
      "Momentum:0.0960200369180252\n",
      "AdaGrad:0.07161353474756577\n",
      "Adam:0.08005199682253404\n",
      "===========iteration:1800===========\n",
      "SGD:0.12439360950929487\n",
      "Momentum:0.05053444320089705\n",
      "AdaGrad:0.029974958325053678\n",
      "Adam:0.07062038994454554\n",
      "===========iteration:1900===========\n",
      "SGD:0.2521556841798347\n",
      "Momentum:0.04026364569158376\n",
      "AdaGrad:0.015758424771944987\n",
      "Adam:0.05218928961627017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('./dl_ex')) #載入父目錄檔案的設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dl_ex.dataset.mnist import load_mnist\n",
    "from dl_ex.ch05.two_layer_net import TwoLayerNet\n",
    "from dl_ex.common.util import smooth_curve\n",
    "from dl_ex.common.multi_layer_net import MultiLayerNet\n",
    "from dl_ex.common.optimizer import *\n",
    "\n",
    "# 0:MNIST載入\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "# 1:實驗設定\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = []    \n",
    "\n",
    "\n",
    "# 2:開始學習\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in optimizers.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "# 3.繪圖\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "由圖可以看出，除了SGD外的3種手法的學速度的確較快，有時最後的辨識效能也比較好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## 6.2 權重的預設值\n",
    "在神經網路中，特別重要的就是權重的預設值。事實上，權重的預設值應該設定為哪種數值，常會影響到神經網路的學習成功與否。本節將針對建議的權重預設值來說明，利用實驗，確認實際的神經網路學習速度。<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### 6.2.1 權重的預設值變成0?\n",
    "後面將會介紹 Weight decay ( 權重衰減 )手法，當作控制過度學習，提高一般化功能的技巧。其是以縮小權重參數值為目的，進行學習的手法。縮小權重值，可以避免過度學習。<br><br>\n",
    "\n",
    "如果想要縮小權重值，預設值也盡量從小數值開始，這是屬於正攻法。想要縮小權重值，把權重的預設值全部設為0呢?其實，這是一個糟糕的想法，因為如此會造成不正確學習。<br><br>\n",
    "\n",
    "為什麼預設值不能設為均一值(或是全部為0)?因為誤差反向傳播法中，所有的權重值都會均一更新。假設在雙層神經網路中，把第1層與第2層的權重設定為0，進行正向傳播時，因為輸入層的權重為0，會傳遞相同值給第2層神經元。第2層神經元全部輸入相同的值，在反向傳播時，第2層的權重會更新成一樣。因此，我們需要的是隨機的預設值。\n",
    "\n",
    "### 6.2.2 隱藏層的活性化分布\n",
    "觀察隱藏層的活性化，可以得到許多資料。以下進行一個簡單的實驗，觀察隱藏層的活性化會隨著權重的預設值產生何種變化。這裡進行的實驗室，在5層的神經網路中(活化函數使用Sigmoid函數)，傳遞隨機產生的輸入資料，利用分布圖繪製各層的活性化資料分布。\n",
    "\n",
    "{Note} 這裡把活化函數之後的輸出資料稱為「活性化( Activation )」，但是根據文獻紀載，在各層之間流動的資料，稱可稱為活性化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000個輸入資烙\n",
    "node_num = 100  # 各隱藏層的神經元數目\n",
    "hidden_layer_size = 5  # 5層隱藏層\n",
    "activations = {}  # 放置活性化資料的地方\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 隨機產生的初始權重預設值！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num) \n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 活性化開始！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 繪圖\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "如圖所示，能看出各層的活性化分布偏向0與1。這裡使用的Sigmoid函數為呈現s型曲線的函數，但隨著sigmoid函數的輸出趨近於0(或1)，該微分值也會趨近於0。因此，在偏向0與1的資料分布中，反向傳播的梯度值會逐漸變小、消失。這就是所謂的「梯度消失( gradient vanishing )」問題。在層數多的深度學習中，梯度消失會造成嚴重的問題。<br><br>\n",
    "\n",
    "然而，若我們使用sd為0.01的常態分配做模擬，會發現集中分配在0.5附近。和前面偏向0與1不同，沒有引起梯度消失問題。可是，活性化分布出現特定偏差，在表現力方面，將會產生重大問題。因為，當多個神經元輸出幾乎一模一樣的數值時，就沒有存在的意義。例如，100個神經元的輸出幾乎相同數值，代表只用1個神經元，就能表現出一樣的情況。因此，活性化分布若出現偏差情況，會有「表現力受限」的問題，造成無法正確學習。因此我們需要在各層傳遞具有適當多樣性的資料。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.2.3 利用MNIST資料集比較權重預設值\n",
    "讓我們以實際資料為對象，提供不同的權重預設值，檢視對神經網路的學會造成何種影響。以下利用「std = 0.01」、「Xavier預設值」、「He預設值」等三種情況來進實驗。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000個輸入資烙\n",
    "node_num = 100  # 各隱藏層的神經元數目\n",
    "hidden_layer_size = 5  # 5層隱藏層\n",
    "activations = {}  # 放置活性化資料的地方\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 隨機產生的初始權重預設值！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    # w = np.random.randn(node_num, node_num) * 0.01\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 活性化分布的函數！\n",
    "    z = sigmoid(a)\n",
    "    # z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 繪圖\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    plt.xlim(0.1, 1)\n",
    "    plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
