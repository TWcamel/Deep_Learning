{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 誤差反向傳播反\n上一章節說明了神經網路的「學習」。此時，神經網路權重參數的梯度( 精確來說是與參數有關的損失函數梯度 )，是利用數值微分計算出來。數值微分很簡單，執行起來也不難，但運算卻較花時間。因此本章要學習的「誤差反向傳播法」，是能以良好效率計算出權重參數梯度的方法。\u003cbr\u003e\u003cbr\u003e\n\n計算誤差反向傳播法有兩種方法，一是利用「算式」，另一種是用「計算圖( computational graph )」。前者雖簡潔但可能忽略掉本質，迷失於算式中，後者則反之。\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 5.1 計算圖\n計算圖中的+節點代表複合函數從內到外拆解( forward )，$\\times$節點則代表複合函數由外到內拆解( backward )，而這兩種拆解動作作用於計算圖上時，是以「層級」為單位。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 5.2 執行單純的層級\n層級是用來執行forward()和backward()等共通方法( 介面 )的部分。 forward()是對應正向傳播，backward()則對應到反向傳播。\u003cbr\u003e\u003cbr\u003e\n接著要執行乘法層。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "class MulLayer:\n    def __init__(self):\n        self.x \u003d None\n        self.y \u003d None\n        \n    def forward(self, x, y):\n        self.x \u003d x\n        self.y \u003d y\n        out \u003d x* y\n        \n        return out\n    def backward(self, dout):\n        dx \u003d dout * self.y #x與y相反\n        dy \u003d dout * self.x\n        \n        return dx, dy\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 解說\n在 __init__中，進行實例變數x與y的初始化，用來保持正向傳播時的輸入值。在forward()中，取得x與y等兩個引數。然而，在backward()中，針對上層傳來的微分( dout )，乘上正向傳播的「相反值」在傳遞給下層。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 5.3 執行活化函數層\n接下來我們將計算圖的思考方法套用在神經網路上，這裡把神經網路的「層」當一個類別來處理。首先，要執行活化函數ReLU與Sigmoid層。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### ReLU層",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "# ReLU層\nclass Relu:\n    def __init__(self):\n        self.mask \u003d None # 實例變數mask\n                         # 是由True / False 組成的Numpy陣列\n    \n    def forward(self, x):\n        self.mask \u003d (x \u003c\u003d 0)\n        out \u003d x.copy()\n        out[self.mask] \u003d 0\n        \n        return out\n    \n    def backward(self, dout):\n        dout[self.mask] \u003d 0\n        dx \u003d dout\n        \n        return dx\n\n# 如果正向傳播時的輸入值小於0，反向傳播的值就會變成0\n# 因此，在反向傳播中，使用正向傳播的mask，針對上層傳來的dout\n# 將mask元素為True的位置設定為0\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Sigmoid層\nSigmoid函數為：\u003cbr\u003e\n$\ny \u003d \\frac{1}{1+exp(-x)} \\tag{5.1}\n$\u003cbr\u003e\n其中，裡面運算子元素有「$\\times$」、「+」、「exp」、「/」節點。\u003cbr\u003e\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nclass Sigmoid:\n    def __init__(self):\n        self.out \u003d None\n    \n    def forward(self, x):\n        out \u003d 1 / (1+ np.exp(-x))\n        self.out \u003d out\n    \n        return out\n    \n    def backward(self, dout):\n        dx \u003d dout * (1 - self.out) # 由公式解推導而得到!( p132 ) \n        \n        return dx\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 5.4 執行 Attine \n### Affine 層 ( 仿射轉換 )\n在神經網路的正向傳播中，使用了矩陣乘積(np.dot())，計算含有權重訊號的總和。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "class Affine: #證明在p134~p136\n    def __init__(self, W, b):\n        self.W \u003d W\n        self.b \u003d b\n        self.x \u003d None\n        self.dW \u003d None\n        self.db \u003d None\n        \n    def forward(self, x):\n        self.x \u003d x\n        out \u003d np.dot(x, self.W) + self.b\n        \n        return out\n    \n    def backward(self, dout):\n        dx \u003d np.dot(dout, self.W.T)\n        self.dW \u003d np.dot(self.x.T, dout)\n        self.db \u003d np.sum(dout, axis\u003d0)\n        \n        return dx\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% \n",
          "is_executing": false
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 5.5 執行 Softmax 層\n最後要說明的是softmax函數。Soft函數是將輸入值正規化後再輸出。如下圖所示。\u003cbr\u003e\n![5.5 softmax](./img/5-5.jpg)\u003cbr\u003e\n由圖片中可知，神經網路的處理分成「推論」以及「學習」階段，解釋如圖。\u003cbr\u003e\n接下來要執行的softmax層，這裡要執行的是，包含損失函數的交叉熵誤差( cross entropy error )之「 Softmax-with-Loss 層 」。\u003cbr\u003e\n![5.5 softmax with loss](./img/5.5.1.PNG)\u003cbr\u003e\n \n由上圖可知，來自Softmax的層的反向傳播，形成($y_1-t_1, y_2-t_2, y_3-t_3$)這樣的「整齊」結果。($y_1,y_2,y_3$)是Softmax層的輸出，($t_1, t_2, t_3$)則是訓練資料，所以($y_1-t_1, y_2-t_2, y_3-t_3$)是Softmax層的輸出與訓練資料的差分。因為，在神經網路的反向傳播中，這個差分的誤差會傳遞給上一層。對於神經網路的學習而言，是非常重要的性質。\u003cbr\u003e\u003cbr\u003e\n\n神經網路的學習目的是，調整權重參數，讓經網路的輸出(Softmax的輸出)趨近訓練資料。因此神經網路的輸出與訓練吃資料的誤差，必須有效率地傳送給上一層。剛才($y_1-t_1, y_2-t_2, y_3-t_3$)的結果等於Softmax層輸出與訓練資料的差直接表現出目前神經網路的輸出與訓練資料的誤差。\u003cbr\u003e\n\n{note}使用MSE當作Softmax函數的損失函數時，反向傳播也會得到($y_1-t_1, y_2-t_2, y_3-t_3$)這種整齊的結果。\u003cbr\u003e\u003cbr\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\ndef cross_entropy_error(y, t):\n    delta \u003d 1e-7\n    return -np.sum(t * np.log(y + delta))\n\ndef softmax (a):\n    c \u003d np.max(a)\n    exp_a \u003d np.exp(a-c) #防範溢位!\n    sum_exp_a \u003d np.sum(exp_a)\n    y \u003d exp_a / sum_exp_a\n    \n    return y\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss \u003d None # 損失函數\n        self.y \u003d None    # Softmax的輸出\n        self.t \u003d None    # 訓練資料( one-hot vector ) \n    \n    def forward(self, x, t):\n        self.t \u003d t\n        self.y \u003d softmax(x)\n        self.loss \u003d cross_entropy_error(self.y, self.t)\n        \n    def backward(self, dout \u003d 1):\n        batch_size \u003d self.t.shape[0]\n        dx \u003d (self.y - self.t) / batch_size #此處以計算圖叫好理解 (看上面的圖)\n        \n        return dx\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 5.6 執行誤差反向傳播法\n組合上一節執行過的各個層級，就能像組合樂高般，建構出神經網路。因此，這裡要一邊組合前面執行過的各層，一邊建構神經網路。\u003cbr\u003e\u003cbr\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 5.6.1 神經網路的學習總圖\n在實際開始執行前，我們先再次確認神經網路的學習總圖。接著再列出神經網路的學習步驟。\u003cbr\u003e\u003cbr\u003e\n\n* 前提\u003cbr\u003e\n神經網路具有可適應的權重與偏權值，調整權重與偏權值，以適應訓練資料，這種過程稱作「學習」。神經網路的學習可以分成以下4步驟進行。\u003cbr\u003e\u003cbr\u003e\n\n* 步驟一( 小批次 )\u003cbr\u003e\n從訓練資料中，隨機抽取部分資料。\n\n* 步驟二( 計算梯度 )\u003cbr\u003e\n計算與各權重參數有關的損失函數梯度。\n\n* 步驟三( 更新參數 )\u003cbr\u003e\n往梯度方向微量更新權重參數。\n\n* 步驟四( 重複 )\u003cbr\u003e\n重複以上步驟。\u003cbr\u003e\u003cbr\u003e\n\n與前面說明過的不同的是，步驟二的「計算梯度」。上一章節使用了數值微分來算梯度，數值微分可以輕易執行，但是相對來說，需要花的時間與之相比會較高。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 5.6.2 執行對應誤差反向傳播法的神經網路\n接下來，我們將以TwoLayerNet來執行。首先，整理這個類別的實例變數與方法如下圖。\u003cbr\u003e\n![5.6誤差反向傳播法的神經網路](./img/5.6.jpg) \u003cbr\u003e\u003cbr\u003e\n\n大部分內容與上一章的「4.5執行學習演算法」共通。與之不同的是，這裡主要使用的是層級。利用層級傳播，就能達到處理辨識結果(predict())、計算梯度(gradient())的目的。\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": "import sys, os\nsys.path.append(os.path.abspath(\u0027./dl_ex\u0027)) #載入父目錄檔案的設定\nimport numpy as np\nfrom dl_ex.common.layers import *\nfrom dl_ex.common.gradient import numerical_gradient\nfrom collections import OrderedDict\n\nclass TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, \n                 weight_init_std\u003d0.01):\n        # 權重初始化\n        self.params \u003d {}\n        self.params[\u0027W1\u0027] \u003d weight_init_std * \\\n                            np.random.randn(input_size, hidden_size)\n        \n        self.params[\u0027b1\u0027] \u003d np.zeros(hidden_size)\n        \n        self.params[\u0027W2\u0027] \u003d weight_init_std * \\\n                            np.random.randn(hidden_size, output_size)\n        \n        self.params[\u0027b2\u0027] \u003d np.zeros(output_size)\n\n        # 產生各層級\n        self.layers \u003d OrderedDict()\n        self.layers[\u0027Affine1\u0027] \u003d \\\n            Affine(self.params[\u0027W1\u0027], self.params[\u0027b1\u0027])\n        self.layers[\u0027Relu1\u0027] \u003d Relu()\n        self.layers[\u0027Affine2\u0027] \u003d \\\n            Affine(self.params[\u0027W2\u0027], self.params[\u0027b2\u0027])\n        \n        self.lastLayer \u003d SoftmaxWithLoss()\n            \n    def predict(self, x):\n        for layer in self.layers.values():\n            x \u003d layer.forward(x)\n    \n    # x:輸入資料, t:訓練資料\n    def loss(self, x, t):\n        y \u003d self.predict(x)\n        \n        return self.lastLayer.forward(y, t)\n    \n    def accuracy(self, x, t):\n        y \u003d self.predict(x)\n        y \u003d np.argmax(y, axis\u003d1)\n        if t.ndim !\u003d 1 : \n            t \u003d np.argmax(t, axis \u003d 1)\n        \n        accuracy \u003d np.sum( y \u003d\u003d t) /  float(x.shape[0])\n        return accuracy\n    \n    # x:輸入資料, t:訓練資料\n    def numerical_gradient(self, x, t):\n        loss_W \u003d lambda W: self.loss(x, t)\n        \n        grads \u003d {}\n        grads[\u0027W1\u0027] \u003d numerical_gradient(loss_W, self.params[\u0027W1\u0027])\n        grads[\u0027b1\u0027] \u003d numerical_gradient(loss_W, self.params[\u0027b1\u0027])\n        grads[\u0027W2\u0027] \u003d numerical_gradient(loss_W, self.params[\u0027W2\u0027])\n        grads[\u0027b2\u0027] \u003d numerical_gradient(loss_W, self.params[\u0027b2\u0027])\n        \n        return grads\n    \n    def gradient(self, x, t):\n        # forward\n        self.loss(x, t)\n        \n        # backward\n        dout \u003d 1\n        dout \u003d self.lastLayer.backward(dout)\n        \n        layers \u003d list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout \u003d layer.backward(dout)\n            \n        # 設定\n        grads \u003d {}\n        grads[\u0027W1\u0027] \u003d self.layers[\u0027Affine1\u0027].dW\n        grads[\u0027b1\u0027] \u003d self.layers[\u0027Affine1\u0027].db\n        grads[\u0027W2\u0027] \u003d self.layers[\u0027Affine2\u0027].dW\n        grads[\u0027b2\u0027] \u003d self.layers[\u0027Affine2\u0027].db\n        \n        return grads\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 解說\n這裡有個重要的概念，就是把神經網路的層當作OrderedDict這一點極為重要。OrderedDict是有序字典。因此，神經網路的正向傳播，只要按造順序呼叫出各層的forward()方法，就能處理完神經網路的前向傳播。另外，反向傳播也只要依造相反方向來呼叫各層即可。\u003cbr\u003e\u003cbr\u003e\n\nAffine層以及ReLU層各自在內部已經處理好正向傳播與反向傳播，而在此進行的是，按造正確順序連結各層，並且依序( 或相反順序 )呼叫出各層即可。\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 5.6.3 梯度檢查\n以數值微分的方式來檢查誤差梯度反向傳播法，這種方式稱為「梯度檢查 ( gradient check )」。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "W1:5.052772720822389e-10\nb1:3.051291303630098e-09\nW2:5.828749507746217e-09\nb2:1.3928355015319083e-07\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import sys, os\nsys.path.append(os.path.abspath(\u0027./dl_ex\u0027)) #載入父目錄檔案的設定\nimport numpy as np\nfrom dl_ex.dataset.mnist import load_mnist\nfrom dl_ex.ch05.two_layer_net import TwoLayerNet\n\n# 載入資料\n(x_train, t_train), (x_test, t_test) \u003d load_mnist(normalize\u003dTrue, one_hot_label\u003dTrue)\n\nnetwork \u003d TwoLayerNet(input_size \u003d 784, hidden_size \u003d 50, output_size \u003d 10)\n\nx_batch \u003d x_train[:3]\nt_batch \u003d t_train[:3]\n\ngrad_numerical \u003d network.numerical_gradient(x_batch, t_batch)\ngrad_backprop \u003d network.gradient(x_batch, t_batch)\n\n# 計算各權重的絕對誤差平均值\nfor key in grad_numerical.keys():\n    diff \u003d np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n    print(key + \":\" + str(diff))\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 解說\n這裡先算出各權重參數的元素差絕對值，在計算出平均值當作誤差。\u003cbr\u003e\u003cbr\u003e\n\n由結果可知，數值微分與反向傳播誤差法計算而得的梯度誤差非常小。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 5.6.4 使用誤差反向傳播法學習\n最後，我們實際用反像誤差進行神經網路的學習。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "0.11738333333333334 0.1225\n",
            "0.5079833333333333 0.5131\n",
            "0.5680666666666667 0.567\n",
            "0.6948166666666666 0.699\n",
            "0.7880166666666667 0.7922\n",
            "0.8204 0.8262\n",
            "0.8428833333333333 0.8484\n",
            "0.8579166666666667 0.8618\n",
            "0.8691833333333333 0.8747\n",
            "0.8747666666666667 0.8796\n",
            "0.88105 0.8857\n",
            "0.8856 0.8884\n",
            "0.8881166666666667 0.8916\n",
            "0.8912666666666667 0.8948\n",
            "0.8942666666666667 0.8974\n",
            "0.8968666666666667 0.8993\n",
            "0.8994 0.9017\n",
            "0.90105 0.9036\n",
            "0.9023333333333333 0.905\n",
            "0.9035833333333333 0.9068\n",
            "0.9051166666666667 0.9071\n",
            "0.9064 0.9076\n",
            "0.9084666666666666 0.9096\n",
            "0.90975 0.9111\n",
            "0.91015 0.9126\n",
            "0.9115833333333333 0.9138\n",
            "0.9123 0.9129\n",
            "0.9133333333333333 0.9148\n",
            "0.9145833333333333 0.9167\n",
            "0.9154 0.9174\n",
            "0.9166 0.9179\n",
            "0.9173 0.9182\n",
            "0.9177333333333333 0.9196\n",
            "0.9188166666666666 0.9203\n",
            "0.9189666666666667 0.921\n",
            "0.92015 0.9229\n",
            "0.9214333333333333 0.923\n",
            "0.9222 0.923\n",
            "0.9222833333333333 0.9253\n",
            "0.9231833333333334 0.9253\n",
            "0.9241 0.9259\n",
            "0.9252333333333334 0.927\n",
            "0.9254666666666667 0.9274\n",
            "0.9262166666666667 0.9283\n",
            "0.92675 0.9291\n",
            "0.9279666666666667 0.9292\n",
            "0.92825 0.9289\n",
            "0.92885 0.93\n",
            "0.9292166666666667 0.9308\n",
            "0.9300666666666667 0.9311\n",
            "0.9304666666666667 0.9313\n",
            "0.9315 0.9313\n",
            "0.9319166666666666 0.9327\n",
            "0.9327166666666666 0.9327\n",
            "0.93345 0.9336\n",
            "0.9338 0.9334\n",
            "0.9349666666666666 0.9342\n",
            "0.9356666666666666 0.9352\n",
            "0.9356333333333333 0.9349\n",
            "0.93645 0.9351\n",
            "0.93695 0.9367\n",
            "0.9374666666666667 0.9358\n",
            "0.9387666666666666 0.9368\n",
            "0.93845 0.9379\n",
            "0.93935 0.9382\n",
            "0.9397666666666666 0.9383\n",
            "0.9406 0.938\n",
            "0.94075 0.9388\n",
            "0.9411666666666667 0.9383\n",
            "0.9417 0.9389\n",
            "0.9416 0.9395\n",
            "0.9423 0.9399\n",
            "0.9428666666666666 0.9404\n",
            "0.9432333333333334 0.9411\n",
            "0.94355 0.9418\n",
            "0.9442 0.9419\n",
            "0.9447333333333333 0.9422\n",
            "0.94525 0.9439\n",
            "0.9452 0.9437\n",
            "0.9455 0.9438\n",
            "0.9460166666666666 0.9439\n",
            "0.9464333333333333 0.9448\n",
            "0.9468666666666666 0.9449\n",
            "0.9469166666666666 0.9454\n",
            "0.9475333333333333 0.9448\n",
            "0.9477666666666666 0.9455\n",
            "0.9479666666666666 0.9463\n",
            "0.9483 0.946\n",
            "0.9486 0.947\n",
            "0.9489833333333333 0.9471\n",
            "0.9491833333333334 0.9465\n",
            "0.9492333333333334 0.9469\n",
            "0.94965 0.947\n",
            "0.94975 0.9479\n",
            "0.95005 0.9482\n",
            "0.9505666666666667 0.948\n",
            "0.9506333333333333 0.9482\n",
            "0.9512 0.9491\n",
            "0.9515333333333333 0.9493\n",
            "0.9517 0.9493\n",
            "0.9521 0.9489\n",
            "0.9520333333333333 0.9503\n",
            "0.9523166666666667 0.9494\n",
            "0.9527333333333333 0.9505\n",
            "0.9530833333333333 0.9512\n",
            "0.9530666666666666 0.9507\n",
            "0.9535666666666667 0.9519\n",
            "0.9538666666666666 0.9515\n",
            "0.9539833333333333 0.9514\n",
            "0.9540166666666666 0.9517\n",
            "0.95455 0.9518\n",
            "0.9545333333333333 0.9519\n",
            "0.9550166666666666 0.9521\n",
            "0.9551833333333334 0.952\n",
            "0.9550666666666666 0.9525\n",
            "0.9554 0.9526\n",
            "0.9562166666666667 0.9527\n",
            "0.9558333333333333 0.9533\n",
            "0.95605 0.9528\n",
            "0.9564666666666667 0.9535\n",
            "0.95685 0.9535\n",
            "0.9568333333333333 0.9538\n",
            "0.9570666666666666 0.9541\n",
            "0.9575833333333333 0.9541\n",
            "0.9574666666666667 0.9541\n",
            "0.9579666666666666 0.9542\n",
            "0.958 0.9541\n",
            "0.95855 0.9543\n",
            "0.9584333333333334 0.9549\n",
            "0.9582666666666667 0.9555\n",
            "0.9586666666666667 0.9558\n",
            "0.9587166666666667 0.955\n",
            "0.9590833333333333 0.9547\n",
            "0.95955 0.9549\n",
            "0.9595833333333333 0.9549\n",
            "0.9598166666666667 0.9555\n",
            "0.9598833333333333 0.9559\n",
            "0.9599166666666666 0.9556\n",
            "0.96065 0.9567\n",
            "0.96065 0.9558\n",
            "0.9607333333333333 0.9561\n",
            "0.9612333333333334 0.9559\n",
            "0.9612833333333334 0.957\n",
            "0.9616 0.9566\n",
            "0.9615666666666667 0.9573\n",
            "0.96145 0.9574\n",
            "0.9617333333333333 0.9574\n",
            "0.9623666666666667 0.9577\n",
            "0.96205 0.9579\n",
            "0.9622666666666667 0.9579\n",
            "0.9626166666666667 0.9581\n",
            "0.9624666666666667 0.9586\n",
            "0.9630166666666666 0.9581\n",
            "0.9629166666666666 0.9591\n",
            "0.9631166666666666 0.9588\n",
            "0.96325 0.9593\n",
            "0.9636166666666667 0.9595\n",
            "0.9639166666666666 0.9592\n",
            "0.9637166666666667 0.9594\n",
            "0.96395 0.9595\n",
            "0.9641 0.9597\n",
            "0.96415 0.9606\n",
            "0.9645333333333334 0.9606\n",
            "0.96445 0.9602\n",
            "0.9645 0.96\n",
            "0.96525 0.9605\n",
            "0.9649666666666666 0.9598\n",
            "0.9651333333333333 0.9598\n",
            "0.9653 0.9608\n",
            "0.9654666666666667 0.9604\n",
            "0.96585 0.9609\n",
            "0.96535 0.9605\n",
            "0.9658833333333333 0.9612\n",
            "0.9660666666666666 0.9615\n",
            "0.96595 0.9612\n",
            "0.9662 0.9612\n",
            "0.96615 0.9614\n",
            "0.9663 0.9609\n",
            "0.9664 0.9613\n",
            "0.9666833333333333 0.9616\n",
            "0.9665166666666667 0.9617\n",
            "0.9670166666666666 0.962\n",
            "0.96705 0.9629\n",
            "0.9670333333333333 0.9632\n",
            "0.9672833333333334 0.9628\n",
            "0.9675833333333334 0.9626\n",
            "0.9673666666666667 0.9631\n",
            "0.9675333333333334 0.9636\n",
            "0.9677333333333333 0.9629\n",
            "0.9677333333333333 0.9635\n",
            "0.9682333333333333 0.9637\n",
            "0.9683333333333334 0.9634\n",
            "0.9683166666666667 0.964\n",
            "0.9683166666666667 0.9634\n",
            "0.9685666666666667 0.964\n",
            "0.96865 0.9641\n",
            "0.9686166666666667 0.9647\n",
            "0.9687333333333333 0.9646\n",
            "0.9688666666666667 0.9642\n",
            "0.96885 0.964\n",
            "0.96925 0.9645\n",
            "0.9694333333333334 0.9647\n",
            "0.9695333333333334 0.9652\n",
            "0.9696833333333333 0.965\n",
            "0.9700166666666666 0.9655\n",
            "0.9701833333333333 0.9647\n",
            "0.9700833333333333 0.9653\n",
            "0.9701833333333333 0.9651\n",
            "0.9702 0.9656\n",
            "0.9703666666666667 0.9652\n",
            "0.9707666666666667 0.9651\n",
            "0.9705166666666667 0.965\n",
            "0.9709833333333333 0.9646\n",
            "0.971 0.9651\n",
            "0.971 0.9662\n",
            "0.9711 0.9662\n",
            "0.9714666666666667 0.9662\n",
            "0.9716333333333333 0.9662\n",
            "0.9713166666666667 0.9657\n",
            "0.9715666666666667 0.966\n",
            "0.9717 0.9665\n",
            "0.9719666666666666 0.9666\n",
            "0.9722 0.9666\n",
            "0.9720833333333333 0.9664\n",
            "0.9721666666666666 0.9668\n",
            "0.9723333333333334 0.9666\n",
            "0.9726166666666667 0.9671\n",
            "0.9725333333333334 0.9667\n",
            "0.97255 0.9659\n",
            "0.97285 0.9669\n",
            "0.9730333333333333 0.9664\n",
            "0.9730666666666666 0.9667\n",
            "0.9732 0.9668\n",
            "0.9735833333333334 0.9663\n",
            "0.97335 0.967\n",
            "0.97325 0.967\n",
            "0.9730666666666666 0.9668\n",
            "0.97365 0.9667\n",
            "0.9736333333333334 0.9669\n",
            "0.9738166666666667 0.9667\n",
            "0.97385 0.9675\n",
            "0.9738666666666667 0.9671\n",
            "0.97415 0.9674\n",
            "0.97435 0.9671\n",
            "0.9744666666666667 0.9678\n",
            "0.9744166666666667 0.967\n",
            "0.9746 0.9678\n",
            "0.9745166666666667 0.9671\n",
            "0.9744666666666667 0.9669\n",
            "0.9745333333333334 0.9673\n",
            "0.9748166666666667 0.9679\n",
            "0.9749333333333333 0.9671\n",
            "0.9752833333333333 0.9672\n",
            "0.9749666666666666 0.9673\n",
            "0.9751333333333333 0.9682\n",
            "0.9751666666666666 0.9681\n",
            "0.9751333333333333 0.968\n",
            "0.9754666666666667 0.9676\n",
            "0.9754 0.9678\n",
            "0.97535 0.9677\n",
            "0.9758333333333333 0.9684\n",
            "0.9756666666666667 0.968\n",
            "0.9757833333333333 0.9681\n",
            "0.97605 0.9678\n",
            "0.9757166666666667 0.9681\n",
            "0.97575 0.9678\n",
            "0.9763 0.9678\n",
            "0.97625 0.9679\n",
            "0.97635 0.9676\n",
            "0.9763666666666667 0.968\n",
            "0.9763166666666667 0.9677\n",
            "0.97645 0.9683\n",
            "0.9767 0.9685\n",
            "0.9765166666666667 0.9678\n",
            "0.9767333333333333 0.9686\n",
            "0.9768333333333333 0.9681\n",
            "0.9767333333333333 0.9679\n",
            "0.9767 0.9689\n",
            "0.9769833333333333 0.9683\n",
            "0.9770333333333333 0.9688\n",
            "0.97695 0.9686\n",
            "0.9769666666666666 0.9686\n",
            "0.9771 0.9685\n",
            "0.9771166666666666 0.968\n",
            "0.9774333333333334 0.9687\n",
            "0.9776 0.9685\n",
            "0.9773 0.9683\n",
            "0.9776833333333333 0.9684\n",
            "0.9773 0.9685\n",
            "0.9775166666666667 0.9685\n",
            "0.9775666666666667 0.9687\n",
            "0.97755 0.9692\n",
            "0.9776333333333334 0.9692\n",
            "0.9776833333333333 0.9695\n",
            "0.97765 0.9688\n",
            "0.9779666666666667 0.9686\n",
            "0.9776833333333333 0.9695\n",
            "0.9779833333333333 0.9692\n",
            "0.97795 0.969\n",
            "0.9779833333333333 0.9696\n",
            "0.9783833333333334 0.969\n",
            "0.9782 0.9689\n",
            "0.9782333333333333 0.969\n",
            "0.9780833333333333 0.9694\n",
            "0.97835 0.9697\n",
            "0.9784 0.9701\n",
            "0.9782166666666666 0.9693\n",
            "0.9784333333333334 0.9692\n",
            "0.97825 0.9693\n",
            "0.9784166666666667 0.9692\n",
            "0.9786333333333334 0.9693\n",
            "0.97845 0.9693\n",
            "0.9789166666666667 0.9691\n",
            "0.9788666666666667 0.9693\n",
            "0.9791666666666666 0.9701\n",
            "0.9788666666666667 0.9697\n",
            "0.9790166666666666 0.9703\n",
            "0.9793 0.9699\n",
            "0.9794 0.9692\n",
            "0.9793166666666666 0.9699\n",
            "0.9793666666666667 0.9705\n",
            "0.9795333333333334 0.9701\n",
            "0.97975 0.9704\n",
            "0.9798166666666667 0.9702\n",
            "0.9798166666666667 0.9704\n",
            "0.97985 0.9703\n",
            "0.9797 0.9702\n",
            "0.9797833333333333 0.9703\n",
            "0.9799833333333333 0.9703\n",
            "0.9800333333333333 0.97\n",
            "0.9801833333333333 0.9702\n",
            "0.9803 0.9705\n",
            "0.9801666666666666 0.9703\n",
            "0.9802333333333333 0.9701\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import sys, os\nsys.path.append(os.pardir)\n\nimport numpy as np\nfrom dl_ex.dataset.mnist import load_mnist\nfrom dl_ex.ch05.two_layer_net  import TwoLayerNet\n\n# 載入數據\n(x_train, t_train), (x_test, t_test) \u003d load_mnist(normalize\u003dTrue, one_hot_label\u003dTrue)\n\nnetwork \u003d TwoLayerNet(input_size\u003d784, hidden_size\u003d50, output_size\u003d10)\n\niters_num \u003d 20000\ntrain_size \u003d x_train.shape[0]\nbatch_size \u003d 1000\nlearning_rate \u003d 0.05\n\ntrain_loss_list \u003d []\ntrain_acc_list \u003d []\ntest_acc_list \u003d []\n\niter_per_epoch \u003d max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask \u003d np.random.choice(train_size, batch_size)\n    x_batch \u003d x_train[batch_mask]\n    t_batch \u003d t_train[batch_mask]\n    \n    # 利用誤差反向傳播法計算梯度\n    #grad \u003d network.numerical_gradient(x_batch, t_batch)\n    grad \u003d network.gradient(x_batch, t_batch)\n    \n    # 更新\n    for key in (\u0027W1\u0027, \u0027b1\u0027, \u0027W2\u0027, \u0027b2\u0027):\n        network.params[key] -\u003d learning_rate * grad[key]\n    \n    loss \u003d network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch \u003d\u003d 0:\n        train_acc \u003d network.accuracy(x_train, t_train)\n        test_acc \u003d network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(train_acc, test_acc)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 5.7 重點整理\n本章學習使用了視覺化方式顯示計算過程的計算圖。使用計算圖，說明神經網路執行的誤差反向傳播法，還有以層為單位，在神經網路進行處理。例如，ReLU層、Softmax-with-Loss層、Affine層、Softmax層等。在各層中執行forward與backward等方法，正向或反向傳播資料，可以快速計算出權重參數的梯度。利用「層」進行模組化。可以在神經網路中，隨意組合各層，輕鬆製作出想要的網路。\u003cbr\u003e\u003cbr\u003e\n\n* 使用計算圖，可以用視覺化方式掌握計算過程。\n* 計算圖的節點是由局部性計算所構成，其能構成整個計算。\n* 計算圖的正向傳播是進行一般計算。利用計算圖的反向傳播，可以計算出各節點的微分。\n* 把神經網路的構成元素當作「層」來執行處理，可以快速計算出梯度(  亦即誤差反向傳播法 )。\n* 比較數值微分與誤差反向傳播法，可以確認誤差反向傳播法的執行過程有沒有錯誤 ( 梯度檢查 )。\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}